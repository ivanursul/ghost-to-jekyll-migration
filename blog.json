{"db":[{"meta":{"exported_on":1498025593478,"version":"005"},"data":{"app_fields":[],"app_settings":[],"apps":[],"permissions":[{"id":1,"uuid":"f1acd3bc-db77-4458-b701-d23899dd3d70","name":"Export database","object_type":"db","action_type":"exportContent","object_id":null,"created_at":"2016-12-03T07:35:07.000Z","created_by":1,"updated_at":"2016-12-03T07:35:07.000Z","updated_by":1},{"id":2,"uuid":"72f82745-2ce9-4da5-8df3-2c3358a56195","name":"Import database","object_type":"db","action_type":"importContent","object_id":null,"created_at":"2016-12-03T07:35:07.000Z","created_by":1,"updated_at":"2016-12-03T07:35:07.000Z","updated_by":1},{"id":3,"uuid":"4f0b8e53-1b0e-4052-b051-05f30f939ebf","name":"Delete all content","object_type":"db","action_type":"deleteAllContent","object_id":null,"created_at":"2016-12-03T07:35:07.000Z","created_by":1,"updated_at":"2016-12-03T07:35:07.000Z","updated_by":1},{"id":4,"uuid":"d8e26b53-73d8-4ed6-803f-025a69e7295e","name":"Send mail","object_type":"mail","action_type":"send","object_id":null,"created_at":"2016-12-03T07:35:07.000Z","created_by":1,"updated_at":"2016-12-03T07:35:07.000Z","updated_by":1},{"id":5,"uuid":"f0a75096-b092-463e-97c4-fdf6d0b4ab5e","name":"Browse notifications","object_type":"notification","action_type":"browse","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":6,"uuid":"46eef7f1-8ac6-486f-a0bf-33cef19d1574","name":"Add notifications","object_type":"notification","action_type":"add","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":7,"uuid":"82bbeae4-3999-4b17-b651-6d525e595121","name":"Delete notifications","object_type":"notification","action_type":"destroy","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":8,"uuid":"a848d481-d7af-42ef-9bd0-a82566595e2b","name":"Browse posts","object_type":"post","action_type":"browse","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":9,"uuid":"b1f8d4bf-293e-4d8a-a1b3-5d292b31284a","name":"Read posts","object_type":"post","action_type":"read","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":10,"uuid":"35ead614-d313-486c-ac5b-e13ac585765a","name":"Edit posts","object_type":"post","action_type":"edit","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":11,"uuid":"18c1946d-d6c2-490d-8194-a415f8553d82","name":"Add posts","object_type":"post","action_type":"add","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":12,"uuid":"d0759ca6-e474-4f69-a448-9addf5660986","name":"Delete posts","object_type":"post","action_type":"destroy","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":13,"uuid":"923a6bb5-8d87-469f-80ec-2d2682c22ac2","name":"Browse settings","object_type":"setting","action_type":"browse","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":14,"uuid":"2d32b82f-6d10-481d-8f3a-60724d74d959","name":"Read settings","object_type":"setting","action_type":"read","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":15,"uuid":"5d516917-855a-4aa7-a2f8-8081dbfc5252","name":"Edit settings","object_type":"setting","action_type":"edit","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":16,"uuid":"ab313f3b-6ab7-40d3-a754-bf8023b70c21","name":"Generate slugs","object_type":"slug","action_type":"generate","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":17,"uuid":"7a8890f3-1c6a-49d7-b001-e0aa0fbdf36d","name":"Browse tags","object_type":"tag","action_type":"browse","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":18,"uuid":"2240988a-f5bf-4f6b-a3b2-7d9ccf3e1615","name":"Read tags","object_type":"tag","action_type":"read","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":19,"uuid":"6335f8d8-7048-4228-88f9-763d04770670","name":"Edit tags","object_type":"tag","action_type":"edit","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":20,"uuid":"f711d09c-f512-4fb9-814e-2e89d6f6ecf4","name":"Add tags","object_type":"tag","action_type":"add","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":21,"uuid":"f9e6e4d4-7b96-4e5a-84bf-52a24ba09e0a","name":"Delete tags","object_type":"tag","action_type":"destroy","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":22,"uuid":"537e16af-1955-4e94-a6b4-16bbd29f375a","name":"Browse themes","object_type":"theme","action_type":"browse","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":23,"uuid":"acb76b38-6309-4eab-a7e9-623b6aba51e9","name":"Edit themes","object_type":"theme","action_type":"edit","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":24,"uuid":"021ac001-a205-4f5b-b9b7-4949b3c1badd","name":"Browse users","object_type":"user","action_type":"browse","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":25,"uuid":"0e897008-1650-45bd-869b-517550b29770","name":"Read users","object_type":"user","action_type":"read","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":26,"uuid":"a5502710-8816-4ec4-a8d3-c410f0e15fab","name":"Edit users","object_type":"user","action_type":"edit","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":27,"uuid":"17355045-55e3-48f4-b985-e0774ddc86eb","name":"Add users","object_type":"user","action_type":"add","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":28,"uuid":"d5b4b35d-d1a1-4973-9a3b-887cb2e4efab","name":"Delete users","object_type":"user","action_type":"destroy","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":29,"uuid":"2e719105-b487-4d05-9043-c0e1fd37852b","name":"Assign a role","object_type":"role","action_type":"assign","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":30,"uuid":"c1e4405e-8cc5-4c48-aacf-3ab0e5bc61ef","name":"Browse roles","object_type":"role","action_type":"browse","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":31,"uuid":"ddb6cde4-ca1c-4719-a050-b31522159a3c","name":"Browse clients","object_type":"client","action_type":"browse","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":32,"uuid":"57f2c462-8e7d-45f7-a27a-ccd979aeb192","name":"Read clients","object_type":"client","action_type":"read","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":33,"uuid":"5b4d2b0e-b1d4-42d5-b946-a1a3fbf12213","name":"Edit clients","object_type":"client","action_type":"edit","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":34,"uuid":"f82acbbc-def3-44ca-9f7a-ef37957f5e0c","name":"Add clients","object_type":"client","action_type":"add","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":35,"uuid":"e63191c6-a0a0-46fa-a518-7a28bcf658c6","name":"Delete clients","object_type":"client","action_type":"destroy","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":36,"uuid":"6bb9da35-740d-416a-915e-b9e037c42e56","name":"Browse subscribers","object_type":"subscriber","action_type":"browse","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":37,"uuid":"e4858336-f300-421e-b274-db79afc84ca4","name":"Read subscribers","object_type":"subscriber","action_type":"read","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":38,"uuid":"91c3138c-4cdc-493c-ba72-71c553fa744e","name":"Edit subscribers","object_type":"subscriber","action_type":"edit","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":39,"uuid":"32bdf1b9-2556-4bff-aa8c-fcbd0a6fed2e","name":"Add subscribers","object_type":"subscriber","action_type":"add","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1},{"id":40,"uuid":"1641fe0c-d24a-4f03-ae42-2e3bae0dbf8f","name":"Delete subscribers","object_type":"subscriber","action_type":"destroy","object_id":null,"created_at":"2016-12-03T07:35:08.000Z","created_by":1,"updated_at":"2016-12-03T07:35:08.000Z","updated_by":1}],"permissions_apps":[],"permissions_roles":[{"id":1,"role_id":1,"permission_id":1},{"id":2,"role_id":1,"permission_id":2},{"id":3,"role_id":1,"permission_id":3},{"id":4,"role_id":1,"permission_id":4},{"id":5,"role_id":1,"permission_id":5},{"id":6,"role_id":1,"permission_id":6},{"id":7,"role_id":1,"permission_id":7},{"id":8,"role_id":1,"permission_id":8},{"id":9,"role_id":1,"permission_id":9},{"id":10,"role_id":1,"permission_id":10},{"id":11,"role_id":1,"permission_id":11},{"id":12,"role_id":1,"permission_id":12},{"id":13,"role_id":1,"permission_id":13},{"id":14,"role_id":1,"permission_id":14},{"id":15,"role_id":1,"permission_id":15},{"id":16,"role_id":1,"permission_id":16},{"id":17,"role_id":1,"permission_id":17},{"id":18,"role_id":1,"permission_id":18},{"id":19,"role_id":1,"permission_id":19},{"id":20,"role_id":1,"permission_id":20},{"id":21,"role_id":1,"permission_id":21},{"id":22,"role_id":1,"permission_id":22},{"id":23,"role_id":1,"permission_id":23},{"id":24,"role_id":1,"permission_id":24},{"id":25,"role_id":1,"permission_id":25},{"id":26,"role_id":1,"permission_id":26},{"id":27,"role_id":1,"permission_id":27},{"id":28,"role_id":1,"permission_id":28},{"id":29,"role_id":1,"permission_id":29},{"id":30,"role_id":1,"permission_id":30},{"id":31,"role_id":1,"permission_id":31},{"id":32,"role_id":1,"permission_id":32},{"id":33,"role_id":1,"permission_id":33},{"id":34,"role_id":1,"permission_id":34},{"id":35,"role_id":1,"permission_id":35},{"id":36,"role_id":1,"permission_id":36},{"id":37,"role_id":1,"permission_id":37},{"id":38,"role_id":1,"permission_id":38},{"id":39,"role_id":1,"permission_id":39},{"id":40,"role_id":1,"permission_id":40},{"id":41,"role_id":2,"permission_id":8},{"id":42,"role_id":2,"permission_id":9},{"id":43,"role_id":2,"permission_id":10},{"id":44,"role_id":2,"permission_id":11},{"id":45,"role_id":2,"permission_id":12},{"id":46,"role_id":2,"permission_id":13},{"id":47,"role_id":2,"permission_id":14},{"id":48,"role_id":2,"permission_id":16},{"id":49,"role_id":2,"permission_id":17},{"id":50,"role_id":2,"permission_id":18},{"id":51,"role_id":2,"permission_id":19},{"id":52,"role_id":2,"permission_id":20},{"id":53,"role_id":2,"permission_id":21},{"id":54,"role_id":2,"permission_id":24},{"id":55,"role_id":2,"permission_id":25},{"id":56,"role_id":2,"permission_id":26},{"id":57,"role_id":2,"permission_id":27},{"id":58,"role_id":2,"permission_id":28},{"id":59,"role_id":2,"permission_id":29},{"id":60,"role_id":2,"permission_id":30},{"id":61,"role_id":2,"permission_id":31},{"id":62,"role_id":2,"permission_id":32},{"id":63,"role_id":2,"permission_id":33},{"id":64,"role_id":2,"permission_id":34},{"id":65,"role_id":2,"permission_id":35},{"id":66,"role_id":2,"permission_id":39},{"id":67,"role_id":3,"permission_id":8},{"id":68,"role_id":3,"permission_id":9},{"id":69,"role_id":3,"permission_id":11},{"id":70,"role_id":3,"permission_id":13},{"id":71,"role_id":3,"permission_id":14},{"id":72,"role_id":3,"permission_id":16},{"id":73,"role_id":3,"permission_id":17},{"id":74,"role_id":3,"permission_id":18},{"id":75,"role_id":3,"permission_id":20},{"id":76,"role_id":3,"permission_id":24},{"id":77,"role_id":3,"permission_id":25},{"id":78,"role_id":3,"permission_id":30},{"id":79,"role_id":3,"permission_id":31},{"id":80,"role_id":3,"permission_id":32},{"id":81,"role_id":3,"permission_id":33},{"id":82,"role_id":3,"permission_id":34},{"id":83,"role_id":3,"permission_id":35},{"id":84,"role_id":3,"permission_id":39}],"permissions_users":[],"posts":[{"id":2,"uuid":"009cfd17-021d-43e2-9dd5-20b1f6c51c51","title":"Setting up ghost blogging platform","slug":"setting-up-ghost-blogging-platform","markdown":"Hi all!\nToday I started to use ghost blogging platform.One I can say for sure - It's much more faster than wordpress or other cms.\nIn this article I will try to explain how to setup your application with some custom theme.\nWhat Will we need for that ?\n\n* Digital Ocean instance. I use Linux for all maintenance, so, if u are windows - related -> Sorry :)\n\n* Ghost Blogging platform\n\n* Custom theme, that you want to setup. I was lucky to buy this theme - [Pepe - Multipurpose Responsive Theme](http://themeforest.net/item/pepe-multipurpose-responsive-theme/9634807?WT.oss_phrase=&WT.oss_rank=10&WT.z_author=muvolab&WT.ac=search_list)\n\n## Let's start\n\nFirstly, create instance on Digital Ocean - I noticed, that ghost is much more faster, than Wordpress, so it should be enough to buy **5$** instance for first time.\nType your instance name, desired size. I recommend access by ssh, which is very powerful way for serving all your instances.\n\nIf you don't have any ssh key yet - follow [this](https://help.github.com/articles/generating-ssh-keys/) instruction on how to create one.\n\nOnce you are ready, attach your public key to digitalocean profile and select it when you will create instance. You can read [this](https://www.digitalocean.com/community/tutorials/how-to-connect-to-your-droplet-with-ssh) article for better understanding.\n\n![Creating Digital Ocean instance](/content/images/2015/02/Screen-Shot-2015-02-07-at-6-29-42-PM.png)\n![](/content/images/2015/02/Screen-Shot-2015-02-07-at-6-29-58-PM.png)\n\nIt should take some time to create instance. Once it ended, go to your local computer **terminal** and type next line\n\n> nano ~/.ssh/config\n\nAnd append this lines\n\n> \tHost blog\n> \t\tUser blogger\n> \t\tHostName IP FROM YOUR DROPLET\n> \t\tIdentityFile ~/.ssh/id_rsa_digitalocean\n\nI set linux user **blogger**, but he is not create yet, so we need to ssh to your server with **root** user.\n\n> ssh root@blog\n\nOnce you are connected, type\n\n> adduser blogger\n\nType all required information, password, name, room number and other info.\n\nDon't forget to connect user to sudo group\n> sudo usermod -a -G sudo blogger\n\nOnce you are connected, exit from ssh.\n\nThen try to connect with default user\n\n> ssh blog\n\nYou will be connected to blogger linux user by default.\n\nThen do some simple instuctions\n\n> curl -sL https://deb.nodesource.com/setup | sudo bash -\n\n> sudo apt-get install -y nodejs\n\n> mkdir tools\n\n> cd tools\n\n> curl -L https://ghost.org/zip/ghost-latest.zip -o ghost.zip\n\n> unzip -uo ghost.zip -d ghost\n\n> cd ghost\n\n> npm install --production\n\n> nano config.js\n\nedit your production host with your required url.\n\nThen type\n\n> npm start --production\n\n\nBut there is one problem. When you will release your ssh connection, your server will fall down.\n\nso type **Control + C** and type\n\n> screen -R ghost\n\nIf you don't have any screen, then install it.\n\n> sudo apt-get install screen\n\nThat will create separate screen( I call it thread, like in Java language) for your ghost server.\n\nPress enter, and type\n\n> npm start --production\n\nWait until server will log that everything deploys succesfully and press **CTRL + A + D** , that combination will switch you into your main screen.\nIn future, if you will need to list all your screen, just type\n\n> screen -list\n\nNow you are ready to custmize your theme.\nI am happy, that I choosed ghost for my blogging.I hope, this article will help you to start your blog rapidly.\n\nThat's all that you need to start\n\nSome moment, on which I want to emphasize\n\nIf you will buy custom theme, and choose one click install ghost platform with digitalocean -> 90 % that you will have problems. The reason is that digitalocean instance ships with old ghost version, so there is always some troubles.\n\n\n![](/content/images/2015/02/Screen-Shot-2015-02-07-at-5-30-23-PM.png)\n\n\n","mobiledoc":null,"html":"<p>Hi all! <br />\nToday I started to use ghost blogging platform.One I can say for sure - It's much more faster than wordpress or other cms. <br />\nIn this article I will try to explain how to setup your application with some custom theme. <br />\nWhat Will we need for that ?</p>\n\n<ul>\n<li><p>Digital Ocean instance. I use Linux for all maintenance, so, if u are windows - related -> Sorry :)</p></li>\n<li><p>Ghost Blogging platform</p></li>\n<li><p>Custom theme, that you want to setup. I was lucky to buy this theme - <a href=\"http://themeforest.net/item/pepe-multipurpose-responsive-theme/9634807?WT.oss_phrase=&amp;WT.oss_rank=10&amp;WT.z_author=muvolab&amp;WT.ac=search_list\">Pepe - Multipurpose Responsive Theme</a></p></li>\n</ul>\n\n<h2 id=\"letsstart\">Let's start</h2>\n\n<p>Firstly, create instance on Digital Ocean - I noticed, that ghost is much more faster, than Wordpress, so it should be enough to buy <strong>5$</strong> instance for first time. <br />\nType your instance name, desired size. I recommend access by ssh, which is very powerful way for serving all your instances.</p>\n\n<p>If you don't have any ssh key yet - follow <a href=\"https://help.github.com/articles/generating-ssh-keys/\">this</a> instruction on how to create one.</p>\n\n<p>Once you are ready, attach your public key to digitalocean profile and select it when you will create instance. You can read <a href=\"https://www.digitalocean.com/community/tutorials/how-to-connect-to-your-droplet-with-ssh\">this</a> article for better understanding.</p>\n\n<p><img src=\"/content/images/2015/02/Screen-Shot-2015-02-07-at-6-29-42-PM.png\" alt=\"Creating Digital Ocean instance\" />\n<img src=\"/content/images/2015/02/Screen-Shot-2015-02-07-at-6-29-58-PM.png\" alt=\"\" /></p>\n\n<p>It should take some time to create instance. Once it ended, go to your local computer <strong>terminal</strong> and type next line</p>\n\n<blockquote>\n  <p>nano ~/.ssh/config</p>\n</blockquote>\n\n<p>And append this lines</p>\n\n<blockquote>\n<pre><code>Host blog\n    User blogger\n    HostName IP FROM YOUR DROPLET\n    IdentityFile ~/.ssh/id_rsa_digitalocean\n</code></pre>\n</blockquote>\n\n<p>I set linux user <strong>blogger</strong>, but he is not create yet, so we need to ssh to your server with <strong>root</strong> user.</p>\n\n<blockquote>\n  <p>ssh root@blog</p>\n</blockquote>\n\n<p>Once you are connected, type</p>\n\n<blockquote>\n  <p>adduser blogger</p>\n</blockquote>\n\n<p>Type all required information, password, name, room number and other info.</p>\n\n<p>Don't forget to connect user to sudo group  </p>\n\n<blockquote>\n  <p>sudo usermod -a -G sudo blogger</p>\n</blockquote>\n\n<p>Once you are connected, exit from ssh.</p>\n\n<p>Then try to connect with default user</p>\n\n<blockquote>\n  <p>ssh blog</p>\n</blockquote>\n\n<p>You will be connected to blogger linux user by default.</p>\n\n<p>Then do some simple instuctions</p>\n\n<blockquote>\n  <p>curl -sL <a href=\"https://deb.nodesource.com/setup\">https://deb.nodesource.com/setup</a> | sudo bash -</p>\n  \n  <p>sudo apt-get install -y nodejs</p>\n  \n  <p>mkdir tools</p>\n  \n  <p>cd tools</p>\n  \n  <p>curl -L <a href=\"https://ghost.org/zip/ghost-latest.zip\">https://ghost.org/zip/ghost-latest.zip</a> -o ghost.zip</p>\n  \n  <p>unzip -uo ghost.zip -d ghost</p>\n  \n  <p>cd ghost</p>\n  \n  <p>npm install --production</p>\n  \n  <p>nano config.js</p>\n</blockquote>\n\n<p>edit your production host with your required url.</p>\n\n<p>Then type</p>\n\n<blockquote>\n  <p>npm start --production</p>\n</blockquote>\n\n<p>But there is one problem. When you will release your ssh connection, your server will fall down.</p>\n\n<p>so type <strong>Control + C</strong> and type</p>\n\n<blockquote>\n  <p>screen -R ghost</p>\n</blockquote>\n\n<p>If you don't have any screen, then install it.</p>\n\n<blockquote>\n  <p>sudo apt-get install screen</p>\n</blockquote>\n\n<p>That will create separate screen( I call it thread, like in Java language) for your ghost server.</p>\n\n<p>Press enter, and type</p>\n\n<blockquote>\n  <p>npm start --production</p>\n</blockquote>\n\n<p>Wait until server will log that everything deploys succesfully and press <strong>CTRL + A + D</strong> , that combination will switch you into your main screen. <br />\nIn future, if you will need to list all your screen, just type</p>\n\n<blockquote>\n  <p>screen -list</p>\n</blockquote>\n\n<p>Now you are ready to custmize your theme. <br />\nI am happy, that I choosed ghost for my blogging.I hope, this article will help you to start your blog rapidly.</p>\n\n<p>That's all that you need to start</p>\n\n<p>Some moment, on which I want to emphasize</p>\n\n<p>If you will buy custom theme, and choose one click install ghost platform with digitalocean -> 90 % that you will have problems. The reason is that digitalocean instance ships with old ghost version, so there is always some troubles.</p>\n\n<p><img src=\"/content/images/2015/02/Screen-Shot-2015-02-07-at-5-30-23-PM.png\" alt=\"\" /></p>","image":"/content/images/2015/02/ghost_logo_big.png","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":"Getting started with ghost blogging platform","meta_description":"This article explains how to setup ghost bloggin platform.","author_id":1,"created_at":"2015-02-07T14:50:29.000Z","created_by":1,"updated_at":"2015-07-27T12:08:25.000Z","updated_by":1,"published_at":"2015-02-07T14:55:04.000Z","published_by":1},{"id":3,"uuid":"c9e1ad34-01d8-4429-b83b-3cb1c271bfb6","title":"Sending emails with Java, Spring and Handlebars","slug":"sending-mails-with-javaxmail-handlebars","markdown":"Nowadays, it's very popular to have applications, that can somehow notificate people.We're gonna use emails to send text and html mails to someone.Whar requirements do we need?\n\nPossibility to send simple text emails\nPosibility to send html emails.\nOf course, to send emails, we need to have some template engine. We will use handlebars for that.\n\nWe will use\n\nSpring.\nMaven as build tool\nJava Mail package\nHandlebars template engine\nGreenmail for \"mocking\" email server.\n\n\nStructure\n\n![](/content/images/2015/02/Screen-Shot-2015-01-19-at-8-06-50-PM.png)\n\nWe need to have four subpackages, exception package for having custom exceptions, model package for storing our model, sender package for main functionality and template package  for storing template service.\n\nYour pom.xml should have similar structure\n\n<script src=\"https://gist.github.com/johnyUA/264a2351757e145c3ab5.js\"></script>\n\nSo, let's create\n\nException package\n\n<script src=\"https://gist.github.com/johnyUA/c97aff0de7c789b38d96.js\"></script>\n\nModel package \n\n<script src=\"https://gist.github.com/johnyUA/4261a67b1b07ef23fcbe.js\"></script>\n\n<script src=\"https://gist.github.com/johnyUA/2d80c5a00d3eec3d631b.js\"></script>\n\n<script src=\"https://gist.github.com/johnyUA/18826b54f157e3237cfb.js\"></script>\n\n<script src=\"https://gist.github.com/johnyUA/0fbeb7fa9ab14debfc68.js\"></script>\n\nWe created all required models and resources for further developing, so let's start adding core functionality\n\n<script src=\"https://gist.github.com/johnyUA/e906a849a0faa724f603.js\"></script>\n\n<script src=\"https://gist.github.com/johnyUA/5c54a21baa82610d2c14.js\"></script>\n\nTemplate package\n\nHave ever you ever think about sending emails with html content ? Imagine, your application sends email with some pretty organized html page. Sound great.\n\nTo do such things, of course, you need to have template engine.\n\nI prefer Handlebards template engine.\n\n<script src=\"https://gist.github.com/johnyUA/9fa009b8a6a369d36bc3.js\"></script>\n\n<script src=\"https://gist.github.com/johnyUA/60fd72b167cb6243c74f.js\"></script>[gist id=\"60fd72b167cb6243c74f\"]\n\nif you have template in your resources/templates folder with name template1.html, then you should call handlebars.compile(\"template1\"); in your code.\n\nUsage\n\nIt's up to you how you will use this functionality, you can think about that on your spare time, I propose to use some sort of little services for each case, for example: sometimes you definitely know, that you will have constant amount of recipients, that will receive email, sometimes you know what will be the topic of your email, so , for each of that cases you can write separate service.\n\nSomething like that.\n\n<script src=\"https://gist.github.com/johnyUA/ed799886b391d3a2f5b0.js\"></script>\n\n<script src=\"https://gist.github.com/johnyUA/2c5e72334d709ffc7062.js\"></script>\n\n<script src=\"https://gist.github.com/johnyUA/92e4d8eebef16e6b46da.js\"></script>\n\n \n\nResources\n\nYour resources folder should like that\n\n \n\n![](/content/images/2015/02/Screen-Shot-2015-02-01-at-5-03-36-PM.png)\n\n<script src=\"https://gist.github.com/johnyUA/10f0d4d029caf07f1fb0.js\"></script>\n\nGreenmail is a fake server, that will allow you to register your beans in Spring. Also, you can use greenforest in your tests, receive mails, etc.\n\n<script src=\"https://gist.github.com/johnyUA/5b924f414b77801b51e1.js\"></script>\n\n<script src=\"https://gist.github.com/johnyUA/ad3eee93880d62899724.js\"></script>\n\n \n\nHow to connect it with your spring application\n\nTo connect this example with your application, you need to include mail-context.xml in your Spring context.\n\nI use something like this for importing my module.\n\n<script src=\"https://gist.github.com/johnyUA/196e1baab252bf1c14d5.js\"></script>\n\n \n\nIf you will have some questions - feel free to ask me.","mobiledoc":null,"html":"<p>Nowadays, it's very popular to have applications, that can somehow notificate people.We're gonna use emails to send text and html mails to someone.Whar requirements do we need?</p>\n\n<p>Possibility to send simple text emails <br />\nPosibility to send html emails. <br />\nOf course, to send emails, we need to have some template engine. We will use handlebars for that.</p>\n\n<p>We will use</p>\n\n<p>Spring. <br />\nMaven as build tool <br />\nJava Mail package <br />\nHandlebars template engine <br />\nGreenmail for \"mocking\" email server.</p>\n\n<p>Structure</p>\n\n<p><img src=\"/content/images/2015/02/Screen-Shot-2015-01-19-at-8-06-50-PM.png\" alt=\"\" /></p>\n\n<p>We need to have four subpackages, exception package for having custom exceptions, model package for storing our model, sender package for main functionality and template package  for storing template service.</p>\n\n<p>Your pom.xml should have similar structure</p>\n\n<script src=\"https://gist.github.com/johnyUA/264a2351757e145c3ab5.js\"></script>\n\n<p>So, let's create</p>\n\n<p>Exception package</p>\n\n<script src=\"https://gist.github.com/johnyUA/c97aff0de7c789b38d96.js\"></script>\n\n<p>Model package </p>\n\n<script src=\"https://gist.github.com/johnyUA/4261a67b1b07ef23fcbe.js\"></script>\n\n<script src=\"https://gist.github.com/johnyUA/2d80c5a00d3eec3d631b.js\"></script>\n\n<script src=\"https://gist.github.com/johnyUA/18826b54f157e3237cfb.js\"></script>\n\n<script src=\"https://gist.github.com/johnyUA/0fbeb7fa9ab14debfc68.js\"></script>\n\n<p>We created all required models and resources for further developing, so let's start adding core functionality</p>\n\n<script src=\"https://gist.github.com/johnyUA/e906a849a0faa724f603.js\"></script>\n\n<script src=\"https://gist.github.com/johnyUA/5c54a21baa82610d2c14.js\"></script>\n\n<p>Template package</p>\n\n<p>Have ever you ever think about sending emails with html content ? Imagine, your application sends email with some pretty organized html page. Sound great.</p>\n\n<p>To do such things, of course, you need to have template engine.</p>\n\n<p>I prefer Handlebards template engine.</p>\n\n<script src=\"https://gist.github.com/johnyUA/9fa009b8a6a369d36bc3.js\"></script>\n\n<script src=\"https://gist.github.com/johnyUA/60fd72b167cb6243c74f.js\"></script>[gist id=\"60fd72b167cb6243c74f\"]\n\nif you have template in your resources/templates folder with name template1.html, then you should call handlebars.compile(\"template1\"); in your code.\n\nUsage\n\nIt's up to you how you will use this functionality, you can think about that on your spare time, I propose to use some sort of little services for each case, for example: sometimes you definitely know, that you will have constant amount of recipients, that will receive email, sometimes you know what will be the topic of your email, so , for each of that cases you can write separate service.\n\nSomething like that.\n\n<script src=\"https://gist.github.com/johnyUA/ed799886b391d3a2f5b0.js\"></script>\n\n<script src=\"https://gist.github.com/johnyUA/2c5e72334d709ffc7062.js\"></script>\n\n<script src=\"https://gist.github.com/johnyUA/92e4d8eebef16e6b46da.js\"></script>\n\n<p>Resources</p>\n\n<p>Your resources folder should like that</p>\n\n<p><img src=\"/content/images/2015/02/Screen-Shot-2015-02-01-at-5-03-36-PM.png\" alt=\"\" /></p>\n\n<script src=\"https://gist.github.com/johnyUA/10f0d4d029caf07f1fb0.js\"></script>\n\n<p>Greenmail is a fake server, that will allow you to register your beans in Spring. Also, you can use greenforest in your tests, receive mails, etc.</p>\n\n<script src=\"https://gist.github.com/johnyUA/5b924f414b77801b51e1.js\"></script>\n\n<script src=\"https://gist.github.com/johnyUA/ad3eee93880d62899724.js\"></script>\n\n<p>How to connect it with your spring application</p>\n\n<p>To connect this example with your application, you need to include mail-context.xml in your Spring context.</p>\n\n<p>I use something like this for importing my module.</p>\n\n<script src=\"https://gist.github.com/johnyUA/196e1baab252bf1c14d5.js\"></script>\n\n<p>If you will have some questions - feel free to ask me.</p>","image":"/content/images/2015/02/708459-1280x800-java1920x1200.jpg","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-02-07T16:32:35.000Z","created_by":1,"updated_at":"2015-07-27T12:08:15.000Z","updated_by":1,"published_at":"2015-02-07T16:37:04.000Z","published_by":1},{"id":4,"uuid":"41a212f6-7c30-419d-bda2-0485d3d8057c","title":"Encoding Filter Not Encoding at all ?","slug":"encoding-filter-not-encoding-at-all","markdown":"Recently I experienced a problem, when I had CharacterEncodingFilter filter in my web.xml, but no encoding were done at all. I started to investigate this problem and find out, that few weeks ago I added additional filters on top of my web.xml, and , as a result, my CharacterEncodingFilter was placed on second or third position.Good news for me, I added integration test with ukrainian words, and my web service returned some incorrect characters.\n\nAfter few minutes, I discovered, that If you want to have correct unicode data, then you need to place encoding filter at the top of all your filters. Thereafter, your filter will the first chain and you will be sure, that all other filters will work with correct unicode.\n\n ","mobiledoc":null,"html":"<p>Recently I experienced a problem, when I had CharacterEncodingFilter filter in my web.xml, but no encoding were done at all. I started to investigate this problem and find out, that few weeks ago I added additional filters on top of my web.xml, and , as a result, my CharacterEncodingFilter was placed on second or third position.Good news for me, I added integration test with ukrainian words, and my web service returned some incorrect characters.</p>\n\n<p>After few minutes, I discovered, that If you want to have correct unicode data, then you need to place encoding filter at the top of all your filters. Thereafter, your filter will the first chain and you will be sure, that all other filters will work with correct unicode.</p>","image":"/content/images/2015/02/spring-by-pivotal.png","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-02-07T16:42:57.000Z","created_by":1,"updated_at":"2015-02-07T16:44:14.000Z","updated_by":1,"published_at":"2015-02-07T16:43:14.000Z","published_by":1},{"id":6,"uuid":"6347fe84-c590-44d6-9219-593dd771fe76","title":"Spring XML and beans inheritance","slug":"spring-xml-and-beans-inheritance","markdown":"If you are using XML configuration in your Spring application, you should know about gorgeous feature, that spring has: bean inheritance. What does this mean ? This mean, that you can create an abstract bean with some predefined properties, and later, you will be able to make an inheritance from this bean.\n\nHere is an example\n\n<script src=\"https://gist.github.com/johnyUA/0c991f4c0490fd409194.js\"></script>\n\nHere you have DefaultDao implementation, which has three fields - persistenceManager, which is the same in every Dao class, queryBuilder for building dynamic queries,  which are dynamic field, and entityClass, which is the name of Class, that will be mapped to result from database.\n\nAnd here we have xml context, that we use without bean inheritance.\n\n<script src=\"https://gist.github.com/johnyUA/fdf6a2ed1c9b11580584.js\"></script>\n\nAfter some modification and refactoring we will receive\n\n<script src=\"https://gist.github.com/johnyUA/70a5ad7ce5555d920d5c.js\"></script>\n\nAs you can see, we reduce number of lines in context and now we are instantiating persistenceManager from one place and if we will want to change it, we will do that very quickly.","mobiledoc":null,"html":"<p>If you are using XML configuration in your Spring application, you should know about gorgeous feature, that spring has: bean inheritance. What does this mean ? This mean, that you can create an abstract bean with some predefined properties, and later, you will be able to make an inheritance from this bean.</p>\n\n<p>Here is an example</p>\n\n<script src=\"https://gist.github.com/johnyUA/0c991f4c0490fd409194.js\"></script>\n\n<p>Here you have DefaultDao implementation, which has three fields - persistenceManager, which is the same in every Dao class, queryBuilder for building dynamic queries,  which are dynamic field, and entityClass, which is the name of Class, that will be mapped to result from database.</p>\n\n<p>And here we have xml context, that we use without bean inheritance.</p>\n\n<script src=\"https://gist.github.com/johnyUA/fdf6a2ed1c9b11580584.js\"></script>\n\n<p>After some modification and refactoring we will receive</p>\n\n<script src=\"https://gist.github.com/johnyUA/70a5ad7ce5555d920d5c.js\"></script>\n\n<p>As you can see, we reduce number of lines in context and now we are instantiating persistenceManager from one place and if we will want to change it, we will do that very quickly.</p>","image":"/content/images/2015/02/spring-framework-project-logo-1.png","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-02-07T21:59:10.000Z","created_by":1,"updated_at":"2015-07-27T12:07:44.000Z","updated_by":1,"published_at":"2015-02-07T22:00:37.000Z","published_by":1},{"id":7,"uuid":"d0b82aca-7e9c-4d21-ba2f-fbf97627e768","title":"Setting up continuous delivery with Jenkins, Maven and Tomcat","slug":"setting-up-continuous-delivery-with-jenkins-maven-and-tomcat","markdown":"Sometimes it's hard for me to deploy your application each time when you chang something in your code. Truly, if it is important for you to deploy your application at least twice per day, and the process of deployment will take up to 10 minutes, then it will result to +- 200 minutes per month and 2400 minutes per year!!! That's almost 2 days.\n\nSo, instead of doing deployment each time manually, I propose you to do that with your Continuous Integration tool.\n\nThe process of continuous deploy is called continuous delivery, which includes Build -> Deploy -> Test -> Release.\n\n## Required tools\n\nIn this article you will need a couple of tools to succed\n\nContinuous Integration tool. It is important to trigger every change you made in your code. For that needs you can use Jenkins continuous integration server.\nBuild tool. It is a common knowledge that nowadays we don't need to build our apps manually, now we build them by ready tools, that simplifies our development process. I will be using maven\nfor that.It helps me to manage my dependencies, split my application into several modules, has many plugins, can check the quality of my code.\nApplication server. If CI server will trigger change to your code - he will need to build and deploy your application somewhere. As application server, I use Tomcat.\n## What's not covered\n\nThere will be no information on how to deal with maven, tomcat, and jenkins tools. You should be familiar with that tools.\nServer configuration. You can read this article for better understanding. The only thing I would like to mention is that you need to have ssh access to your server.\nData Storage configuration. There will be issues with database, and you really need to setup you storage by your own.\nThird party tools. I use migrations for database changes and there was some problems with this type of tools.\nPreamble\n\n## What you need to know about continuous delivery ?\n\nFirstly, I would suggest to read wiki page.\n\nIn few words, it is a practice of automated delivery process. You don't need to deploy, build your application every time, because CI server will do that for you.\n\nSo, this mean that if we have web application, then every time when we will commit changes to git server, jenkins will trigger that, will build project, will create war archieve and deploy it to our dev server.\n\n## Syllabus\n\nFor better understanding we will split our work into several parts:\n\nDevelopment server configuration.\nTomcat Maven plugin, configuration.\nJenkins configuration.\nSetting up integration test job.\nDevelopment server configuration\n\nOn your dev server you will need to have next apps installed\n\nTomcat, running on 8080 port\nDatabase, in my case, running on 5432 port\nJava installed - 1.7 version\nAs I have mentioned before, you can use this article to setup your basic dev server.\n\nAfter you setup all required software, you can to make some things:\n\nMake ssh access to dev server from jenkins build tool. I will explain why: If you will need to do some work with database(running migrations, for example), you will need to access your database. For example, if you have postgres database, then you definetely know, that, by default, postgres is running on localhost on 5432 port and don't have access outside server, so, you can't connect to database by remote IP on port 5432. There is two options that you can do: configure postgresql to have access outside, which is undesirable or you can make ssh port forwarding , you will setup local port, for example, 2222 to forward everything on 5432 port on your remote dev server.\nTomcat enhancement. You can configure nginx to listen to 80 port and to redirect all requests to 8080 tomcat port, which will be closed to outside requests.\nStart tomcat. You can make aliases to easily deploy Tomcat:\n\n> alias tomcat-start='~/tools/tomcat/bin/startup.sh'\n\n> alias tomcat-stop='~/tools/tomcat/bin/shutdown.sh'\n\nThen execute in terminal\n\n> tomcat-start\n\n## Tomcat maven configuration\n\nThis part is required for our configuration because we will use tomcat maven plugin to deploy our application into dev-server.Note, that application will be deployed only in case of successful build.\n\n## To make things work we will need:\n\nConfiguring new user in tomcat.\nAdd server into ~/.m2/settings.xml file on your jenkins machine. This part is needed because Tomcat requires to have such information about server.\nAdding plugin.\nAdding server\n\nThis is need to be done to be able to deploy your war into your tomcat.\n\nLogin into your dev-server, cd into your tomcat directory and execute:\n\nnano ./conf/tomcat-users.xml\nYou should have something like this\n\n<script src=\"https://gist.github.com/johnyUA/dc18ff0a37a5451fa94f.js\"></script>\n\nDon't forget to restart tomcat\n\nConfiguring maven settings.xml\n\nThis action should be done in jenkins server.\n\nIf maven will deploy our app into tomcat server, then maven should have tomcat credentials. Credentials are stored in your home directory/.m2/settings.xml\n\nIf your don't have such file, then create it or execute\n\n> nano ~/.m2/settings.xml\n\nand add next lines\n\n<script src=\"https://gist.github.com/johnyUA/7bc0f10e14653d8a7640.js\"></script>\n\nid is server name, which will be specified in tomcat plugin.\n\n## Adding plugin\n\nTo add tomcat maven plugin , add this in plugins section\n\n<script src=\"https://gist.github.com/johnyUA/99c306ee8e7793536731.js\"></script>\n\nand properties section\n\n<script src=\"https://gist.github.com/johnyUA/1d7fa79fd1b84f91973d.js\"></script>\n\nIf you did everything right, then you should be able to deploy your application into your development server\n\n> mvn tomcat7:redeploy\n\nYou should receive build success message.\n\n## Jenkins configuration\n\nBefore this part I want to say, that I assume that you've already setup your main job, and it is working.\n\nNow, if you did all previous steps, you can start configuring jenkins to make a deployment.\n\nI'm assuming that you have already installed jenkins, added your job without deployment and you are waiting for build setup.In my application I have several modules, where one of them - is module with war packaging, so he is a submodule.\n\n![](/content/images/2015/02/Screen-Shot-2015-01-11-at-4-58-20-PM.png)\n\n**is-lnu-rest-api**  is my **webapp**, so it will be the module, where we  will store our tomcat plugin.\n\nEnter your jenkins server, open your job and press configure\n![](/content/images/2015/02/Screen-Shot-2015-01-11-at-5-02-12-PM.png)\n\njenkins job\n\nThen go to Build section and press Add post-build step -> Invoke top level maven targets\n\n![](/content/images/2015/02/Screen-Shot-2015-01-11-at-5-06-54-PM-1024x417.png)\n\nAdding top level maven targets\n \n\nAnd add appropriate settings\n\n![](/content/images/2015/02/Screen-Shot-2015-01-11-at-5-20-00-PM-1024x517.png)\n\nNow you can start building your application, and if you did everything right - After each commit your server will be redeployed.\n\n## Setting up integration test job\n\nSo, what we have? We manage to configure our jenkins job to trigger git changes, run unit tests, make quality checks, and deploy application to some dev-server. We have another thing to do: integration testing. I want to run integration tests each time, when some changes happen. Do that make sense ? My position is that we definetely need to run those integration tests each time when changes happen. That will allow us to detect broken functionality as soon as possible.Of course, you need to have separate qa server for running those tests, but it is worth it.\n\nSo, what is the right way for running integration tests with continuous integration tools ? I didn't find exact answer for that, but in my opinion we need to trigger our main build, which will deploy application, and in case of success we should run integration tests.\n\nGo to Jenkins main page, create job, simillar to your first job, but with other Build triggers.\n\n![](/content/images/2015/02/Screen-Shot-2015-01-12-at-9-12-18-PM-1024x260.png)\n\nChoose your main project job.\n\nNote, that you main job should handle project deploy, so, when you will start new job, that will run integration tests, you will be sure, that webapp is deployed and is up to date.\n\nI'm not sure in which way you run your integration tests, but I have separate module for them, so to run them, I have next Build section\n\n![](/content/images/2015/02/Screen-Shot-2015-01-12-at-9-28-58-PM-1024x170.png)\n\n \n\nHowever, there is a chance, that you have separate servers for dev and for qa. For such approach you need to modify second integration job to run tomcat:redeploy on qa server.\n\nAfter all manipulations, you should have the following functionality:\n\nYour main job will run junit tests, findbugs analys, checkstyle, etc...\nYour main job will deploy war to remote dev server\nYour integration job will run integration tests","mobiledoc":null,"html":"<p>Sometimes it's hard for me to deploy your application each time when you chang something in your code. Truly, if it is important for you to deploy your application at least twice per day, and the process of deployment will take up to 10 minutes, then it will result to +- 200 minutes per month and 2400 minutes per year!!! That's almost 2 days.</p>\n\n<p>So, instead of doing deployment each time manually, I propose you to do that with your Continuous Integration tool.</p>\n\n<p>The process of continuous deploy is called continuous delivery, which includes Build -> Deploy -> Test -> Release.</p>\n\n<h2 id=\"requiredtools\">Required tools</h2>\n\n<p>In this article you will need a couple of tools to succed</p>\n\n<p>Continuous Integration tool. It is important to trigger every change you made in your code. For that needs you can use Jenkins continuous integration server. <br />\nBuild tool. It is a common knowledge that nowadays we don't need to build our apps manually, now we build them by ready tools, that simplifies our development process. I will be using maven <br />\nfor that.It helps me to manage my dependencies, split my application into several modules, has many plugins, can check the quality of my code. <br />\nApplication server. If CI server will trigger change to your code - he will need to build and deploy your application somewhere. As application server, I use Tomcat.  </p>\n\n<h2 id=\"whatsnotcovered\">What's not covered</h2>\n\n<p>There will be no information on how to deal with maven, tomcat, and jenkins tools. You should be familiar with that tools. <br />\nServer configuration. You can read this article for better understanding. The only thing I would like to mention is that you need to have ssh access to your server. <br />\nData Storage configuration. There will be issues with database, and you really need to setup you storage by your own. <br />\nThird party tools. I use migrations for database changes and there was some problems with this type of tools. <br />\nPreamble</p>\n\n<h2 id=\"whatyouneedtoknowaboutcontinuousdelivery\">What you need to know about continuous delivery ?</h2>\n\n<p>Firstly, I would suggest to read wiki page.</p>\n\n<p>In few words, it is a practice of automated delivery process. You don't need to deploy, build your application every time, because CI server will do that for you.</p>\n\n<p>So, this mean that if we have web application, then every time when we will commit changes to git server, jenkins will trigger that, will build project, will create war archieve and deploy it to our dev server.</p>\n\n<h2 id=\"syllabus\">Syllabus</h2>\n\n<p>For better understanding we will split our work into several parts:</p>\n\n<p>Development server configuration. <br />\nTomcat Maven plugin, configuration. <br />\nJenkins configuration. <br />\nSetting up integration test job. <br />\nDevelopment server configuration</p>\n\n<p>On your dev server you will need to have next apps installed</p>\n\n<p>Tomcat, running on 8080 port <br />\nDatabase, in my case, running on 5432 port <br />\nJava installed - 1.7 version <br />\nAs I have mentioned before, you can use this article to setup your basic dev server.</p>\n\n<p>After you setup all required software, you can to make some things:</p>\n\n<p>Make ssh access to dev server from jenkins build tool. I will explain why: If you will need to do some work with database(running migrations, for example), you will need to access your database. For example, if you have postgres database, then you definetely know, that, by default, postgres is running on localhost on 5432 port and don't have access outside server, so, you can't connect to database by remote IP on port 5432. There is two options that you can do: configure postgresql to have access outside, which is undesirable or you can make ssh port forwarding , you will setup local port, for example, 2222 to forward everything on 5432 port on your remote dev server. <br />\nTomcat enhancement. You can configure nginx to listen to 80 port and to redirect all requests to 8080 tomcat port, which will be closed to outside requests. <br />\nStart tomcat. You can make aliases to easily deploy Tomcat:</p>\n\n<blockquote>\n  <p>alias tomcat-start='~/tools/tomcat/bin/startup.sh'</p>\n  \n  <p>alias tomcat-stop='~/tools/tomcat/bin/shutdown.sh'</p>\n</blockquote>\n\n<p>Then execute in terminal</p>\n\n<blockquote>\n  <p>tomcat-start</p>\n</blockquote>\n\n<h2 id=\"tomcatmavenconfiguration\">Tomcat maven configuration</h2>\n\n<p>This part is required for our configuration because we will use tomcat maven plugin to deploy our application into dev-server.Note, that application will be deployed only in case of successful build.</p>\n\n<h2 id=\"tomakethingsworkwewillneed\">To make things work we will need:</h2>\n\n<p>Configuring new user in tomcat. <br />\nAdd server into ~/.m2/settings.xml file on your jenkins machine. This part is needed because Tomcat requires to have such information about server. <br />\nAdding plugin. <br />\nAdding server</p>\n\n<p>This is need to be done to be able to deploy your war into your tomcat.</p>\n\n<p>Login into your dev-server, cd into your tomcat directory and execute:</p>\n\n<p>nano ./conf/tomcat-users.xml <br />\nYou should have something like this</p>\n\n<script src=\"https://gist.github.com/johnyUA/dc18ff0a37a5451fa94f.js\"></script>\n\n<p>Don't forget to restart tomcat</p>\n\n<p>Configuring maven settings.xml</p>\n\n<p>This action should be done in jenkins server.</p>\n\n<p>If maven will deploy our app into tomcat server, then maven should have tomcat credentials. Credentials are stored in your home directory/.m2/settings.xml</p>\n\n<p>If your don't have such file, then create it or execute</p>\n\n<blockquote>\n  <p>nano ~/.m2/settings.xml</p>\n</blockquote>\n\n<p>and add next lines</p>\n\n<script src=\"https://gist.github.com/johnyUA/7bc0f10e14653d8a7640.js\"></script>\n\n<p>id is server name, which will be specified in tomcat plugin.</p>\n\n<h2 id=\"addingplugin\">Adding plugin</h2>\n\n<p>To add tomcat maven plugin , add this in plugins section</p>\n\n<script src=\"https://gist.github.com/johnyUA/99c306ee8e7793536731.js\"></script>\n\n<p>and properties section</p>\n\n<script src=\"https://gist.github.com/johnyUA/1d7fa79fd1b84f91973d.js\"></script>\n\n<p>If you did everything right, then you should be able to deploy your application into your development server</p>\n\n<blockquote>\n  <p>mvn tomcat7:redeploy</p>\n</blockquote>\n\n<p>You should receive build success message.</p>\n\n<h2 id=\"jenkinsconfiguration\">Jenkins configuration</h2>\n\n<p>Before this part I want to say, that I assume that you've already setup your main job, and it is working.</p>\n\n<p>Now, if you did all previous steps, you can start configuring jenkins to make a deployment.</p>\n\n<p>I'm assuming that you have already installed jenkins, added your job without deployment and you are waiting for build setup.In my application I have several modules, where one of them - is module with war packaging, so he is a submodule.</p>\n\n<p><img src=\"/content/images/2015/02/Screen-Shot-2015-01-11-at-4-58-20-PM.png\" alt=\"\" /></p>\n\n<p><strong>is-lnu-rest-api</strong>  is my <strong>webapp</strong>, so it will be the module, where we  will store our tomcat plugin.</p>\n\n<p>Enter your jenkins server, open your job and press configure <br />\n<img src=\"/content/images/2015/02/Screen-Shot-2015-01-11-at-5-02-12-PM.png\" alt=\"\" /></p>\n\n<p>jenkins job</p>\n\n<p>Then go to Build section and press Add post-build step -> Invoke top level maven targets</p>\n\n<p><img src=\"/content/images/2015/02/Screen-Shot-2015-01-11-at-5-06-54-PM-1024x417.png\" alt=\"\" /></p>\n\n<p>Adding top level maven targets</p>\n\n<p>And add appropriate settings</p>\n\n<p><img src=\"/content/images/2015/02/Screen-Shot-2015-01-11-at-5-20-00-PM-1024x517.png\" alt=\"\" /></p>\n\n<p>Now you can start building your application, and if you did everything right - After each commit your server will be redeployed.</p>\n\n<h2 id=\"settingupintegrationtestjob\">Setting up integration test job</h2>\n\n<p>So, what we have? We manage to configure our jenkins job to trigger git changes, run unit tests, make quality checks, and deploy application to some dev-server. We have another thing to do: integration testing. I want to run integration tests each time, when some changes happen. Do that make sense ? My position is that we definetely need to run those integration tests each time when changes happen. That will allow us to detect broken functionality as soon as possible.Of course, you need to have separate qa server for running those tests, but it is worth it.</p>\n\n<p>So, what is the right way for running integration tests with continuous integration tools ? I didn't find exact answer for that, but in my opinion we need to trigger our main build, which will deploy application, and in case of success we should run integration tests.</p>\n\n<p>Go to Jenkins main page, create job, simillar to your first job, but with other Build triggers.</p>\n\n<p><img src=\"/content/images/2015/02/Screen-Shot-2015-01-12-at-9-12-18-PM-1024x260.png\" alt=\"\" /></p>\n\n<p>Choose your main project job.</p>\n\n<p>Note, that you main job should handle project deploy, so, when you will start new job, that will run integration tests, you will be sure, that webapp is deployed and is up to date.</p>\n\n<p>I'm not sure in which way you run your integration tests, but I have separate module for them, so to run them, I have next Build section</p>\n\n<p><img src=\"/content/images/2015/02/Screen-Shot-2015-01-12-at-9-28-58-PM-1024x170.png\" alt=\"\" /></p>\n\n<p>However, there is a chance, that you have separate servers for dev and for qa. For such approach you need to modify second integration job to run tomcat:redeploy on qa server.</p>\n\n<p>After all manipulations, you should have the following functionality:</p>\n\n<p>Your main job will run junit tests, findbugs analys, checkstyle, etc... <br />\nYour main job will deploy war to remote dev server <br />\nYour integration job will run integration tests</p>","image":"/content/images/2015/02/headshot-1.png","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-02-07T22:01:11.000Z","created_by":1,"updated_at":"2015-07-27T12:07:28.000Z","updated_by":1,"published_at":"2015-02-07T22:09:26.000Z","published_by":1},{"id":8,"uuid":"c1480d93-572c-4464-9f64-9c515432f5b2","title":"Java, Jenkins and UnsatisfiedLinkError","slug":"java-jenkins-and-unsatisfiedlinkerror","markdown":"Today I experienced issue with Jenkins - After some system modifitcation I received failed build with message\n\n> Caused by: java.lang.UnsatisfiedLinkError: > /home/${user}/tools/jdk1.7.0_71/jre/lib/i386/xawt/libmawt.so: libXrender.so.1: cannot open shared object file: No such file or directory\n\nSo I search in the internet and found the solution\n\n> sudo apt-get install libxrender-dev\n\n> sudo apt-get install libxtst-dev\n\nI'm really not sure what it was, I guess it could be because I accidentally installed openjdk instead of oracle jdk.\n\nIn fact, It helped me.","mobiledoc":null,"html":"<p>Today I experienced issue with Jenkins - After some system modifitcation I received failed build with message</p>\n\n<blockquote>\n  <p>Caused by: java.lang.UnsatisfiedLinkError: > /home/${user}/tools/jdk1.7.0_71/jre/lib/i386/xawt/libmawt.so: libXrender.so.1: cannot open shared object file: No such file or directory</p>\n</blockquote>\n\n<p>So I search in the internet and found the solution</p>\n\n<blockquote>\n  <p>sudo apt-get install libxrender-dev</p>\n  \n  <p>sudo apt-get install libxtst-dev</p>\n</blockquote>\n\n<p>I'm really not sure what it was, I guess it could be because I accidentally installed openjdk instead of oracle jdk.</p>\n\n<p>In fact, It helped me.</p>","image":"/content/images/2015/02/headshot-2.png","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-02-07T22:22:55.000Z","created_by":1,"updated_at":"2015-07-27T12:07:12.000Z","updated_by":1,"published_at":"2015-02-07T22:23:32.000Z","published_by":1},{"id":9,"uuid":"78ee9e76-62b8-4214-9d22-4c0833a2258c","title":"Gatling integration tool - making more flexibility","slug":"untitled","markdown":"I recently had a chance to investigate how to increase flexibility of my integration test, written on gatling and scala.\n\nI have already written around 50 test cases, and I need couple of improvements:\n\n* Be able to change url of my web service. By default, it should run on localhost, but as soon as I will configure continuous delivery, I will need to change web service url to some remote ip. For that needs I am planning to use maven preferences.\n\n* More precise configuration of my setup function. By this time, I had no assertion strategies and I was confused that my tests go green even if some test case failed.\n \n\n## Dynamic web service url\n\nI was hoping that gatling has something like Spring has - placeholders, but it occurs that the only way I can get property from maven is by system property\n\n> val host = System.getProperty(\"integration.host\")\n\n<script src=\"https://gist.github.com/johnyUA/42c0c8e8963a4443489b.js\"></script>\n\nYou also need to add additional configuration on your gatling plugin in maven\n\n> &lt;jvmArgs> \n&lt;jvmArg>-Dintegration.host=${integration.url}&lt;/jvmArg>\n&lt;/jvmArgs>\n \nWhere ${integration.url} is taken from maven module properties\n\nHere is full pom.xml\n\n<script src=\"https://gist.github.com/johnyUA/d9d51e59a8b2119079ee.js\"></script>\n\nAfter this modification, You will be able to change your basicUrl\n\n> mvn test -Dintegration.url=http:${REMOTE_IP}:8080/....\n\nand my test scenarios will run on dedicated server.\n\nSuch improvement is very important in case if you are using continuous delivery in your daily development.Here is  an example:\n\nYou can configure continuous integration process on your project with two jobs:\n\nOne job will run tests, plugins, builds wars, statistics, etc + use tomcat deploy plugin to deploy your war somewhere.\nSecond job will triggers only if first job will run succesfully. In case of success, second job will run integration tests. And here is the bottleneck, where you will need dynamic web service url. You will configure second job with specific maven instructions for your **REMOTE_IP** - where deployed application from first job will be running.\nGlobal setup configuration\n\nI didn't pay attention to this part of gatling functionality. It occurs for me, that we can configure gatling in the way we want. That's mean that I am free to setup my build in the way of I want, I can configure gatling to fail build only if some percent of requests are failed:\n\n> global.successfulRequests.percent.greaterThan(95)\n\nNote, that by default, every build will be failed only if some compilation errors will occur. In all other cases build will be succesful.\n\nHere is an example of gatling setup\n\n> setUp(...)\n.assertions( global.responseTime.mean.lessThan(50), \nglobal.responseTime.max.between(50, 500),     \nglobal.successfulRequests.count.greaterThan(1500),\nglobal.allRequests.percent.is(100)\n\nHere is full scala object\n\n<script src=\"https://gist.github.com/johnyUA/6417fbc05aa99fa8d89f.js\"></script>\n\nThat's it for today, I hope, in near future I will publish something about integration between jenkins and gatling(gatling has plugin for jenkins :)).","mobiledoc":null,"html":"<p>I recently had a chance to investigate how to increase flexibility of my integration test, written on gatling and scala.</p>\n\n<p>I have already written around 50 test cases, and I need couple of improvements:</p>\n\n<ul>\n<li><p>Be able to change url of my web service. By default, it should run on localhost, but as soon as I will configure continuous delivery, I will need to change web service url to some remote ip. For that needs I am planning to use maven preferences.</p></li>\n<li><p>More precise configuration of my setup function. By this time, I had no assertion strategies and I was confused that my tests go green even if some test case failed.</p></li>\n</ul>\n\n<h2 id=\"dynamicwebserviceurl\">Dynamic web service url</h2>\n\n<p>I was hoping that gatling has something like Spring has - placeholders, but it occurs that the only way I can get property from maven is by system property</p>\n\n<blockquote>\n  <p>val host = System.getProperty(\"integration.host\")</p>\n</blockquote>\n\n<script src=\"https://gist.github.com/johnyUA/42c0c8e8963a4443489b.js\"></script>\n\n<p>You also need to add additional configuration on your gatling plugin in maven</p>\n\n<blockquote>\n  <p>&lt;jvmArgs> \n  &lt;jvmArg>-Dintegration.host=${integration.url}&lt;/jvmArg>\n  &lt;/jvmArgs></p>\n</blockquote>\n\n<p>Where ${integration.url} is taken from maven module properties</p>\n\n<p>Here is full pom.xml</p>\n\n<script src=\"https://gist.github.com/johnyUA/d9d51e59a8b2119079ee.js\"></script>\n\n<p>After this modification, You will be able to change your basicUrl</p>\n\n<blockquote>\n  <p>mvn test -Dintegration.url=http:${REMOTE_IP}:8080/....</p>\n</blockquote>\n\n<p>and my test scenarios will run on dedicated server.</p>\n\n<p>Such improvement is very important in case if you are using continuous delivery in your daily development.Here is  an example:</p>\n\n<p>You can configure continuous integration process on your project with two jobs:</p>\n\n<p>One job will run tests, plugins, builds wars, statistics, etc + use tomcat deploy plugin to deploy your war somewhere. <br />\nSecond job will triggers only if first job will run succesfully. In case of success, second job will run integration tests. And here is the bottleneck, where you will need dynamic web service url. You will configure second job with specific maven instructions for your <strong>REMOTE_IP</strong> - where deployed application from first job will be running. <br />\nGlobal setup configuration</p>\n\n<p>I didn't pay attention to this part of gatling functionality. It occurs for me, that we can configure gatling in the way we want. That's mean that I am free to setup my build in the way of I want, I can configure gatling to fail build only if some percent of requests are failed:</p>\n\n<blockquote>\n  <p>global.successfulRequests.percent.greaterThan(95)</p>\n</blockquote>\n\n<p>Note, that by default, every build will be failed only if some compilation errors will occur. In all other cases build will be succesful.</p>\n\n<p>Here is an example of gatling setup</p>\n\n<blockquote>\n  <p>setUp(...)\n  .assertions( global.responseTime.mean.lessThan(50), \n  global.responseTime.max.between(50, 500), <br />\n  global.successfulRequests.count.greaterThan(1500), <br />\n  global.allRequests.percent.is(100)</p>\n</blockquote>\n\n<p>Here is full scala object</p>\n\n<script src=\"https://gist.github.com/johnyUA/6417fbc05aa99fa8d89f.js\"></script>\n\n<p>That's it for today, I hope, in near future I will publish something about integration between jenkins and gatling(gatling has plugin for jenkins :)).</p>","image":"/content/images/2015/02/picto_big.png","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-02-08T06:09:26.000Z","created_by":1,"updated_at":"2015-07-27T12:06:59.000Z","updated_by":1,"published_at":"2015-02-08T06:21:24.000Z","published_by":1},{"id":10,"uuid":"f552c881-bec7-46d9-94df-7d45869ac437","title":"How to integrate Jenkins with Bitbucket","slug":"how-to-integrate-jenkins-with-bitbucket","markdown":"Recently I had an issue with integrating Jenkins CI with Bitbucket CVS.\nMy goal was to trigger jenkins to start running job, when commit to bitbucket will be pushed.\nSteps I made to made jenkins trigger job:\n\n###### Install Bitbucket plugin\nSet the Jekins job's Build trigger to Poll SCM, but do not specify a schedule\n###### Create a github post-receive trigger to notify the URL\nhttp://yourserver/git/notifyCommit?url=<URL of the Git repository>\nexample\n\nhttp://builds.yoursite.com/git/notifyCommit?url=git@bitbucket.org:nick/project.git\nThis will tell Jenkins to trigger whenever code in bitbucket will be changed.\n\nNote, that Jenkins will check whether changes were made and will start only in case of new changes in repository.","mobiledoc":null,"html":"<p>Recently I had an issue with integrating Jenkins CI with Bitbucket CVS. <br />\nMy goal was to trigger jenkins to start running job, when commit to bitbucket will be pushed. <br />\nSteps I made to made jenkins trigger job:</p>\n\n<h6 id=\"installbitbucketplugin\">Install Bitbucket plugin</h6>\n\n<p>Set the Jekins job's Build trigger to Poll SCM, but do not specify a schedule  </p>\n\n<h6 id=\"createagithubpostreceivetriggertonotifytheurl\">Create a github post-receive trigger to notify the URL</h6>\n\n<p><a href=\"http://yourserver/git/notifyCommit?url=\">http://yourserver/git/notifyCommit?url=</a><URL of the Git repository> <br />\nexample</p>\n\n<p><a href=\"http://builds.yoursite.com/git/notifyCommit?url=git@bitbucket.org:nick/project.git\">http://builds.yoursite.com/git/notifyCommit?url=git@bitbucket.org:nick/project.git</a> <br />\nThis will tell Jenkins to trigger whenever code in bitbucket will be changed.</p>\n\n<p>Note, that Jenkins will check whether changes were made and will start only in case of new changes in repository.</p>","image":"","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-02-08T06:23:34.000Z","created_by":1,"updated_at":"2015-07-27T12:06:50.000Z","updated_by":1,"published_at":"2015-02-08T06:24:51.000Z","published_by":1},{"id":11,"uuid":"1c3ec38e-a515-4839-8540-2d3b5d425de5","title":"Spark - Micro Web Framework for creating web applications with minimal effort","slug":"spark-micro-web-framework-for-creating-web-applications-with-minimal-effort","markdown":"Today I am going to write few words about Spark framework - micro framework for rapid web application development.\n\nRequirements\n\n* Java 8 installed. We're goind to use lambdas.\n* Maven 3.+\n* Eclipse IDE\n* Creating project\n\nNavigate to your project desired location and execute in terminal:\n\n> mvn archetype:generate -DgroupId=org.ivanursul.spark -DartifactId=spark-example -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false\n\nThen open your IDE and import maven project\n\n#### Adding dependency\n\nTo include required libraries just add one dependency\n\n> &lt;dependency>\n    &lt;groupId>com.sparkjava&lt;/groupId>\n    &lt;artifactId>spark-core&lt;/artifactId>\n    &lt;version>2.1&lt;/version>\n&lt;/dependency>\n\nDon't forget to add maven-compiler-plugin\n\n<script src=\"https://gist.github.com/johnyUA/0874e5239adc82dee517.js\"></script>\n\nNow you are ready to write some code.\n\nOfficial site says, that it is enough to write something like\n\n\n\timport static spark.Spark.*;\n    public class HelloWorld {\n        public static void main(String[] args) {\n            get(\"/hello\", (req, res) -> \"Hello World\");\n        }\n    }\n    \nand after execution of program you car test this app by typing\n\nhttp://localhost:4567/hello\nSeems to be very easy, but let's try something harder.\n\n#### Implementing CRUD operations\n\nTo implement CRUD(create, remove, update, delete) for, say, person entity, we will gonna need:\n\nDTO(Person)\nSerializer/Deserializer(GSON? :) )\nNote, that I will do it as simple, as I can, without any structures, just for testing this framework.\n\n#### Let's start\n\nAdd gson dependency\n\n> &lt;dependency>\n &lt;groupId>com.google.code.gson&lt;/groupId>\n &lt;artifactId>gson&lt;/artifactId>\n &lt;version>2.3.1&lt;/version>\n&lt;/dependency>\n\n**Create person class**\n\n<script src=\"https://gist.github.com/johnyUA/daf4ad2580eed421fb7f.js\"></script>\n\n**Create main class**\n\n<script src=\"https://gist.github.com/johnyUA/6d918d4c72188844e9aa.js\"></script>\n\nThat seems to be all, that is needed.\n\nWhat did we made ?\n\n* Create method with POST http method\n* Update method with PUT http method\n* Get method with GET http method\n* Delete method with DELETE http method\n* We created a static Map to store persons\n* We used gson to serialize/deserialize data\n\n#### How to test?\n\nIf you have Postman extension for chrome, you can try to import my collection for this tutorial\n\nhttps://www.getpostman.com/collections/071a319d71ab1a639c69\n\n#### Impressions\n\nReally, I see, that this framework is very easy to learn, easy to start, but not easy to maintain.\n\nFor quick POC's this framework is quite good, but for something really big and scalable - I think not.\n\nThe problem will occur, when I will try to change something, then I will notice, that there are static fields, classes, everywhere, and It will be very hard to maintain such code.\n\n#### Resources\n\nYou can find github project [here](https://github.com/johnyUA/spark-getting-started).","mobiledoc":null,"html":"<p>Today I am going to write few words about Spark framework - micro framework for rapid web application development.</p>\n\n<p>Requirements</p>\n\n<ul>\n<li>Java 8 installed. We're goind to use lambdas.</li>\n<li>Maven 3.+</li>\n<li>Eclipse IDE</li>\n<li>Creating project</li>\n</ul>\n\n<p>Navigate to your project desired location and execute in terminal:</p>\n\n<blockquote>\n  <p>mvn archetype:generate -DgroupId=org.ivanursul.spark -DartifactId=spark-example -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false</p>\n</blockquote>\n\n<p>Then open your IDE and import maven project</p>\n\n<h4 id=\"addingdependency\">Adding dependency</h4>\n\n<p>To include required libraries just add one dependency</p>\n\n<blockquote>\n  <p>&lt;dependency>\n      &lt;groupId>com.sparkjava&lt;/groupId>\n      &lt;artifactId>spark-core&lt;/artifactId>\n      &lt;version>2.1&lt;/version>\n  &lt;/dependency></p>\n</blockquote>\n\n<p>Don't forget to add maven-compiler-plugin</p>\n\n<script src=\"https://gist.github.com/johnyUA/0874e5239adc82dee517.js\"></script>\n\n<p>Now you are ready to write some code.</p>\n\n<p>Official site says, that it is enough to write something like</p>\n\n<pre><code>import static spark.Spark.*;\npublic class HelloWorld {\n    public static void main(String[] args) {\n        get(\"/hello\", (req, res) -&gt; \"Hello World\");\n    }\n}\n</code></pre>\n\n<p>and after execution of program you car test this app by typing</p>\n\n<p><a href=\"http://localhost:4567/hello\">http://localhost:4567/hello</a> <br />\nSeems to be very easy, but let's try something harder.</p>\n\n<h4 id=\"implementingcrudoperations\">Implementing CRUD operations</h4>\n\n<p>To implement CRUD(create, remove, update, delete) for, say, person entity, we will gonna need:</p>\n\n<p>DTO(Person) <br />\nSerializer/Deserializer(GSON? :) ) <br />\nNote, that I will do it as simple, as I can, without any structures, just for testing this framework.</p>\n\n<h4 id=\"letsstart\">Let's start</h4>\n\n<p>Add gson dependency</p>\n\n<blockquote>\n  <p>&lt;dependency>\n   &lt;groupId>com.google.code.gson&lt;/groupId>\n   &lt;artifactId>gson&lt;/artifactId>\n   &lt;version>2.3.1&lt;/version>\n  &lt;/dependency></p>\n</blockquote>\n\n<p><strong>Create person class</strong></p>\n\n<script src=\"https://gist.github.com/johnyUA/daf4ad2580eed421fb7f.js\"></script>\n\n<p><strong>Create main class</strong></p>\n\n<script src=\"https://gist.github.com/johnyUA/6d918d4c72188844e9aa.js\"></script>\n\n<p>That seems to be all, that is needed.</p>\n\n<p>What did we made ?</p>\n\n<ul>\n<li>Create method with POST http method</li>\n<li>Update method with PUT http method</li>\n<li>Get method with GET http method</li>\n<li>Delete method with DELETE http method</li>\n<li>We created a static Map to store persons</li>\n<li>We used gson to serialize/deserialize data</li>\n</ul>\n\n<h4 id=\"howtotest\">How to test?</h4>\n\n<p>If you have Postman extension for chrome, you can try to import my collection for this tutorial</p>\n\n<p><a href=\"https://www.getpostman.com/collections/071a319d71ab1a639c69\">https://www.getpostman.com/collections/071a319d71ab1a639c69</a></p>\n\n<h4 id=\"impressions\">Impressions</h4>\n\n<p>Really, I see, that this framework is very easy to learn, easy to start, but not easy to maintain.</p>\n\n<p>For quick POC's this framework is quite good, but for something really big and scalable - I think not.</p>\n\n<p>The problem will occur, when I will try to change something, then I will notice, that there are static fields, classes, everywhere, and It will be very hard to maintain such code.</p>\n\n<h4 id=\"resources\">Resources</h4>\n\n<p>You can find github project <a href=\"https://github.com/johnyUA/spark-getting-started\">here</a>.</p>","image":"","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-02-08T07:12:21.000Z","created_by":1,"updated_at":"2015-07-27T12:06:30.000Z","updated_by":1,"published_at":"2015-02-08T07:19:14.000Z","published_by":1},{"id":12,"uuid":"e1c20eb1-942b-48a8-a20d-a4823c98a6f8","title":"Getting in touch with Liquibase Migration Tool","slug":"getting-started-liquibase","markdown":"Have you experienced a situation, when you had a database in your system, and you needed to migrate all it's structure, to, say, another database ? Of course, you faced such situations and, of course, you manually copied all data from one server instance to another ? Something familiar ?\n\n#### Alternative approach\n\nHow about idea of writing high-level instructions on how to deal with your database ? For example, you say \"Create this table, and add this column , and insert this index, ...\" only once and all subsequent times you will just reuse this instructions ?\n\nSuch idea is called \"Database migration tool\", there is several migration frameworks for java, but I will show how to work with single tool - liquibase.\n\nThe idea is that you store this instructions in xml files and when you run your liquibase migration script for the first time, liquibase will create two tables in your database  databasechangelog and databasechangeloglock, where all information about running migrations kept.\n\nSay, you have only one migration changeset\n\n<script src=\"https://gist.github.com/johnyUA/a88227a4917741539e23.js\"></script>\n\nTo run such script I use maven, here is pom.xml,as usual\n\n<script src=\"https://gist.github.com/johnyUA/22da0b907b1ca7dca765.js\"></script>\n\n> mvn liquibase:update\n\nIf you wish, you can run liquibase from command line, the only thing you need is to set required parameters\n\n> java -jar liquibase.jar --driver=com.mysql.jdbc.Driver \\\n     --classpath=/path/to/classes \\\n     --changeLogFile=com/example/db.changelog.xml \\\n     --url=\"jdbc:mysql://localhost/example\" \\\n     --username=user \\\n     --password=asdf \\\n     update\n     \n#### Must know\n\nLiquibase migrations are running top-down, so consider writing your scripts  knowing this fact. This is important. For example, if you have tableA with foreign key of tableB, and table B with foreignKey of tableA, in case of ignorance if this fact you will write your migration with following way\n\n<script src=\"https://gist.github.com/johnyUA/d3fca83fbe8c35efb11d.js\"></script>\n\nAnd this migration will fail, because during the execution of first migration there will be no tableB.\nTo Solve this sort of issue you can do\n\n<script src=\"https://gist.github.com/johnyUA/52a88ca4c33ab2066830.js\"></script>\n\nEach migration unit is being executed in transaction mode. That mean, that this unit will execute fully without exceptions, or will not be executed at all.\nYou can generate migrations from designed database - just execute\n\n> mvn liquibase:generateChangeLog\n\n#### Suggestions\n\nHere is some suggestions on how to use liquibase:\n\nYou split your liquibase migrations into different parts\nEach global part should support appropriate version of your system.Say, you have version 1.0, 1.1 , and 1.2, so there should be 3 different folders with version 1.0, 1.1 and 1.2. Pay attention, that after executing folder with version 1.2 your system should have all the changes from version 1.0 and 1.1.\nEach folder with version should have a cumulative file, which will import all migration files from that folder(data structure migrations, data migrations, etc...)\nFollow your own conventions, as usual. If you decided to have some default structure of your migrations, then support this structure.\nConclusions\n\nI am using liquibase for around 1 year and during this time I can confidently say, that this is very poweful tool. It can simplify your life, you will forget about problems with synchronizing your database versions.","mobiledoc":null,"html":"<p>Have you experienced a situation, when you had a database in your system, and you needed to migrate all it's structure, to, say, another database ? Of course, you faced such situations and, of course, you manually copied all data from one server instance to another ? Something familiar ?</p>\n\n<h4 id=\"alternativeapproach\">Alternative approach</h4>\n\n<p>How about idea of writing high-level instructions on how to deal with your database ? For example, you say \"Create this table, and add this column , and insert this index, ...\" only once and all subsequent times you will just reuse this instructions ?</p>\n\n<p>Such idea is called \"Database migration tool\", there is several migration frameworks for java, but I will show how to work with single tool - liquibase.</p>\n\n<p>The idea is that you store this instructions in xml files and when you run your liquibase migration script for the first time, liquibase will create two tables in your database  databasechangelog and databasechangeloglock, where all information about running migrations kept.</p>\n\n<p>Say, you have only one migration changeset</p>\n\n<script src=\"https://gist.github.com/johnyUA/a88227a4917741539e23.js\"></script>\n\n<p>To run such script I use maven, here is pom.xml,as usual</p>\n\n<script src=\"https://gist.github.com/johnyUA/22da0b907b1ca7dca765.js\"></script>\n\n<blockquote>\n  <p>mvn liquibase:update</p>\n</blockquote>\n\n<p>If you wish, you can run liquibase from command line, the only thing you need is to set required parameters</p>\n\n<blockquote>\n  <p>java -jar liquibase.jar --driver=com.mysql.jdbc.Driver \\\n       --classpath=/path/to/classes \\\n       --changeLogFile=com/example/db.changelog.xml \\\n       --url=\"jdbc:mysql://localhost/example\" \\\n       --username=user \\\n       --password=asdf \\\n       update</p>\n</blockquote>\n\n<h4 id=\"mustknow\">Must know</h4>\n\n<p>Liquibase migrations are running top-down, so consider writing your scripts  knowing this fact. This is important. For example, if you have tableA with foreign key of tableB, and table B with foreignKey of tableA, in case of ignorance if this fact you will write your migration with following way</p>\n\n<script src=\"https://gist.github.com/johnyUA/d3fca83fbe8c35efb11d.js\"></script>\n\n<p>And this migration will fail, because during the execution of first migration there will be no tableB. <br />\nTo Solve this sort of issue you can do</p>\n\n<script src=\"https://gist.github.com/johnyUA/52a88ca4c33ab2066830.js\"></script>\n\n<p>Each migration unit is being executed in transaction mode. That mean, that this unit will execute fully without exceptions, or will not be executed at all. <br />\nYou can generate migrations from designed database - just execute</p>\n\n<blockquote>\n  <p>mvn liquibase:generateChangeLog</p>\n</blockquote>\n\n<h4 id=\"suggestions\">Suggestions</h4>\n\n<p>Here is some suggestions on how to use liquibase:</p>\n\n<p>You split your liquibase migrations into different parts <br />\nEach global part should support appropriate version of your system.Say, you have version 1.0, 1.1 , and 1.2, so there should be 3 different folders with version 1.0, 1.1 and 1.2. Pay attention, that after executing folder with version 1.2 your system should have all the changes from version 1.0 and 1.1. <br />\nEach folder with version should have a cumulative file, which will import all migration files from that folder(data structure migrations, data migrations, etc...) <br />\nFollow your own conventions, as usual. If you decided to have some default structure of your migrations, then support this structure. <br />\nConclusions</p>\n\n<p>I am using liquibase for around 1 year and during this time I can confidently say, that this is very poweful tool. It can simplify your life, you will forget about problems with synchronizing your database versions.</p>","image":"","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-02-08T07:20:16.000Z","created_by":1,"updated_at":"2015-07-27T12:06:18.000Z","updated_by":1,"published_at":"2015-02-08T07:24:04.000Z","published_by":1},{"id":13,"uuid":"81897b30-3231-473e-939b-4ae88b267bdb","title":"Testing RESTful CRUD Web Service with Gatling Tool","slug":"untitled-2","markdown":"If you follow the right way of designing your web app, and choosed RESTful architecture, then it should be easy for you to test your appplication. By the REST style, you should implement CRUD operations for each entity in your application. What does it mean:\n\nYou should be able to Create,Read, Update, Delete(CRUD) your entities.\nYou should provide statuses for each operations. For example, for Create operation you should have 201 status in case of succesful operation.\nYou application must have good exception handling. For example, if you will make GET request for retrieving some entity called person with id - 1, and there will be no such record in your storage with this id, then you need to tell your user, that there is no such person, and set status to 404(Not found)\nSo, given that you will keep in mind all information above and will design your app with such instructions.\n\nNow it's time to write some tests. Which framework you should choose?\n\nI choosed Gatling framework. Reasons:\n\n* It runs on jvm\n* It has maven module\n\n#### Configuration\n\nAs I mentioned above, I will use maven for building my app.\n\nSo, I will create a separate module for running my integration tests.\n\n![](/content/images/2015/02/Screen-Shot-2015-01-03-at-4-33-55-PM.png)\n\nGatling integration module\n\nwith jar packaging\n\nSetup you pom.xml like in below:\n\n<script src=\"https://gist.github.com/johnyUA/c96c985e54e5168bc3ea.js\"></script>\n\n#### Scala IDE\n\nTo write tests with gatling, you need to have IDE with scala support.I choosed Scala ready IDE. You can download it here. After scala installation, Right click on project -> Configure - > Add Scala Nature.\n\nThen create Source Folder in src/test/scala.\n\nNow you are ready to write tests.\n\n#### Creating first test\n\nTo have scalable structure, create a Scala class - org.lnu.is.integration.IntegrationTest.\n\n<script src=\"https://gist.github.com/johnyUA/d1c2ab901d5f827980a0.js\"></script>\n\nThis class will be entry point for all future tests.\n\nNow let's create integration test for person(just an example), note that it should be object, not a class - it's a special singleton construction in scala.\n\n<script src=\"https://gist.github.com/johnyUA/32c4d07f9170ee8aae12.js\"></script>\n\nEdit your **IntegrationTest** class\n\n<script src=\"https://gist.github.com/johnyUA/f9754f16384df60088ad.js\"></script>\n\n#### Adding resources\n\nNote, that all data(json templates, etc..) are store in data folder\n\ndata/person/post.json:\n\n<script src=\"https://gist.github.com/johnyUA/d8f7f01f4317b4c3ca22.js\"></script>\n\ndata/person/put.json:\n\n<script src=\"https://gist.github.com/johnyUA/dc8d00afb9bed69ef34c.js\"></script>\n\n#### Describing scenario\n\nThe idea of scenario is:\n\n* Call POST method to create new person, get generated identifier.\n* Call GET method /persons/{identifier} and make sure that there is such record in the system and response has status 200(OK).\n* Call PUT method to update some columns.\n* Call GET method to make sure, that columns from above method are updated.\n* Call DELETE method\n* Call GET method and make sure, that status is 404(Not Found)\n\nGenerating custom values\n\nThere are cases, where you need to generate unique value for each request.\n\nFor example, some person identifier needs to be unique.\n\nFor solving such issue I use session configuration:\n\n    .exec(session => {\n      session\n        .set(\"idnum\", UUID.randomUUID())})\nand in json file I just make a placeholder:\n\n> \"identifier\": \"${idnum}\",\n\nThat seems to be all, that you need .\n\n#### Running test\n\nStart your server, open terminal, go to your integration module folder and execute\n\n> mvn clean install\n\nWatch the results.","mobiledoc":null,"html":"<p>If you follow the right way of designing your web app, and choosed RESTful architecture, then it should be easy for you to test your appplication. By the REST style, you should implement CRUD operations for each entity in your application. What does it mean:</p>\n\n<p>You should be able to Create,Read, Update, Delete(CRUD) your entities. <br />\nYou should provide statuses for each operations. For example, for Create operation you should have 201 status in case of succesful operation. <br />\nYou application must have good exception handling. For example, if you will make GET request for retrieving some entity called person with id - 1, and there will be no such record in your storage with this id, then you need to tell your user, that there is no such person, and set status to 404(Not found) <br />\nSo, given that you will keep in mind all information above and will design your app with such instructions.</p>\n\n<p>Now it's time to write some tests. Which framework you should choose?</p>\n\n<p>I choosed Gatling framework. Reasons:</p>\n\n<ul>\n<li>It runs on jvm</li>\n<li>It has maven module</li>\n</ul>\n\n<h4 id=\"configuration\">Configuration</h4>\n\n<p>As I mentioned above, I will use maven for building my app.</p>\n\n<p>So, I will create a separate module for running my integration tests.</p>\n\n<p><img src=\"/content/images/2015/02/Screen-Shot-2015-01-03-at-4-33-55-PM.png\" alt=\"\" /></p>\n\n<p>Gatling integration module</p>\n\n<p>with jar packaging</p>\n\n<p>Setup you pom.xml like in below:</p>\n\n<script src=\"https://gist.github.com/johnyUA/c96c985e54e5168bc3ea.js\"></script>\n\n<h4 id=\"scalaide\">Scala IDE</h4>\n\n<p>To write tests with gatling, you need to have IDE with scala support.I choosed Scala ready IDE. You can download it here. After scala installation, Right click on project -> Configure - > Add Scala Nature.</p>\n\n<p>Then create Source Folder in src/test/scala.</p>\n\n<p>Now you are ready to write tests.</p>\n\n<h4 id=\"creatingfirsttest\">Creating first test</h4>\n\n<p>To have scalable structure, create a Scala class - org.lnu.is.integration.IntegrationTest.</p>\n\n<script src=\"https://gist.github.com/johnyUA/d1c2ab901d5f827980a0.js\"></script>\n\n<p>This class will be entry point for all future tests.</p>\n\n<p>Now let's create integration test for person(just an example), note that it should be object, not a class - it's a special singleton construction in scala.</p>\n\n<script src=\"https://gist.github.com/johnyUA/32c4d07f9170ee8aae12.js\"></script>\n\n<p>Edit your <strong>IntegrationTest</strong> class</p>\n\n<script src=\"https://gist.github.com/johnyUA/f9754f16384df60088ad.js\"></script>\n\n<h4 id=\"addingresources\">Adding resources</h4>\n\n<p>Note, that all data(json templates, etc..) are store in data folder</p>\n\n<p>data/person/post.json:</p>\n\n<script src=\"https://gist.github.com/johnyUA/d8f7f01f4317b4c3ca22.js\"></script>\n\n<p>data/person/put.json:</p>\n\n<script src=\"https://gist.github.com/johnyUA/dc8d00afb9bed69ef34c.js\"></script>\n\n<h4 id=\"describingscenario\">Describing scenario</h4>\n\n<p>The idea of scenario is:</p>\n\n<ul>\n<li>Call POST method to create new person, get generated identifier.</li>\n<li>Call GET method /persons/{identifier} and make sure that there is such record in the system and response has status 200(OK).</li>\n<li>Call PUT method to update some columns.</li>\n<li>Call GET method to make sure, that columns from above method are updated.</li>\n<li>Call DELETE method</li>\n<li>Call GET method and make sure, that status is 404(Not Found)</li>\n</ul>\n\n<p>Generating custom values</p>\n\n<p>There are cases, where you need to generate unique value for each request.</p>\n\n<p>For example, some person identifier needs to be unique.</p>\n\n<p>For solving such issue I use session configuration:</p>\n\n<pre><code>.exec(session =&gt; {\n  session\n    .set(\"idnum\", UUID.randomUUID())})\n</code></pre>\n\n<p>and in json file I just make a placeholder:</p>\n\n<blockquote>\n  <p>\"identifier\": \"${idnum}\",</p>\n</blockquote>\n\n<p>That seems to be all, that you need .</p>\n\n<h4 id=\"runningtest\">Running test</h4>\n\n<p>Start your server, open terminal, go to your integration module folder and execute</p>\n\n<blockquote>\n  <p>mvn clean install</p>\n</blockquote>\n\n<p>Watch the results.</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-02-08T07:24:30.000Z","created_by":1,"updated_at":"2015-07-27T12:05:58.000Z","updated_by":1,"published_at":"2015-02-08T07:30:45.000Z","published_by":1},{"id":14,"uuid":"1bd28868-9fc1-4559-ad24-715aeded442d","title":"Develop your Angular.js application with Yeoman, Grunt and Bower","slug":"getting-started-angular","markdown":"I position myself as backend developer, but I truly undestand, that I should know more about frontend world. So, by this post, I will start investigating frontend world.\n\nThis time I will try angular.js - popular framework for building responsive web applications.\n\nNotice, that raw angular.js withour any thirdparty libraries is not so powerful, so I will be using Yeoman, Grunt and Bower in this article for convenient work.\n\n#### Expectation\n\nThis tutorial will cover:\n\n* Generating of bare angular.js project.\n* Use Grunt to make testing\n* Use Grunt to deploy application\n* Use Bower to add thirdparty plugins\n* Making some little changes to our application.\n\nI assume that you have already installed npm package manager.\n\n#### Prerequisites\n\nTo be confident during this tutorial I recommend you to have the following skills and resources available:\n\nBasic knowledge of command line\nnode.js and npm\nKnowledge of HTML, CSS, JS\nReady, and tested Vanilla JS framework :D (Just joke)\nResources\n\nYou can find this project on his GitHub page\n\n#### Installation\n\nBefore installation, I should say some words about setting up npm. When I first downloaded and installed npm on OS X and tried to configure project from various tutorials - I received plenty of permission exceptions in my terminal. So, I recommend to make changes, mentioned below in the link\n\nFixing npm permissions\n\nAfter that, you can start.In terminal, run:\n\n> npm install -g yo grunt-cli bower\n\nThis command should install yo, grunt and bower. Right now, we have a powerful set of instruments, that will help us to build our demo application.\n\nI will write couple of words about each framework.\n\n#### Yeoman\n\nYeoman helps you to kickstart new projects, prescribing best practices and tools to help you stay productive.\n\nYou should read more on their official site - yeoman.io\n\n#### YO\n\nYo is a CLI too for running yeoman generators\n\nGithub page - github.com/yeoman/yo\n\n#### Bower\n\nBower works by fetching and installing packages from all over, taking care of hunting, finding, downloading, and saving the stuff youre looking for. Bower keeps track of these packages in a manifest file, bower.json. How you use packages is up to you. Bower provides hooks to facilitate using packages in your tools and workflows.\n\nOfficial site - bower.io\n\nOne of the best features of Yeoman is the ability to use custom generators. Were going to intall the AngularJS generator to help us get up and running with Angular as quick as possible.\n\nRun the following to install the AngularJS generator:\n\n> npm install -g generator-angular\n\n> bower install angular-bootstrap --save\n\nNow its time to generate a shiny new AngularJS application. In a fresh project directory, run:\n\n#### yo angular\n\nGenerator will ask you couple of questions, I choosed Bootstrap and accept default settings for packages.\n\n##### What is generated?\n\nLets take a look at what Yeomans given us:\n\n* .bowerrc\n* .editorconfig\n* .gitattributes\n* .gitignore\n* .jshintrc\n* Gruntfile.js\n* app/\n* bower.json\n* node_modules/\n* package.json\n* test/\n* test/karma.conf.js\n\nLets go over some of the more important things to notice here:\n\n###### app/ directory\nThe app directory contains your static app. It has your html, css and javascript in it and its where youll spend most of your time developing.\n\n###### package.json\n\nThe package.json file helps npm to identify our project as well as to manage all of its dependencies. It can also contain all sorts of other metadata relevant to your project.\n\n###### node_modules\n\nThis one is self explanatory. This is where all the node modules that your project depends on are stored.\n###### Gruntfile.js\nThe Gruntfile is a javascript file that is responsible for configuring your project as well as any tasks or plugins that your project requires. For instance, your gruntfile might specify that your project uses Uglify and that you want it to run uglify on a particular directory at build time. More about Grunt in a moment.\n###### bower.json\nThe component.json file is used to inform the Bower package manager of your projects dependencies as well as other metadata.\n###### .bowerrc\nThe .bowerrc file is used to pass general config options to bower.\n###### Karma files\nKarma is a testing framework. Well use it to run a few tests for our Angular app. All karma files are stored in test/ folder.\nI agree, there's a lot of  folders and configuration files   but once you get more familiar with the output of a yeoman generator however, youll realize that all this frameworks handles most of the work for you.\n\nEverything, what can be automazied - need to be automized, because it will ruin you and you will waste your expensive time.\n\nLets add a few more things to our project before we start get on to some actual development.\n\n###### app folder\n\nLet's investigrate out app/ directory.\n\n* scripts/\n* styles/\n* views/\n* favicon.ico\n* index.html\n* robots.txt\n* index.html\n\nThis should be familiar to most of you, this is the core html page of your app.\nscripts/ directory\nAgain, familiar territory to most. This is where your apps javascript is stored. Note that libraries such as AngularJS will exist in the components directory, so scripts/ is for your files that you write!\nstyles/ directory\nAll your css/sass to make your app look moar pretty.\nviews\nThis nifty folder is where your Angular Templates will reside.\n\n###### Angular JS\n\nSo, let's take a close look at angular.js architecture:\n\nI worked with backbone.js - similar framework, and there was concept of controller(route), view(as view class) and template.\n\nHere I can see, that we have concept of module, controller and view\n\nThe Module: **/app/scripts/app.js**\n\n<script src=\"https://gist.github.com/johnyUA/535a65510a0d4e135f7d.js\"></script>\n\n\n\nThe Controller: **/app/scripts/controllers/main.js**\n\n<script src=\"https://gist.github.com/johnyUA/fc76a989209aab170d79.js\"></script>\n\n \n\nThe View: app/views/main.html\n\n<script src=\"https://gist.github.com/johnyUA/58dfaf31561e61399152.js\"></script>\n\n**Start the server**\n\nIt seems that grunt has it's own embedded server, that runs on 9000 port\n\nto start server, execute\n\n**grunt serve**\n\nThen you should get following page\n\n![](/content/images/2015/02/Screen-Shot-2015-01-02-at-3-54-15-PM-1024x516.png)\n\n \n\n#### Running tests\n\nAs I mentioned before, you can run tests\n\ngrunt test\n\nyou should get\n\n![](/content/images/2015/02/Screen-Shot-2015-01-02-at-3-58-43-PM-1024x640.png)\n\nTest for controller looks like this\n\n<script src=\"https://gist.github.com/johnyUA/cde564974557c808a749.js\"></script>\n\n \n\n#### Conclusion\n\nAs far, as I can make some conclusion, I can say, that this framework looks pretty good. I hope, I will write some article about continuous integration of such applications.\n","mobiledoc":null,"html":"<p>I position myself as backend developer, but I truly undestand, that I should know more about frontend world. So, by this post, I will start investigating frontend world.</p>\n\n<p>This time I will try angular.js - popular framework for building responsive web applications.</p>\n\n<p>Notice, that raw angular.js withour any thirdparty libraries is not so powerful, so I will be using Yeoman, Grunt and Bower in this article for convenient work.</p>\n\n<h4 id=\"expectation\">Expectation</h4>\n\n<p>This tutorial will cover:</p>\n\n<ul>\n<li>Generating of bare angular.js project.</li>\n<li>Use Grunt to make testing</li>\n<li>Use Grunt to deploy application</li>\n<li>Use Bower to add thirdparty plugins</li>\n<li>Making some little changes to our application.</li>\n</ul>\n\n<p>I assume that you have already installed npm package manager.</p>\n\n<h4 id=\"prerequisites\">Prerequisites</h4>\n\n<p>To be confident during this tutorial I recommend you to have the following skills and resources available:</p>\n\n<p>Basic knowledge of command line <br />\nnode.js and npm <br />\nKnowledge of HTML, CSS, JS <br />\nReady, and tested Vanilla JS framework :D (Just joke) <br />\nResources</p>\n\n<p>You can find this project on his GitHub page</p>\n\n<h4 id=\"installation\">Installation</h4>\n\n<p>Before installation, I should say some words about setting up npm. When I first downloaded and installed npm on OS X and tried to configure project from various tutorials - I received plenty of permission exceptions in my terminal. So, I recommend to make changes, mentioned below in the link</p>\n\n<p>Fixing npm permissions</p>\n\n<p>After that, you can start.In terminal, run:</p>\n\n<blockquote>\n  <p>npm install -g yo grunt-cli bower</p>\n</blockquote>\n\n<p>This command should install yo, grunt and bower. Right now, we have a powerful set of instruments, that will help us to build our demo application.</p>\n\n<p>I will write couple of words about each framework.</p>\n\n<h4 id=\"yeoman\">Yeoman</h4>\n\n<p>Yeoman helps you to kickstart new projects, prescribing best practices and tools to help you stay productive.</p>\n\n<p>You should read more on their official site - yeoman.io</p>\n\n<h4 id=\"yo\">YO</h4>\n\n<p>Yo is a CLI too for running yeoman generators</p>\n\n<p>Github page - github.com/yeoman/yo</p>\n\n<h4 id=\"bower\">Bower</h4>\n\n<p>Bower works by fetching and installing packages from all over, taking care of hunting, finding, downloading, and saving the stuff youre looking for. Bower keeps track of these packages in a manifest file, bower.json. How you use packages is up to you. Bower provides hooks to facilitate using packages in your tools and workflows.</p>\n\n<p>Official site - bower.io</p>\n\n<p>One of the best features of Yeoman is the ability to use custom generators. Were going to intall the AngularJS generator to help us get up and running with Angular as quick as possible.</p>\n\n<p>Run the following to install the AngularJS generator:</p>\n\n<blockquote>\n  <p>npm install -g generator-angular</p>\n  \n  <p>bower install angular-bootstrap --save</p>\n</blockquote>\n\n<p>Now its time to generate a shiny new AngularJS application. In a fresh project directory, run:</p>\n\n<h4 id=\"yoangular\">yo angular</h4>\n\n<p>Generator will ask you couple of questions, I choosed Bootstrap and accept default settings for packages.</p>\n\n<h5 id=\"whatisgenerated\">What is generated?</h5>\n\n<p>Lets take a look at what Yeomans given us:</p>\n\n<ul>\n<li>.bowerrc</li>\n<li>.editorconfig</li>\n<li>.gitattributes</li>\n<li>.gitignore</li>\n<li>.jshintrc</li>\n<li>Gruntfile.js</li>\n<li>app/</li>\n<li>bower.json</li>\n<li>node_modules/</li>\n<li>package.json</li>\n<li>test/</li>\n<li>test/karma.conf.js</li>\n</ul>\n\n<p>Lets go over some of the more important things to notice here:</p>\n\n<h6 id=\"appdirectory\">app/ directory</h6>\n\n<p>The app directory contains your static app. It has your html, css and javascript in it and its where youll spend most of your time developing.</p>\n\n<h6 id=\"packagejson\">package.json</h6>\n\n<p>The package.json file helps npm to identify our project as well as to manage all of its dependencies. It can also contain all sorts of other metadata relevant to your project.</p>\n\n<h6 id=\"node_modules\">node_modules</h6>\n\n<p>This one is self explanatory. This is where all the node modules that your project depends on are stored.  </p>\n\n<h6 id=\"gruntfilejs\">Gruntfile.js</h6>\n\n<p>The Gruntfile is a javascript file that is responsible for configuring your project as well as any tasks or plugins that your project requires. For instance, your gruntfile might specify that your project uses Uglify and that you want it to run uglify on a particular directory at build time. More about Grunt in a moment.  </p>\n\n<h6 id=\"bowerjson\">bower.json</h6>\n\n<p>The component.json file is used to inform the Bower package manager of your projects dependencies as well as other metadata.  </p>\n\n<h6 id=\"bowerrc\">.bowerrc</h6>\n\n<p>The .bowerrc file is used to pass general config options to bower.  </p>\n\n<h6 id=\"karmafiles\">Karma files</h6>\n\n<p>Karma is a testing framework. Well use it to run a few tests for our Angular app. All karma files are stored in test/ folder. <br />\nI agree, there's a lot of  folders and configuration files   but once you get more familiar with the output of a yeoman generator however, youll realize that all this frameworks handles most of the work for you.</p>\n\n<p>Everything, what can be automazied - need to be automized, because it will ruin you and you will waste your expensive time.</p>\n\n<p>Lets add a few more things to our project before we start get on to some actual development.</p>\n\n<h6 id=\"appfolder\">app folder</h6>\n\n<p>Let's investigrate out app/ directory.</p>\n\n<ul>\n<li>scripts/</li>\n<li>styles/</li>\n<li>views/</li>\n<li>favicon.ico</li>\n<li>index.html</li>\n<li>robots.txt</li>\n<li>index.html</li>\n</ul>\n\n<p>This should be familiar to most of you, this is the core html page of your app. <br />\nscripts/ directory <br />\nAgain, familiar territory to most. This is where your apps javascript is stored. Note that libraries such as AngularJS will exist in the components directory, so scripts/ is for your files that you write! <br />\nstyles/ directory <br />\nAll your css/sass to make your app look moar pretty. <br />\nviews <br />\nThis nifty folder is where your Angular Templates will reside.</p>\n\n<h6 id=\"angularjs\">Angular JS</h6>\n\n<p>So, let's take a close look at angular.js architecture:</p>\n\n<p>I worked with backbone.js - similar framework, and there was concept of controller(route), view(as view class) and template.</p>\n\n<p>Here I can see, that we have concept of module, controller and view</p>\n\n<p>The Module: <strong>/app/scripts/app.js</strong></p>\n\n<script src=\"https://gist.github.com/johnyUA/535a65510a0d4e135f7d.js\"></script>\n\n<p>The Controller: <strong>/app/scripts/controllers/main.js</strong></p>\n\n<script src=\"https://gist.github.com/johnyUA/fc76a989209aab170d79.js\"></script>\n\n<p>The View: app/views/main.html</p>\n\n<script src=\"https://gist.github.com/johnyUA/58dfaf31561e61399152.js\"></script>\n\n<p><strong>Start the server</strong></p>\n\n<p>It seems that grunt has it's own embedded server, that runs on 9000 port</p>\n\n<p>to start server, execute</p>\n\n<p><strong>grunt serve</strong></p>\n\n<p>Then you should get following page</p>\n\n<p><img src=\"/content/images/2015/02/Screen-Shot-2015-01-02-at-3-54-15-PM-1024x516.png\" alt=\"\" /></p>\n\n<h4 id=\"runningtests\">Running tests</h4>\n\n<p>As I mentioned before, you can run tests</p>\n\n<p>grunt test</p>\n\n<p>you should get</p>\n\n<p><img src=\"/content/images/2015/02/Screen-Shot-2015-01-02-at-3-58-43-PM-1024x640.png\" alt=\"\" /></p>\n\n<p>Test for controller looks like this</p>\n\n<script src=\"https://gist.github.com/johnyUA/cde564974557c808a749.js\"></script>\n\n<h4 id=\"conclusion\">Conclusion</h4>\n\n<p>As far, as I can make some conclusion, I can say, that this framework looks pretty good. I hope, I will write some article about continuous integration of such applications.</p>","image":"","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-02-08T07:32:29.000Z","created_by":1,"updated_at":"2015-07-27T12:05:49.000Z","updated_by":1,"published_at":"2015-02-08T07:40:39.000Z","published_by":1},{"id":15,"uuid":"9208bab8-d35c-4cdc-a087-3c84f1824b11","title":"How to setup java enviroment on server","slug":"setup-server-environment","markdown":"Just to generalize all that I studied from administration field, I decided to write all steps needed to setup your java environment on the web.\n\n#### What will we need?\n\nServer/local machine.Say, you need to deploy your app on the web and you need a server. I will be using digitalocean for that purpose. Of course, you will need ssh access.\nJava Development Kit. I will be using jdk 1.7 \nMaven build tool. For building apps. We can setup as much tools, as we want(Ant, Gradle), I choosed maven because I use it in my daily development.\nSetting up ssh key\n\nIf you already have ssh key , then you are free to skip this part.\n\nGo to your terminal and type\n\n> ssh-keygen -t rsa\n\nYou will be asked to enter ssh key location - I choosed \n\n> ~/.ssh/id_rsa_example\n\nAfter that, executed:\n\n> cat ~/.ssh/id_rsa_example.pub\n\nYou will get public key, that you need to store in your digitalocean instance in SSH page\n\nName this ssh and copy output from previous cat procedure.\n\n![](/content/images/2015/02/Screen-Shot-2015-01-01-at-4-45-55-PM-300x182.png)\n\nPS - Adding your ssh to digitalocean menu means, that when you will create server instance and choose that ssh key in wizard - it will automatically appear in server instance ~/.ssh/authorized_keys.\n\nPS - if you will forget to attach your ssh key to appropriate droplet you can always add this ssh key manually by typing\n\n> ssh-copy-id user@host // In Linux\ncat ~/.ssh/id_rsa_example.pub | ssh user@host \"cat >> ~/.ssh/authorized_keys\" // Os x\n\nFinally, you are ready to create server instance.\n\n#### Creating server instance\n\nIt's up to you how to create your server instance, I usually do this by Digitalocean web application.\n\nSo, go to digital ocean profile page, and find Create Droplet button\n\n![](/content/images/2015/02/Screen-Shot-2015-01-01-at-5-02-40-PM-300x191.png)\n\n \n\nAnd second part\n\n![](/content/images/2015/02/Screen-Shot-2015-01-01-at-5-03-27-PM-300x191.png)\n\n\n\n##### Finding IP and Creating restricted user\n\nGo to Droplets page in your Digitalocean account and find your just created server. There should be some assigned ip for your server. Remember it.\n\nThen execute\n\n> ssh root@{your ip}\n\nand create new user:\n\n> adduser customuser\n\nfollow wizard, type password, etc..\n\n#### SSH Configuration\n\nThen go your local pc Terminal and type\n\n> nano ~/.ssh/config\n\nand add next configuration:\n\n\tHost server-example\n        User customuser\n        HostName {ip of server}\n        IdentityFile ~/.ssh/id_rsa_example\nNow you are ready to connect to your server in convenient way\n\nYou can type\n\n> ssh server-example\n\nand start using your server.\n\n#### Java configuration\n\n> ssh example-server\n\n> mkdir tools\n\n> wget https://www.dropbox.com/s/9ffnfp5nbhlhy82/jdk-7u71-linux-i586.tar.gz?dl=0\n\n> tar zxvf jdk-7u71-linux-i586.tar.gz\n\n> ln -s /home/customuser/tools/jdk1.7.0_71 /home/customuser/tools/java\n\n> cd\n\n> nano .bashrc\n\nadd this lines to the end of file\n\n> export JAVA_HOME=~/tools/java\n\n> export PATH=$PATH:$JAVA_HOME/bin\n\npress Control + X , then type yes\n\n#### Maven configuration\n\nThe same with maven\n\n> ssh example-server\n\n> wget https://www.dropbox.com/s/gywe7n81nt89cv2/apache-maven-3.2.5-bin.tar.gz?dl=0\n\n> tar zxvf apache-maven-3.2.5-bin.tar.gz\n\n> ln -s /home/customuser/tools/apache-maven-3.2.5 /home/customuser/tools/maven\n\n> cd\n\n> nano .bashrc\n\nadd additional configuration.\n\n> export M2_HOME=~/tools/maven\n\n> export JAVA_HOME=~/tools/java\n\n> export PATH=$PATH:$M2_HOME/bin:$JAVA_HOME/bin\n\nTha's all, now you are ready to deploy your apps to remote server.\n\nPS - I will be happy to receive some criticism from you, that would help me to improve this article.","mobiledoc":null,"html":"<p>Just to generalize all that I studied from administration field, I decided to write all steps needed to setup your java environment on the web.</p>\n\n<h4 id=\"whatwillweneed\">What will we need?</h4>\n\n<p>Server/local machine.Say, you need to deploy your app on the web and you need a server. I will be using digitalocean for that purpose. Of course, you will need ssh access. <br />\nJava Development Kit. I will be using jdk 1.7 <br />\nMaven build tool. For building apps. We can setup as much tools, as we want(Ant, Gradle), I choosed maven because I use it in my daily development. <br />\nSetting up ssh key</p>\n\n<p>If you already have ssh key , then you are free to skip this part.</p>\n\n<p>Go to your terminal and type</p>\n\n<blockquote>\n  <p>ssh-keygen -t rsa</p>\n</blockquote>\n\n<p>You will be asked to enter ssh key location - I choosed </p>\n\n<blockquote>\n  <p>~/.ssh/id<em>rsa</em>example</p>\n</blockquote>\n\n<p>After that, executed:</p>\n\n<blockquote>\n  <p>cat ~/.ssh/id<em>rsa</em>example.pub</p>\n</blockquote>\n\n<p>You will get public key, that you need to store in your digitalocean instance in SSH page</p>\n\n<p>Name this ssh and copy output from previous cat procedure.</p>\n\n<p><img src=\"/content/images/2015/02/Screen-Shot-2015-01-01-at-4-45-55-PM-300x182.png\" alt=\"\" /></p>\n\n<p>PS - Adding your ssh to digitalocean menu means, that when you will create server instance and choose that ssh key in wizard - it will automatically appear in server instance ~/.ssh/authorized_keys.</p>\n\n<p>PS - if you will forget to attach your ssh key to appropriate droplet you can always add this ssh key manually by typing</p>\n\n<blockquote>\n  <p>ssh-copy-id user@host // In Linux\n  cat ~/.ssh/id<em>rsa</em>example.pub | ssh user@host \"cat >> ~/.ssh/authorized_keys\" // Os x</p>\n</blockquote>\n\n<p>Finally, you are ready to create server instance.</p>\n\n<h4 id=\"creatingserverinstance\">Creating server instance</h4>\n\n<p>It's up to you how to create your server instance, I usually do this by Digitalocean web application.</p>\n\n<p>So, go to digital ocean profile page, and find Create Droplet button</p>\n\n<p><img src=\"/content/images/2015/02/Screen-Shot-2015-01-01-at-5-02-40-PM-300x191.png\" alt=\"\" /></p>\n\n<p>And second part</p>\n\n<p><img src=\"/content/images/2015/02/Screen-Shot-2015-01-01-at-5-03-27-PM-300x191.png\" alt=\"\" /></p>\n\n<h5 id=\"findingipandcreatingrestricteduser\">Finding IP and Creating restricted user</h5>\n\n<p>Go to Droplets page in your Digitalocean account and find your just created server. There should be some assigned ip for your server. Remember it.</p>\n\n<p>Then execute</p>\n\n<blockquote>\n  <p>ssh root@{your ip}</p>\n</blockquote>\n\n<p>and create new user:</p>\n\n<blockquote>\n  <p>adduser customuser</p>\n</blockquote>\n\n<p>follow wizard, type password, etc..</p>\n\n<h4 id=\"sshconfiguration\">SSH Configuration</h4>\n\n<p>Then go your local pc Terminal and type</p>\n\n<blockquote>\n  <p>nano ~/.ssh/config</p>\n</blockquote>\n\n<p>and add next configuration:</p>\n\n<pre><code>Host server-example\n    User customuser\n    HostName {ip of server}\n    IdentityFile ~/.ssh/id_rsa_example\n</code></pre>\n\n<p>Now you are ready to connect to your server in convenient way</p>\n\n<p>You can type</p>\n\n<blockquote>\n  <p>ssh server-example</p>\n</blockquote>\n\n<p>and start using your server.</p>\n\n<h4 id=\"javaconfiguration\">Java configuration</h4>\n\n<blockquote>\n  <p>ssh example-server</p>\n  \n  <p>mkdir tools</p>\n  \n  <p>wget <a href=\"https://www.dropbox.com/s/9ffnfp5nbhlhy82/jdk-7u71-linux-i586.tar.gz?dl=0\">https://www.dropbox.com/s/9ffnfp5nbhlhy82/jdk-7u71-linux-i586.tar.gz?dl=0</a></p>\n  \n  <p>tar zxvf jdk-7u71-linux-i586.tar.gz</p>\n  \n  <p>ln -s /home/customuser/tools/jdk1.7.0_71 /home/customuser/tools/java</p>\n  \n  <p>cd</p>\n  \n  <p>nano .bashrc</p>\n</blockquote>\n\n<p>add this lines to the end of file</p>\n\n<blockquote>\n  <p>export JAVA_HOME=~/tools/java</p>\n  \n  <p>export PATH=$PATH:$JAVA_HOME/bin</p>\n</blockquote>\n\n<p>press Control + X , then type yes</p>\n\n<h4 id=\"mavenconfiguration\">Maven configuration</h4>\n\n<p>The same with maven</p>\n\n<blockquote>\n  <p>ssh example-server</p>\n  \n  <p>wget <a href=\"https://www.dropbox.com/s/gywe7n81nt89cv2/apache-maven-3.2.5-bin.tar.gz?dl=0\">https://www.dropbox.com/s/gywe7n81nt89cv2/apache-maven-3.2.5-bin.tar.gz?dl=0</a></p>\n  \n  <p>tar zxvf apache-maven-3.2.5-bin.tar.gz</p>\n  \n  <p>ln -s /home/customuser/tools/apache-maven-3.2.5 /home/customuser/tools/maven</p>\n  \n  <p>cd</p>\n  \n  <p>nano .bashrc</p>\n</blockquote>\n\n<p>add additional configuration.</p>\n\n<blockquote>\n  <p>export M2_HOME=~/tools/maven</p>\n  \n  <p>export JAVA_HOME=~/tools/java</p>\n  \n  <p>export PATH=$PATH:$M2<em>HOME/bin:$JAVA</em>HOME/bin</p>\n</blockquote>\n\n<p>Tha's all, now you are ready to deploy your apps to remote server.</p>\n\n<p>PS - I will be happy to receive some criticism from you, that would help me to improve this article.</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-02-08T07:42:24.000Z","created_by":1,"updated_at":"2015-07-27T12:05:20.000Z","updated_by":1,"published_at":"2015-02-08T07:50:58.000Z","published_by":1},{"id":17,"uuid":"c0354e7f-d2a5-48e7-836d-5fb4514a12bf","title":"Getting started with Robot Framework","slug":"getting-started-with-robot-framework","markdown":"Robot Framework is a generic test automation framework for acceptance testing and acceptance test-driven development (ATDD). It has easy-to-use tabular test data syntax and it utilizes the keyword-driven testing approach. Its testing capabilities can be extended by test libraries implemented either with Python or Java, and users can create new higher-level keywords from existing ones using the same syntax that is used for creating test cases.\n\nRobot Framework is operating system and application independent. The core framework is implemented using Python and runs also on Jython (JVM) and IronPython (.NET).\n\nWe will be using Robot Framework with Java + Maven build tool.\n\nNeed to have\n\n* Java. I use java 1.7\n* Maven. I use Maven 3.2.2 version\n* Downloaded SeleniumLibrary.\n* Willings to start something new and interesting :)\n* Preparations\n\nAll you gonna need to start using Robot Framework is his SeleniumLibrary, please, download it.\n\nSo, just create a simple maven project with jar packaging.\n\nOpen your project object model file - pom.xml and add Robot Framework plugin -\n\n<script src=\"https://gist.github.com/johnyUA/86f46698f93f81e229bd.js\"></script>\n\nAfter that, create two folders\n\n* src/test/robotframework/acceptance\n* src/test/resources/robotframework/libraries\n\nThen go to your downloaded library folder, find robotframework-seleniumlibrary-2.9.1.tar.gz, extract this tar archieve, open, go to src folder and copy\n\nrobotframework-seleniumlibrary-2.9.1/src/SeleniumLibrary to\n\n{project folder}/src/test/resources/robotframework/libraries/SeleniumLibrary\n\nYou should have similary structure:\n\n![](/content/images/2015/02/Screen-Shot-2015-01-01-at-1-06-19-PM-139x300.png)\n\n \n\nNow,  you have all required configuration and the only thing that is left to do is to create appropriate test cases in your project.\n\nGo to /src/test/robotframework/acceptance and create BasicTest.txt\n\n<script src=\"https://gist.github.com/johnyUA/1d8e79bec743e993c12e.js\"></script>\n\n \n\nThen open your terminal, navigate to your project folder and type\n\n> mvn verify\n\n\nAlso, you can find generated robot folder in your project target folder and open reports.html\n\nAs you can see, the test is quite simple, without any hard logic, just to demonstrate how to work with this framework.\n\nThis example has it's [own github page](https://github.com/johnyUA/robot-framework-getting-started)","mobiledoc":null,"html":"<p>Robot Framework is a generic test automation framework for acceptance testing and acceptance test-driven development (ATDD). It has easy-to-use tabular test data syntax and it utilizes the keyword-driven testing approach. Its testing capabilities can be extended by test libraries implemented either with Python or Java, and users can create new higher-level keywords from existing ones using the same syntax that is used for creating test cases.</p>\n\n<p>Robot Framework is operating system and application independent. The core framework is implemented using Python and runs also on Jython (JVM) and IronPython (.NET).</p>\n\n<p>We will be using Robot Framework with Java + Maven build tool.</p>\n\n<p>Need to have</p>\n\n<ul>\n<li>Java. I use java 1.7</li>\n<li>Maven. I use Maven 3.2.2 version</li>\n<li>Downloaded SeleniumLibrary.</li>\n<li>Willings to start something new and interesting :)</li>\n<li>Preparations</li>\n</ul>\n\n<p>All you gonna need to start using Robot Framework is his SeleniumLibrary, please, download it.</p>\n\n<p>So, just create a simple maven project with jar packaging.</p>\n\n<p>Open your project object model file - pom.xml and add Robot Framework plugin -</p>\n\n<script src=\"https://gist.github.com/johnyUA/86f46698f93f81e229bd.js\"></script>\n\n<p>After that, create two folders</p>\n\n<ul>\n<li>src/test/robotframework/acceptance</li>\n<li>src/test/resources/robotframework/libraries</li>\n</ul>\n\n<p>Then go to your downloaded library folder, find robotframework-seleniumlibrary-2.9.1.tar.gz, extract this tar archieve, open, go to src folder and copy</p>\n\n<p>robotframework-seleniumlibrary-2.9.1/src/SeleniumLibrary to</p>\n\n<p>{project folder}/src/test/resources/robotframework/libraries/SeleniumLibrary</p>\n\n<p>You should have similary structure:</p>\n\n<p><img src=\"/content/images/2015/02/Screen-Shot-2015-01-01-at-1-06-19-PM-139x300.png\" alt=\"\" /></p>\n\n<p>Now,  you have all required configuration and the only thing that is left to do is to create appropriate test cases in your project.</p>\n\n<p>Go to /src/test/robotframework/acceptance and create BasicTest.txt</p>\n\n<script src=\"https://gist.github.com/johnyUA/1d8e79bec743e993c12e.js\"></script>\n\n<p>Then open your terminal, navigate to your project folder and type</p>\n\n<blockquote>\n  <p>mvn verify</p>\n</blockquote>\n\n<p>Also, you can find generated robot folder in your project target folder and open reports.html</p>\n\n<p>As you can see, the test is quite simple, without any hard logic, just to demonstrate how to work with this framework.</p>\n\n<p>This example has it's <a href=\"https://github.com/johnyUA/robot-framework-getting-started\">own github page</a></p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-02-08T19:02:33.000Z","created_by":1,"updated_at":"2015-07-27T12:05:08.000Z","updated_by":1,"published_at":"2015-02-08T19:05:17.000Z","published_by":1},{"id":18,"uuid":"683b6267-8a2e-4182-b77c-db41cf3c1a4e","title":"Spring XML Namespaces handling","slug":"spring-xml-namespaces-handling","markdown":"You all know, that spring supports xml namespaces for defining custom bean definition.\n\nFor example, if you want to set mvn annotation driven benavior, you can write such code in your xml\n\n<script src=\"https://gist.github.com/ivanursul/b5ca5c3922e56e3809fc.js\"></script>\n\nAs you noticed, there is no <bean ... /> in this piece of code, but there is <mvc:annotation-driven>\n\nSo, who is responsible for handling this namespaces?\n\nAnswer is: NamespaceHandler\n\n<script src=\"https://gist.github.com/johnyUA/2f21efd721cab7b7419f.js\"></script>\n\nFor example, here is AnnotationDrivenBeanDefinitionParser implementation.\n\nEvery Spring namespace has an associated NamespaceHandler implementation. The namespace schemas are mapped to schema files inside Spring JARs in various spring.schemas files.\n\nThe XML schema namespaces are also mapped to handler classes in spring.handlers files (several as each Spring JAR might introduce different namespaces). For your convenience here is a list of most common namespaces:\n\nSpring core\n\naop - AopNamespaceHandler\nc - SimpleConstructorNamespaceHandler\ncache - CacheNamespaceHandler\ncontext - ContextNamespaceHandler\njdbc - JdbcNamespaceHandler\njee - JeeNamespaceHandler\njms - JmsNamespaceHandler\nlang - LangNamespaceHandler\nmvc - MvcNamespaceHandler\noxm - OxmNamespaceHandler\np - SimplePropertyNamespaceHandler\ntask - TaskNamespaceHandler\ntx - TxNamespaceHandler\nutil - UtilNamespaceHandler\nSpring Security\n\nsecurity - SecurityNamespaceHandler\noauth - OAuthSecurityNamespaceHandler\nSpring integration\n\nint - IntegrationNamespaceHandler\namqp - AmqpNamespaceHandler\nevent - EventNamespaceHandler\nfeed - FeedNamespaceHandler\nfile - FileNamespaceHandler\nftp - FtpNamespaceHandler\ngemfire - GemfireIntegrationNamespaceHandler\ngroovy - GroovyNamespaceHandler\nhttp - HttpNamespaceHandler\nip - IpNamespaceHandler\njdbc - JdbcNamespaceHandler\njms - JmsNamespaceHandler\njmx - JmxNamespaceHandler\nmail - MailNamespaceHandler\nredis - RedisNamespaceHandler\nrmi - RmiNamespaceHandler\nscript - ScriptNamespaceHandler\nsecurity - IntegrationSecurityNamespaceHandler\nsftp - SftpNamespaceHandler\nstream - StreamNamespaceHandler\ntwitter - TwitterNamespaceHandler\nws - WsNamespaceHandler\nxml - IntegrationXmlNamespaceHandler\nxmpp - XmppNamespaceHandler","mobiledoc":null,"html":"<p>You all know, that spring supports xml namespaces for defining custom bean definition.</p>\n\n<p>For example, if you want to set mvn annotation driven benavior, you can write such code in your xml</p>\n\n<script src=\"https://gist.github.com/ivanursul/b5ca5c3922e56e3809fc.js\"></script>\n\n<p>As you noticed, there is no <bean ... /> in this piece of code, but there is <mvc:annotation-driven></p>\n\n<p>So, who is responsible for handling this namespaces?</p>\n\n<p>Answer is: NamespaceHandler</p>\n\n<script src=\"https://gist.github.com/johnyUA/2f21efd721cab7b7419f.js\"></script>\n\n<p>For example, here is AnnotationDrivenBeanDefinitionParser implementation.</p>\n\n<p>Every Spring namespace has an associated NamespaceHandler implementation. The namespace schemas are mapped to schema files inside Spring JARs in various spring.schemas files.</p>\n\n<p>The XML schema namespaces are also mapped to handler classes in spring.handlers files (several as each Spring JAR might introduce different namespaces). For your convenience here is a list of most common namespaces:</p>\n\n<p>Spring core</p>\n\n<p>aop - AopNamespaceHandler <br />\nc - SimpleConstructorNamespaceHandler <br />\ncache - CacheNamespaceHandler <br />\ncontext - ContextNamespaceHandler <br />\njdbc - JdbcNamespaceHandler <br />\njee - JeeNamespaceHandler <br />\njms - JmsNamespaceHandler <br />\nlang - LangNamespaceHandler <br />\nmvc - MvcNamespaceHandler <br />\noxm - OxmNamespaceHandler <br />\np - SimplePropertyNamespaceHandler <br />\ntask - TaskNamespaceHandler <br />\ntx - TxNamespaceHandler <br />\nutil - UtilNamespaceHandler <br />\nSpring Security</p>\n\n<p>security - SecurityNamespaceHandler <br />\noauth - OAuthSecurityNamespaceHandler <br />\nSpring integration</p>\n\n<p>int - IntegrationNamespaceHandler <br />\namqp - AmqpNamespaceHandler <br />\nevent - EventNamespaceHandler <br />\nfeed - FeedNamespaceHandler <br />\nfile - FileNamespaceHandler <br />\nftp - FtpNamespaceHandler <br />\ngemfire - GemfireIntegrationNamespaceHandler <br />\ngroovy - GroovyNamespaceHandler <br />\nhttp - HttpNamespaceHandler <br />\nip - IpNamespaceHandler <br />\njdbc - JdbcNamespaceHandler <br />\njms - JmsNamespaceHandler <br />\njmx - JmxNamespaceHandler <br />\nmail - MailNamespaceHandler <br />\nredis - RedisNamespaceHandler <br />\nrmi - RmiNamespaceHandler <br />\nscript - ScriptNamespaceHandler <br />\nsecurity - IntegrationSecurityNamespaceHandler <br />\nsftp - SftpNamespaceHandler <br />\nstream - StreamNamespaceHandler <br />\ntwitter - TwitterNamespaceHandler <br />\nws - WsNamespaceHandler <br />\nxml - IntegrationXmlNamespaceHandler <br />\nxmpp - XmppNamespaceHandler</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-02-08T19:05:39.000Z","created_by":1,"updated_at":"2016-12-17T13:48:32.000Z","updated_by":1,"published_at":"2015-02-08T19:07:00.000Z","published_by":1},{"id":19,"uuid":"eb56821b-72ee-4595-a4c1-697eb7206006","title":"Google Guice - Hello world example","slug":"google-guice-getting-started","markdown":"It has been more than two years since I first acquainted with Spring Dependency Injection technology.\n\nAt first, I didn't understood what it was, but after few weeks I realized what powerful instrument do I have.\n\nFrom that time I started to love this type of object instantiation and I really don't want to instantiate objects in some other way.\n\nBut today I want to represent you a new framework for bean instantiation - Google Guice.\n\nSo, how to try this Google Guice dependency injection framework ?\n\nFirst of all, open your Eclipse/InteliJIDEA IDE and create maven project: write groupId, artifactId and other things.\n\nAfter that, go to your pom.xml file and add additional dependency:\n\n<script src=\"https://gist.github.com/johnyUA/60718dbd62173fee3315.js\"></script>\n\nThen, you should create interface NotificationService:\n\n<script src=\"https://gist.github.com/johnyUA/3084d08aa96ccbf7c904.js\"></script>\n\nWe will have multiple implementations of above service to show posibillities of Google Guice\n\n<script src=\"https://gist.github.com/johnyUA/448f6f104c490b26ce89.js\"></script>\n\nEmail service is one of the implementations of NotificationService\n\nOne thing that I noticed is that Google Guice has it's own @Singleton annotation - from JSR330.\n\nGoogle Guice 3.0 added the support for JSR-330 and we can use annotations from com.google.inject or javax.inject package.\n\nNext, we will create second implementation of NotificationService - FacebookService:\n\n<script src=\"https://gist.github.com/johnyUA/a4a8ad0c60a2ecc8fe2a.js\"></script>\n\nConsumer class\n\nSince we are implementing dependency injection in our application, we wont initialize the service class in application. Google Guice support both setter-based and constructor-based dependency injection. Our application class that consumes the service looks like below.\n\n<script src=\"https://gist.github.com/johnyUA/71429d73d4d58088ae45.js\"></script>\n\nOr, if you want constructor-based instantiation\n\n<script src=\"https://gist.github.com/johnyUA/b0cb409aa635db568ee3.js\"></script>\n\nBinding Service implementation\n\nObviously google guice will not know which service to use, we have to configure it by extending AbstractModule abstract class and provide implementation for configure() method.\n\n<script src=\"https://gist.github.com/johnyUA/33b0de2619d8180b9264.js\"></script>\n\nAs you can see that we can bind any of the implementation to service class. For example, if we want to change to EmailService we would just need to change the bindings.\n\nClient Application\n\nOur setup is ready, lets see how to use it with a simple java class.\n\n<script src=\"https://gist.github.com/johnyUA/7b78e07f2b584cc0a585.js\"></script>\n\nThe implementation is very easy to understand. We need to create Injector object using Guice class createInjector() method where we pass our injector class implementation object. Then we use injector to initialize our consumer class. If we run above class, it will produce following output.\n\nMessage sent to Facebook user Ivan@mail.com with message=Hi, Ivan\n\nUse of Google Guice for implementing dependency injection in application is very easy and it does it beautifully. Its used in Google APIs so we can assume that its highly tested and reliable code. Download the project from above and play around with it to learn more.","mobiledoc":null,"html":"<p>It has been more than two years since I first acquainted with Spring Dependency Injection technology.</p>\n\n<p>At first, I didn't understood what it was, but after few weeks I realized what powerful instrument do I have.</p>\n\n<p>From that time I started to love this type of object instantiation and I really don't want to instantiate objects in some other way.</p>\n\n<p>But today I want to represent you a new framework for bean instantiation - Google Guice.</p>\n\n<p>So, how to try this Google Guice dependency injection framework ?</p>\n\n<p>First of all, open your Eclipse/InteliJIDEA IDE and create maven project: write groupId, artifactId and other things.</p>\n\n<p>After that, go to your pom.xml file and add additional dependency:</p>\n\n<script src=\"https://gist.github.com/johnyUA/60718dbd62173fee3315.js\"></script>\n\n<p>Then, you should create interface NotificationService:</p>\n\n<script src=\"https://gist.github.com/johnyUA/3084d08aa96ccbf7c904.js\"></script>\n\n<p>We will have multiple implementations of above service to show posibillities of Google Guice</p>\n\n<script src=\"https://gist.github.com/johnyUA/448f6f104c490b26ce89.js\"></script>\n\n<p>Email service is one of the implementations of NotificationService</p>\n\n<p>One thing that I noticed is that Google Guice has it's own @Singleton annotation - from JSR330.</p>\n\n<p>Google Guice 3.0 added the support for JSR-330 and we can use annotations from com.google.inject or javax.inject package.</p>\n\n<p>Next, we will create second implementation of NotificationService - FacebookService:</p>\n\n<script src=\"https://gist.github.com/johnyUA/a4a8ad0c60a2ecc8fe2a.js\"></script>\n\n<p>Consumer class</p>\n\n<p>Since we are implementing dependency injection in our application, we wont initialize the service class in application. Google Guice support both setter-based and constructor-based dependency injection. Our application class that consumes the service looks like below.</p>\n\n<script src=\"https://gist.github.com/johnyUA/71429d73d4d58088ae45.js\"></script>\n\n<p>Or, if you want constructor-based instantiation</p>\n\n<script src=\"https://gist.github.com/johnyUA/b0cb409aa635db568ee3.js\"></script>\n\n<p>Binding Service implementation</p>\n\n<p>Obviously google guice will not know which service to use, we have to configure it by extending AbstractModule abstract class and provide implementation for configure() method.</p>\n\n<script src=\"https://gist.github.com/johnyUA/33b0de2619d8180b9264.js\"></script>\n\n<p>As you can see that we can bind any of the implementation to service class. For example, if we want to change to EmailService we would just need to change the bindings.</p>\n\n<p>Client Application</p>\n\n<p>Our setup is ready, lets see how to use it with a simple java class.</p>\n\n<script src=\"https://gist.github.com/johnyUA/7b78e07f2b584cc0a585.js\"></script>\n\n<p>The implementation is very easy to understand. We need to create Injector object using Guice class createInjector() method where we pass our injector class implementation object. Then we use injector to initialize our consumer class. If we run above class, it will produce following output.</p>\n\n<p>Message sent to Facebook user Ivan@mail.com with message=Hi, Ivan</p>\n\n<p>Use of Google Guice for implementing dependency injection in application is very easy and it does it beautifully. Its used in Google APIs so we can assume that its highly tested and reliable code. Download the project from above and play around with it to learn more.</p>","image":"","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-02-08T19:07:48.000Z","created_by":1,"updated_at":"2015-07-27T12:04:50.000Z","updated_by":1,"published_at":"2015-02-08T19:09:36.000Z","published_by":1},{"id":20,"uuid":"faf9caf7-c207-4b47-bbf7-b844d3f5a1e5","title":"Jenkins and Maven SQL Plugin for cleaning Postgresql database","slug":"jenkins-ans-maven-sql-plugin-for-cleaning-database","markdown":"How many times did you experienced issue, when you need to clean your database from unused data ?For example, you have a qa server with thousands of rows, which you don't need anymore, so it's time to delete them.\nTo clean up your database, someone call separate scripts for each table, someone create-drops database.I decided to delete all tables.\nTo remove all rows, you can use maven sql plugin, which has powerful possibilities for doing work with sql.\n\n* First of all, if you have your maven application, I would reccomend to create separate module for sql. If you don't want to do this, just leave it as it is in your root pom.xml.\n\n* You will need to add Maven Sql Plugin. Then, you need to map execution to install goal.\n\n* You will need to create resource folder.This will be the place, where you will store your scripts.\n\n\n#### Before we start\n\nI don't want anyone to think, that such module is the best place for storing your database schemas creation.For this purposes there is a good framework called [liquibase](http://www.liquibase.org/) - it saves migration history and can surely know, what should be updated, and what should not.\n\nSuch module is always good for adding additional data into your dev/qa/uat/release environments.\n\nThis module can also be very useful, when you want to do some manipulations with database.\n\n\n#### Maven SQL plugin\n\nTo create maven sql plugin, add plugin into **pom.xml** **build** section\n\n<script src=\"https://gist.github.com/johnyUA/9e96aea61f9e5e590c5c.js\"></script>\n\nThis plugin is mapped to **install** goal and will executes as soon as you will run **mvn install** command.\n\n#### Advices\n\n* If you have multiple modules, don't add your module into root pom.xml - just remote it from there. Such module will run each time when you will run your standard **mvn clean install** command, which will affect your database data.\n\n* Keep database properties in your root pom.xml. I always do like that, and I can use this properties in each child module - dao, sql or migration modules.\n\n* If you have some data insertion in your scripts - always synchronize your changes with your project version.If you have **0.9.1** version of your system - create a folder **0.9.1** with all changes from that version. That will help you a lot while working in production.\n\n\n#### Conclusions\n\nAs a said before, I use this module for doing database cleaning in my project. The reason I am doint that is that I have migration scripts in liquibase, that, from time to time needs to be reupdated. Due to the fact, that liquibase stores changeset structure, next time, when I will run my updated migration script - I will receive fail. Of course, I know, that this is an issue, because I don't need to reupdate changeset - I need to write new one, but currently I am not in production - so I want to keep liquibase scripts simple.\n\nI published jenkins job with **is-lnu-clean/pom.xml** maven pom and **mvn install** command, so when I need to clean up my database - I do that easily with my jenkins.\n","mobiledoc":null,"html":"<p>How many times did you experienced issue, when you need to clean your database from unused data ?For example, you have a qa server with thousands of rows, which you don't need anymore, so it's time to delete them. <br />\nTo clean up your database, someone call separate scripts for each table, someone create-drops database.I decided to delete all tables. <br />\nTo remove all rows, you can use maven sql plugin, which has powerful possibilities for doing work with sql.</p>\n\n<ul>\n<li><p>First of all, if you have your maven application, I would reccomend to create separate module for sql. If you don't want to do this, just leave it as it is in your root pom.xml.</p></li>\n<li><p>You will need to add Maven Sql Plugin. Then, you need to map execution to install goal.</p></li>\n<li><p>You will need to create resource folder.This will be the place, where you will store your scripts.</p></li>\n</ul>\n\n<h4 id=\"beforewestart\">Before we start</h4>\n\n<p>I don't want anyone to think, that such module is the best place for storing your database schemas creation.For this purposes there is a good framework called <a href=\"http://www.liquibase.org/\">liquibase</a> - it saves migration history and can surely know, what should be updated, and what should not.</p>\n\n<p>Such module is always good for adding additional data into your dev/qa/uat/release environments.</p>\n\n<p>This module can also be very useful, when you want to do some manipulations with database.</p>\n\n<h4 id=\"mavensqlplugin\">Maven SQL plugin</h4>\n\n<p>To create maven sql plugin, add plugin into <strong>pom.xml</strong> <strong>build</strong> section</p>\n\n<script src=\"https://gist.github.com/johnyUA/9e96aea61f9e5e590c5c.js\"></script>\n\n<p>This plugin is mapped to <strong>install</strong> goal and will executes as soon as you will run <strong>mvn install</strong> command.</p>\n\n<h4 id=\"advices\">Advices</h4>\n\n<ul>\n<li><p>If you have multiple modules, don't add your module into root pom.xml - just remote it from there. Such module will run each time when you will run your standard <strong>mvn clean install</strong> command, which will affect your database data.</p></li>\n<li><p>Keep database properties in your root pom.xml. I always do like that, and I can use this properties in each child module - dao, sql or migration modules.</p></li>\n<li><p>If you have some data insertion in your scripts - always synchronize your changes with your project version.If you have <strong>0.9.1</strong> version of your system - create a folder <strong>0.9.1</strong> with all changes from that version. That will help you a lot while working in production.</p></li>\n</ul>\n\n<h4 id=\"conclusions\">Conclusions</h4>\n\n<p>As a said before, I use this module for doing database cleaning in my project. The reason I am doint that is that I have migration scripts in liquibase, that, from time to time needs to be reupdated. Due to the fact, that liquibase stores changeset structure, next time, when I will run my updated migration script - I will receive fail. Of course, I know, that this is an issue, because I don't need to reupdate changeset - I need to write new one, but currently I am not in production - so I want to keep liquibase scripts simple.</p>\n\n<p>I published jenkins job with <strong>is-lnu-clean/pom.xml</strong> maven pom and <strong>mvn install</strong> command, so when I need to clean up my database - I do that easily with my jenkins.</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-02-09T13:07:27.000Z","created_by":1,"updated_at":"2015-07-27T12:04:26.000Z","updated_by":1,"published_at":"2015-02-09T15:58:32.000Z","published_by":1},{"id":21,"uuid":"4a9b0a63-1fd9-4af1-9c4d-4a79a7ae0e4f","title":"The Great Dictator's Speech","slug":"the-great-dictators-speech","markdown":"<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/w8HdOHrc3OQ\" frameborder=\"0\" allowfullscreen></iframe>\n\nIm sorry, but I dont want to be an emperor. Thats not my business. I dont want to rule or conquer anyone. I should like to help everyone - if possible - Jew, Gentile - black man - white. We all want to help one another. Human beings are like that. We want to live by each others happiness - not by each others misery. We dont want to hate and despise one another. In this world there is room for everyone. And the good earth is rich and can provide for everyone. The way of life can be free and beautiful, but we have lost the way.\n\nGreed has poisoned mens souls, has barricaded the world with hate, has goose-stepped us into misery and bloodshed. We have developed speed, but we have shut ourselves in. Machinery that gives abundance has left us in want. Our knowledge has made us cynical. Our cleverness, hard and unkind. We think too much and feel too little. More than machinery we need humanity. More than cleverness we need kindness and gentleness. Without these qualities, life will be violent and all will be lost....\n\n!/images/photos/0000/0874/Great_Dictator_Pub_140-6_normal.jpg! The aeroplane and the radio have brought us closer together. The very nature of these inventions cries out for the goodness in men - cries out for universal brotherhood - for the unity of us all. Even now my voice is reaching millions throughout the world - millions of despairing men, women, and little children - victims of a system that makes men torture and imprison innocent people.\n\nTo those who can hear me, I say - do not despair. The misery that is now upon us is but the passing of greed - the bitterness of men who fear the way of human progress. The hate of men will pass, and dictators die, and the power they took from the people will return to the people. And so long as men die, liberty will never perish. .....\n\nSoldiers! dont give yourselves to brutes - men who despise you - enslave you - who regiment your lives - tell you what to do - what to think and what to feel! Who drill you - diet you - treat you like cattle, use you as cannon fodder. Dont give yourselves to these unnatural men - machine men with machine minds and machine hearts! You are not machines! You are not cattle! You are men! You have the love of humanity in your hearts! You dont hate! Only the unloved hate - the unloved and the unnatural! Soldiers! Dont fight for slavery! Fight for liberty!\n\nIn the 17th Chapter of St Luke it is written: the Kingdom of God is within man - not one man nor a group of men, but in all men! In you! You, the people have the power - the power to create machines. The power to create happiness! You, the people, have the power to make this life free and beautiful, to make this life a wonderful adventure.\n\nThen - in the name of democracy - let us use that power - let us all unite. Let us fight for a new world - a decent world that will give men a chance to work - that will give youth a future and old age a security. By the promise of these things, brutes have risen to power. But they lie! They do not fulfil that promise. They never will!\n\nDictators free themselves but they enslave the people! Now let us fight to fulfil that promise! Let us fight to free the world - to do away with national barriers - to do away with greed, with hate and intolerance. Let us fight for a world of reason, a world where science and progress will lead to all mens happiness. Soldiers! in the name of democracy, let us all unite!\nhttp://www.charliechaplin.com/en/synopsis/a\n-The-Great-Dictator-s-Speech\n","mobiledoc":null,"html":"<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/w8HdOHrc3OQ\" frameborder=\"0\" allowfullscreen></iframe>\n\n<p>Im sorry, but I dont want to be an emperor. Thats not my business. I dont want to rule or conquer anyone. I should like to help everyone - if possible - Jew, Gentile - black man - white. We all want to help one another. Human beings are like that. We want to live by each others happiness - not by each others misery. We dont want to hate and despise one another. In this world there is room for everyone. And the good earth is rich and can provide for everyone. The way of life can be free and beautiful, but we have lost the way.</p>\n\n<p>Greed has poisoned mens souls, has barricaded the world with hate, has goose-stepped us into misery and bloodshed. We have developed speed, but we have shut ourselves in. Machinery that gives abundance has left us in want. Our knowledge has made us cynical. Our cleverness, hard and unkind. We think too much and feel too little. More than machinery we need humanity. More than cleverness we need kindness and gentleness. Without these qualities, life will be violent and all will be lost....</p>\n\n<p>!/images/photos/0000/0874/Great<em>Dictator</em>Pub<em>140-6</em>normal.jpg! The aeroplane and the radio have brought us closer together. The very nature of these inventions cries out for the goodness in men - cries out for universal brotherhood - for the unity of us all. Even now my voice is reaching millions throughout the world - millions of despairing men, women, and little children - victims of a system that makes men torture and imprison innocent people.</p>\n\n<p>To those who can hear me, I say - do not despair. The misery that is now upon us is but the passing of greed - the bitterness of men who fear the way of human progress. The hate of men will pass, and dictators die, and the power they took from the people will return to the people. And so long as men die, liberty will never perish. .....</p>\n\n<p>Soldiers! dont give yourselves to brutes - men who despise you - enslave you - who regiment your lives - tell you what to do - what to think and what to feel! Who drill you - diet you - treat you like cattle, use you as cannon fodder. Dont give yourselves to these unnatural men - machine men with machine minds and machine hearts! You are not machines! You are not cattle! You are men! You have the love of humanity in your hearts! You dont hate! Only the unloved hate - the unloved and the unnatural! Soldiers! Dont fight for slavery! Fight for liberty!</p>\n\n<p>In the 17th Chapter of St Luke it is written: the Kingdom of God is within man - not one man nor a group of men, but in all men! In you! You, the people have the power - the power to create machines. The power to create happiness! You, the people, have the power to make this life free and beautiful, to make this life a wonderful adventure.</p>\n\n<p>Then - in the name of democracy - let us use that power - let us all unite. Let us fight for a new world - a decent world that will give men a chance to work - that will give youth a future and old age a security. By the promise of these things, brutes have risen to power. But they lie! They do not fulfil that promise. They never will!</p>\n\n<p>Dictators free themselves but they enslave the people! Now let us fight to fulfil that promise! Let us fight to free the world - to do away with national barriers - to do away with greed, with hate and intolerance. Let us fight for a world of reason, a world where science and progress will lead to all mens happiness. Soldiers! in the name of democracy, let us all unite! <br />\n<a href=\"http://www.charliechaplin.com/en/synopsis/a\">http://www.charliechaplin.com/en/synopsis/a</a> <br />\n-The-Great-Dictator-s-Speech</p>","image":"","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-02-09T13:49:38.000Z","created_by":1,"updated_at":"2015-07-27T12:04:41.000Z","updated_by":1,"published_at":"2015-02-09T13:50:51.000Z","published_by":1},{"id":22,"uuid":"cf168132-ed22-475f-93f0-0ca87ebbeec4","title":"Combining POSTGRESQL and JAVA enums with HIBERNATE together","slug":"combining-postgresql-and-java-enums-with-hibernate-together","markdown":"How many times did you have to use java enums in your JPA/Hubernate entities ? Yes, instead of having some relations with reference-tables, which are some sort of dictionaries, you wrote simple field with **varchar** type\n\nSomething like class **Model** with **RowStatus** enum\n<script src=\"https://gist.github.com/johnyUA/8c05eddba45aaf60e1df.js\"></script>\n\n<script src=\"https://gist.github.com/johnyUA/5ee2b70b64a73699ec32.js\"></script>\n\nRowStatus is used to detemine whether row is deleted from the system, or row is active.It seems, that this should be enough to have no problems while using this functionality.\n\nBut there is one potential problem: When we set enum in our system, we made a contract, that we will send only values from **RowStatus** enum. But how about updating data outside our system ? For example, system administrator of your database made some critical update  and committed some critical bug - instead of values from RowStatus - **ACTIVE**, **DELETED** he updated row with syntax error - **DELITED**. From now on, when Hibernate will try to map such row - he will throw an exception.\nThat's an issue. \n\n#### Possible solutions ?\n\nThe only solution that I can offer you is that you will controll adding your data on database. You can use database enums, constraints, triggers, etc...\n\nI choosed postgres enums.\n\nTo add such, you simply need to create your new type\n\n> CREATE TYPE qrow_status AS ENUM ('ACTIVE', 'DELETED', 'REATTACHED');\n\nThen you are free to create your tables with new type\n\n> CREATE TABLE q_rf_streettype\n> (\n  \n >  id bigserial NOT NULL, \n  \n >  status **qrow_status** NOT NULL,\n \n > ...\n \n> )\n\nYou can work with this column as simple **varchar** type.\n\n\n#### Combine with JPA/Hibernate.\n\nModificate your entity\n\n<script src=\"https://gist.github.com/johnyUA/e380880279088e82b988.js\"></script>\n\nand add additional class\n\n<script src=\"https://gist.github.com/johnyUA/6b1de7a41202f9a01d51.js\"></script>\n\nRemember, that your enum column should now have **row_status** type, not **varchar** or some else.","mobiledoc":null,"html":"<p>How many times did you have to use java enums in your JPA/Hubernate entities ? Yes, instead of having some relations with reference-tables, which are some sort of dictionaries, you wrote simple field with <strong>varchar</strong> type</p>\n\n<p>Something like class <strong>Model</strong> with <strong>RowStatus</strong> enum  </p>\n\n<script src=\"https://gist.github.com/johnyUA/8c05eddba45aaf60e1df.js\"></script>\n\n<script src=\"https://gist.github.com/johnyUA/5ee2b70b64a73699ec32.js\"></script>\n\n<p>RowStatus is used to detemine whether row is deleted from the system, or row is active.It seems, that this should be enough to have no problems while using this functionality.</p>\n\n<p>But there is one potential problem: When we set enum in our system, we made a contract, that we will send only values from <strong>RowStatus</strong> enum. But how about updating data outside our system ? For example, system administrator of your database made some critical update  and committed some critical bug - instead of values from RowStatus - <strong>ACTIVE</strong>, <strong>DELETED</strong> he updated row with syntax error - <strong>DELITED</strong>. From now on, when Hibernate will try to map such row - he will throw an exception. <br />\nThat's an issue. </p>\n\n<h4 id=\"possiblesolutions\">Possible solutions ?</h4>\n\n<p>The only solution that I can offer you is that you will controll adding your data on database. You can use database enums, constraints, triggers, etc...</p>\n\n<p>I choosed postgres enums.</p>\n\n<p>To add such, you simply need to create your new type</p>\n\n<blockquote>\n  <p>CREATE TYPE qrow_status AS ENUM ('ACTIVE', 'DELETED', 'REATTACHED');</p>\n</blockquote>\n\n<p>Then you are free to create your tables with new type</p>\n\n<blockquote>\n  <p>CREATE TABLE q<em>rf</em>streettype\n  (</p>\n  \n  <p>id bigserial NOT NULL, </p>\n  \n  <p>status <strong>qrow_status</strong> NOT NULL,</p>\n  \n  <p>...</p>\n  \n  <p>)</p>\n</blockquote>\n\n<p>You can work with this column as simple <strong>varchar</strong> type.</p>\n\n<h4 id=\"combinewithjpahibernate\">Combine with JPA/Hibernate.</h4>\n\n<p>Modificate your entity</p>\n\n<script src=\"https://gist.github.com/johnyUA/e380880279088e82b988.js\"></script>\n\n<p>and add additional class</p>\n\n<script src=\"https://gist.github.com/johnyUA/6b1de7a41202f9a01d51.js\"></script>\n\n<p>Remember, that your enum column should now have <strong>row_status</strong> type, not <strong>varchar</strong> or some else.</p>","image":"","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-02-15T07:00:46.000Z","created_by":1,"updated_at":"2015-07-27T12:03:55.000Z","updated_by":1,"published_at":"2015-02-15T07:39:41.000Z","published_by":1},{"id":23,"uuid":"8d564d99-caa4-4dc6-9203-3651f1575787","title":"Why we need to delegate some work to database.","slug":"why-we-need-to-delegate-some-work-to-database","markdown":"Before writing this post what I want to underscore is that this port is mostly for middle layer developers, that don't now many things about database.\nAround year ago I started to work on some freelance project.I tried to use all up-to-date technologies: Cool web framework Spring with DI(Dependency Injection), ORM framework by **java persistence api(JPA)** specification, simple MySQL database. I decided to make RESTful service, and, in the end, I had 20+ methods there.I finished my part of project and delegated another one to other people. The reason I am writing this post is that application had one BIG ISSUE, that I understood recently: It did most of the db logic in the middle layer. Yes, all compatibility checks, some data validation and other staff were done in my web service. I would like to emphasize why It is bad:\n\nThere is a chance that somone will use your database beyond your application, for example, someone will insert invalid row.Then there is a big risk that your application will broke it's constistency - some data will now be invalid or broken. That's why I make a conclusion that when you are choosing where to store your logic - in the database or in application - think about consistency of your database without your middle - layer application.\n\nI can show real life example: I have a database with 130+ tables, where one of such tables is table about persons - all the persons, with plenty of personTypes - students, teachers, professors, etc.\n\n<script src=\"https://gist.github.com/johnyUA/e2ac4aff7ac089b057aa.js\"></script>\n\nMy desire behavior is that when user updates **personTypeId** row to some different from existing value  - then there should be new row with new updated data, but there also should be old one row.\n\nHow can we achieve this ? Of course, you have your web application written on java/.net/python/php and you can do additional check if **personTypeId** is different from value in database - then you can make new insertion instead of update. But as I said before, this method brokes your consistency.\n\nTo be abosultely sure that your data will keep consistent state, you can make your logic on database by adding triggers.I position myself as java developer, and have very poor experience with database. Thanks to other db developers, I got database with well-designed schema.Finally, I got advices from my coworker Nazar with instructions on how to use triggers.\nSo, here is algorithm of work\n\n* We will create trigger, that will triggers update **operations**\n\n* if new personTypeId will be different from old one, then we will insert new row into our database with new updated data and we will **not** update old row.\n\nHere you can see sql script\n\n<script src=\"https://gist.github.com/johnyUA/4326301b16739d52701b.js\"></script>\n\nAs you can see, trigger will fire, when there will be update action on row, and in case if personTypeId will de different, there will a new row, but old row will be not updated.Also, there are some operations with remapping required relations on person additional tables.\n\nThe only problem, that I see here is that if user will update row - he will receive old one with no information about location of new inserted row. If someones know, how to deal with such problem - I will be happy to read your suggestions.\n\nI would like to conclude by saying, that this post is just my impression on new approach, and if there is some troubles here - feel free to contact me.\n\n","mobiledoc":null,"html":"<p>Before writing this post what I want to underscore is that this port is mostly for middle layer developers, that don't now many things about database. <br />\nAround year ago I started to work on some freelance project.I tried to use all up-to-date technologies: Cool web framework Spring with DI(Dependency Injection), ORM framework by <strong>java persistence api(JPA)</strong> specification, simple MySQL database. I decided to make RESTful service, and, in the end, I had 20+ methods there.I finished my part of project and delegated another one to other people. The reason I am writing this post is that application had one BIG ISSUE, that I understood recently: It did most of the db logic in the middle layer. Yes, all compatibility checks, some data validation and other staff were done in my web service. I would like to emphasize why It is bad:</p>\n\n<p>There is a chance that somone will use your database beyond your application, for example, someone will insert invalid row.Then there is a big risk that your application will broke it's constistency - some data will now be invalid or broken. That's why I make a conclusion that when you are choosing where to store your logic - in the database or in application - think about consistency of your database without your middle - layer application.</p>\n\n<p>I can show real life example: I have a database with 130+ tables, where one of such tables is table about persons - all the persons, with plenty of personTypes - students, teachers, professors, etc.</p>\n\n<script src=\"https://gist.github.com/johnyUA/e2ac4aff7ac089b057aa.js\"></script>\n\n<p>My desire behavior is that when user updates <strong>personTypeId</strong> row to some different from existing value  - then there should be new row with new updated data, but there also should be old one row.</p>\n\n<p>How can we achieve this ? Of course, you have your web application written on java/.net/python/php and you can do additional check if <strong>personTypeId</strong> is different from value in database - then you can make new insertion instead of update. But as I said before, this method brokes your consistency.</p>\n\n<p>To be abosultely sure that your data will keep consistent state, you can make your logic on database by adding triggers.I position myself as java developer, and have very poor experience with database. Thanks to other db developers, I got database with well-designed schema.Finally, I got advices from my coworker Nazar with instructions on how to use triggers. <br />\nSo, here is algorithm of work</p>\n\n<ul>\n<li><p>We will create trigger, that will triggers update <strong>operations</strong></p></li>\n<li><p>if new personTypeId will be different from old one, then we will insert new row into our database with new updated data and we will <strong>not</strong> update old row.</p></li>\n</ul>\n\n<p>Here you can see sql script</p>\n\n<script src=\"https://gist.github.com/johnyUA/4326301b16739d52701b.js\"></script>\n\n<p>As you can see, trigger will fire, when there will be update action on row, and in case if personTypeId will de different, there will a new row, but old row will be not updated.Also, there are some operations with remapping required relations on person additional tables.</p>\n\n<p>The only problem, that I see here is that if user will update row - he will receive old one with no information about location of new inserted row. If someones know, how to deal with such problem - I will be happy to read your suggestions.</p>\n\n<p>I would like to conclude by saying, that this post is just my impression on new approach, and if there is some troubles here - feel free to contact me.</p>","image":"","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-02-16T22:04:58.000Z","created_by":1,"updated_at":"2015-07-27T12:03:37.000Z","updated_by":1,"published_at":"2015-02-20T23:27:46.000Z","published_by":1},{"id":24,"uuid":"73a159ce-4913-42c2-8fcb-9a7c16bfced6","title":"yrillic query params in Tomcat Application Server","slug":"syrillic-query-params-in-tomcat-application-server","markdown":"Today I had a problem with Tomcat encoding with query params.I have solution for search entities.It was working with **latin** words. But when I tried to add query params in **cyrillic**, tomcat treat them as unreadable symbols, despite the fact, that I have URLEncodingFilter in my web application, that encodes everything in **UTF-8**,\n\nSo, request like this turns to be invalid.\n    \n    http://{HOST}:8080/is-lnu-rest-api/api/specoffers/types?name=\n    \nSo, as usual, I started investigation of this issue. First, I scanned my project, especially **web.xml** deployment descriptor for some unusual encoding thing, but I failed, I found nothing. So I delegated all the investigation to google, and found that I should edit my \n\n\ttomcat/conf/server.xml\n    \n\nYou need to find Connector with port=\"8080\" and add two lines.\n\n    <Connector port=\"8080\" protocol=\"HTTP/1.1\"\n    \tconnectionTimeout=\"20000\"\n\t    useBodyEncodingForURI=\"true\" <!-- This line -->\n\t    URIEncoding=\"UTF-8\" <!-- This line -->\n        redirectPort=\"8443\" />\n\nRestart your tomcat, and now everythig will be okay","mobiledoc":null,"html":"<p>Today I had a problem with Tomcat encoding with query params.I have solution for search entities.It was working with <strong>latin</strong> words. But when I tried to add query params in <strong>cyrillic</strong>, tomcat treat them as unreadable symbols, despite the fact, that I have URLEncodingFilter in my web application, that encodes everything in <strong>UTF-8</strong>,</p>\n\n<p>So, request like this turns to be invalid.</p>\n\n<pre><code>http://{HOST}:8080/is-lnu-rest-api/api/specoffers/types?name=\n</code></pre>\n\n<p>So, as usual, I started investigation of this issue. First, I scanned my project, especially <strong>web.xml</strong> deployment descriptor for some unusual encoding thing, but I failed, I found nothing. So I delegated all the investigation to google, and found that I should edit my </p>\n\n<pre><code>tomcat/conf/server.xml\n</code></pre>\n\n<p>You need to find Connector with port=\"8080\" and add two lines.</p>\n\n<pre><code>&lt;Connector port=\"8080\" protocol=\"HTTP/1.1\"\n    connectionTimeout=\"20000\"\n    useBodyEncodingForURI=\"true\" &lt;!-- This line --&gt;\n    URIEncoding=\"UTF-8\" &lt;!-- This line --&gt;\n    redirectPort=\"8443\" /&gt;\n</code></pre>\n\n<p>Restart your tomcat, and now everythig will be okay</p>","image":"","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-04-01T12:47:07.000Z","created_by":1,"updated_at":"2015-07-27T12:03:23.000Z","updated_by":1,"published_at":"2015-04-01T13:08:00.000Z","published_by":1},{"id":25,"uuid":"e8c8e147-0b6f-42ae-bd88-41d55dacac21","title":"Writing JUnit Rules","slug":"rules-in-junit","markdown":"## Agenda\n\n</br>\n\n\n* Description of rules\n* **@Rule** vs **@ClassRule**\n* Some useful rules\n* Writing your own rule\n\n##### Description of rules\n\nRules allow very flexible addition or redefinition of the behavior of each test method in a test class. Testers can reuse or extend one of the provided Rules below, or write their own.\n\nThis mean, that you can controll the behavior of your test case, you can catch exceptions, write before-after methods, etc.\n\nI would recommend to read more about rules here\nhttps://github.com/junit-team/junit/wiki/Rules\n\n##### @Rule vs @ClassRule\n\nThere is a difference between **@Rule** and **@ClassRule** in JUnit.We can differ them similiar as **@Before** and **@BeforeClass**. If you will annotate field ,that implements **org.junit.rules.TestRule** with **@Rule** annotation, then it will run in each method. But, if you will annotate this field as **@ClassRule**, then it will run once.\n\n##### Useful rules examples\n\n<script src=\"https://gist.github.com/johnyUA/bac0bac583210d52d5fa.js\"></script>\n\n##### Writing your own rule\n\nWe will write custom **TestRule**, that will log actions. We will have two \n\n<script src=\"https://gist.github.com/johnyUA/15a96f98ddf4ecb01fd1.js\"></script>\n\n\n<script src=\"https://gist.github.com/johnyUA/47876fc4c56646ddcaa8.js\"></script>\n\nWe will get\n\n    before: global\n    before: local\n    after: local\n    after: global\n","mobiledoc":null,"html":"<h2 id=\"agenda\">Agenda</h2>\n\n<p></br></p>\n\n<ul>\n<li>Description of rules</li>\n<li><strong>@Rule</strong> vs <strong>@ClassRule</strong></li>\n<li>Some useful rules</li>\n<li>Writing your own rule</li>\n</ul>\n\n<h5 id=\"descriptionofrules\">Description of rules</h5>\n\n<p>Rules allow very flexible addition or redefinition of the behavior of each test method in a test class. Testers can reuse or extend one of the provided Rules below, or write their own.</p>\n\n<p>This mean, that you can controll the behavior of your test case, you can catch exceptions, write before-after methods, etc.</p>\n\n<p>I would recommend to read more about rules here <br />\n<a href=\"https://github.com/junit-team/junit/wiki/Rules\">https://github.com/junit-team/junit/wiki/Rules</a></p>\n\n<h5 id=\"rulevsclassrule\">@Rule vs @ClassRule</h5>\n\n<p>There is a difference between <strong>@Rule</strong> and <strong>@ClassRule</strong> in JUnit.We can differ them similiar as <strong>@Before</strong> and <strong>@BeforeClass</strong>. If you will annotate field ,that implements <strong>org.junit.rules.TestRule</strong> with <strong>@Rule</strong> annotation, then it will run in each method. But, if you will annotate this field as <strong>@ClassRule</strong>, then it will run once.</p>\n\n<h5 id=\"usefulrulesexamples\">Useful rules examples</h5>\n\n<script src=\"https://gist.github.com/johnyUA/bac0bac583210d52d5fa.js\"></script>\n\n<h5 id=\"writingyourownrule\">Writing your own rule</h5>\n\n<p>We will write custom <strong>TestRule</strong>, that will log actions. We will have two </p>\n\n<script src=\"https://gist.github.com/johnyUA/15a96f98ddf4ecb01fd1.js\"></script>\n\n<script src=\"https://gist.github.com/johnyUA/47876fc4c56646ddcaa8.js\"></script>\n\n<p>We will get</p>\n\n<pre><code>before: global\nbefore: local\nafter: local\nafter: global\n</code></pre>","image":"","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-04-01T14:11:02.000Z","created_by":1,"updated_at":"2015-07-27T12:03:12.000Z","updated_by":1,"published_at":"2015-04-01T15:23:30.000Z","published_by":1},{"id":26,"uuid":"dc04ba46-7b32-4118-bf53-b229c513bc7e","title":"JHipster context is not loading on DigitalOcean cloud server","slug":"jhipster-context-is-not-loading-on-digitalocean-cloud-server","markdown":"Few weeks ago I started to work with [**JHipster**](https://jhipster.github.io/) - Java Framework for rapid development.I am not going to write a review of this framework - I hope, I will find time to do it later, but today I am going to write some useful information about pitfalls, that you can reach during your work with **JHipster**.\n\n#####Jhipster project unable to start itself on digitalocean\n\nJust FYI, Jhipster is built on top of Spring Boot Framework + Angular JS, so to start application, you need just to execute single command\n\n\tmvn spring-boot:run\n    \nThat's a convenient way for deploying application - no separate application servers, only embedded, only hardcore.\n\nThat's why Spring Boot stores some embedded tomcat server, which contains all properties.\n\nSo, the problem is that sometimes, when you deploy your application on Digital Ocean cloud server - you need to wait for a long time to finish deploying process.\n\nIn my case, the problem with long deploy is that Tomcat loves to use **/dev/random** function, instead of **/dev/urandom**.\n\nTo get a good example of how random and urandom works, just go to terminal and type\n\n\tcat /dev/random\n\nThen try to move your mouse, you will receive additional values.\n\nThen type\n\n\tcat /dev/urandom\n\nAnd you will get your result much more faster.\n\nIn Jhipster/Spring Boot project you can solve this issue with adding custom parameter\n\n\t-Djava.security.egd=file:/dev/./urandom\n\nIn spring-boot-maven-plugin   \n\n\n\n        <plugin>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-maven-plugin</artifactId>\n            <configuration>\n                <jvmArguments>-Djava.rmi.server.hostname=localhost -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=5005 -Djava.security.egd=file:/dev/./urandom</jvmArguments>\n                <arguments>\n                    <argument>--spring.profiles.active=dev</argument>\n                </arguments>\n            </configuration>\n        </plugin>\n\nI described this issue on official Jhipster page:\nhttps://github.com/jhipster/generator-jhipster/issues/1425#issuecomment-97861947","mobiledoc":null,"html":"<p>Few weeks ago I started to work with <a href=\"https://jhipster.github.io/\"><strong>JHipster</strong></a> - Java Framework for rapid development.I am not going to write a review of this framework - I hope, I will find time to do it later, but today I am going to write some useful information about pitfalls, that you can reach during your work with <strong>JHipster</strong>.</p>\n\n<h5 id=\"jhipsterprojectunabletostartitselfondigitalocean\">Jhipster project unable to start itself on digitalocean</h5>\n\n<p>Just FYI, Jhipster is built on top of Spring Boot Framework + Angular JS, so to start application, you need just to execute single command</p>\n\n<pre><code>mvn spring-boot:run\n</code></pre>\n\n<p>That's a convenient way for deploying application - no separate application servers, only embedded, only hardcore.</p>\n\n<p>That's why Spring Boot stores some embedded tomcat server, which contains all properties.</p>\n\n<p>So, the problem is that sometimes, when you deploy your application on Digital Ocean cloud server - you need to wait for a long time to finish deploying process.</p>\n\n<p>In my case, the problem with long deploy is that Tomcat loves to use <strong>/dev/random</strong> function, instead of <strong>/dev/urandom</strong>.</p>\n\n<p>To get a good example of how random and urandom works, just go to terminal and type</p>\n\n<pre><code>cat /dev/random\n</code></pre>\n\n<p>Then try to move your mouse, you will receive additional values.</p>\n\n<p>Then type</p>\n\n<pre><code>cat /dev/urandom\n</code></pre>\n\n<p>And you will get your result much more faster.</p>\n\n<p>In Jhipster/Spring Boot project you can solve this issue with adding custom parameter</p>\n\n<pre><code>-Djava.security.egd=file:/dev/./urandom\n</code></pre>\n\n<p>In spring-boot-maven-plugin   </p>\n\n<pre><code>    &lt;plugin&gt;\n        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n        &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;\n        &lt;configuration&gt;\n            &lt;jvmArguments&gt;-Djava.rmi.server.hostname=localhost -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=5005 -Djava.security.egd=file:/dev/./urandom&lt;/jvmArguments&gt;\n            &lt;arguments&gt;\n                &lt;argument&gt;--spring.profiles.active=dev&lt;/argument&gt;\n            &lt;/arguments&gt;\n        &lt;/configuration&gt;\n    &lt;/plugin&gt;\n</code></pre>\n\n<p>I described this issue on official Jhipster page: <br />\n<a href=\"https://github.com/jhipster/generator-jhipster/issues/1425#issuecomment-97861947\">https://github.com/jhipster/generator-jhipster/issues/1425#issuecomment-97861947</a></p>","image":"","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-04-30T17:00:22.000Z","created_by":1,"updated_at":"2015-07-27T12:02:31.000Z","updated_by":1,"published_at":"2015-04-30T17:17:15.000Z","published_by":1},{"id":27,"uuid":"71ae5fbb-071c-4a6a-aced-ffdff5fa369d","title":"Socket types in a different countries","slug":"socket-types-in-different-countries","markdown":"While planning your travel, don't forget that the type of socket, that u're using - will not always fit with the socket of the country, that you are visiting.\nSo just in case you will need different types of sockets - I store them in infographic below\n![](/content/images/2015/05/Plug-Types-of-the-world-WiFi_Socket-744x570.png)\n\nI think, there is no need to buy socket adapter before visiting some country, I'm sure that you can buy one just after plane lands in airport.","mobiledoc":null,"html":"<p>While planning your travel, don't forget that the type of socket, that u're using - will not always fit with the socket of the country, that you are visiting. <br />\nSo just in case you will need different types of sockets - I store them in infographic below <br />\n<img src=\"/content/images/2015/05/Plug-Types-of-the-world-WiFi_Socket-744x570.png\" alt=\"\" /></p>\n\n<p>I think, there is no need to buy socket adapter before visiting some country, I'm sure that you can buy one just after plane lands in airport.</p>","image":"","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-05-04T16:57:04.000Z","created_by":1,"updated_at":"2015-07-27T12:02:16.000Z","updated_by":1,"published_at":"2015-05-04T17:14:34.000Z","published_by":1},{"id":28,"uuid":"32e17f40-ae43-436f-a761-01485225bd62","title":"Java - MetaSpace vs PermGen","slug":"java-perm-gen-vs-metaspace","markdown":"It's been a long period since Java 8 was introduced.New Java comes with a lot of new features.\n\nOne of these features is the complete removal of the Permanent Generation (PermGen) space which has been announced by Oracle since the release of JDK 7. Interned strings, for example, have already been removed from the PermGen space since JDK 7. The JDK 8 release finalizes its decommissioning. \n\nThis article will share the information that I found so far on the PermGen successor: Metaspace. \n\nThe final specifications, tuning flags and documentation around Metaspace should be available on Java 8 official documentation.\n\n#### Metaspace\n\nThe JDK 8 HotSpot JVM is now using native memory for the representation of class metadata and is called Metaspace; similar to the Oracle JRockit and IBM JVM's.\n\nThe good news is that it means no more **java.lang.OutOfMemoryError: PermGen** space problems and no need for you to tune and monitor this memory space anymorenot so fast. Instead of **java.lang.OutOfMemoryError: PermGen** you are now able to receive **java.lang.OutOfMemoryError: Metadata space** .While this change is invisible by default, we will show you next that you will still need to worry about the class metadata memory footprint. Please also keep in mind that this new feature does not magically eliminate class and classloader memory leaks. You will need to track down these problems using a different approach and by learning the new naming convention.\n\n######PermGen space situation\n\nThis memory space is completely removed.\nThe PermSize and MaxPermSize JVM arguments are ignored and a warning is issued if present at start-up.\n\n######Metaspace memory allocation model\n\nMost allocations for the class metadata are now allocated out of native memory.\nThe klasses that were used to describe class metadata have been removed.\n\n######Metaspace capacity\n\nBy default class metadata allocation is limited by the amount of available native memory (capacity will of course depend if you use a 32-bit JVM vs. 64-bit along with OS virtual memory availability).\nA new flag is available (MaxMetaspaceSize), allowing you to limit the amount of native memory used for class metadata. If you dont specify this flag, the Metaspace will dynamically re-size depending of the application demand at runtime.\n\n######Metaspace garbage collection\n\nGarbage collection of the dead classes and classloaders is triggered once the class metadata usage reaches the MaxMetaspaceSize.\nProper monitoring & tuning of the Metaspace will obviously be required in order to limit the frequency or delay of such garbage collections. Excessive Metaspace garbage collections may be a symptom of classes, classloaders memory leak or inadequate sizing for your application.\n\n######Java heap space impact\n\nSome miscellaneous data has been moved to the Java heap space. This means you may observe an increase of the Java heap space following a future JDK 8 upgrade.\n\nMetaspace monitoring\n\nMetaspace usage is available from the HotSpot 1.8 verbose GC log output.\nJstat & JVisualVM have not been updated at this point based on our testing with b75 and the old PermGen space references are still present.\n\n######Conclusions\n\n* Permanent Generation memory is no longer available in Java 8\n* If you will try to add **-XX:MaxPermSize** or **-XX:PermSize** parameters, you will receive warning.\n* Be default, Metaspace will automatically increase allocated memory, that's the main practical difference betweeb **PermGen** and **MetaSpace**.\n* You can set Maxmetaspace - **-XX:MaxMetaspaceSize=128m**\n* If you will reach max meta space size - Java will fail with exception, similar to PermGen exception.\n","mobiledoc":null,"html":"<p>It's been a long period since Java 8 was introduced.New Java comes with a lot of new features.</p>\n\n<p>One of these features is the complete removal of the Permanent Generation (PermGen) space which has been announced by Oracle since the release of JDK 7. Interned strings, for example, have already been removed from the PermGen space since JDK 7. The JDK 8 release finalizes its decommissioning. </p>\n\n<p>This article will share the information that I found so far on the PermGen successor: Metaspace. </p>\n\n<p>The final specifications, tuning flags and documentation around Metaspace should be available on Java 8 official documentation.</p>\n\n<h4 id=\"metaspace\">Metaspace</h4>\n\n<p>The JDK 8 HotSpot JVM is now using native memory for the representation of class metadata and is called Metaspace; similar to the Oracle JRockit and IBM JVM's.</p>\n\n<p>The good news is that it means no more <strong>java.lang.OutOfMemoryError: PermGen</strong> space problems and no need for you to tune and monitor this memory space anymorenot so fast. Instead of <strong>java.lang.OutOfMemoryError: PermGen</strong> you are now able to receive <strong>java.lang.OutOfMemoryError: Metadata space</strong> .While this change is invisible by default, we will show you next that you will still need to worry about the class metadata memory footprint. Please also keep in mind that this new feature does not magically eliminate class and classloader memory leaks. You will need to track down these problems using a different approach and by learning the new naming convention.</p>\n\n<h6 id=\"permgenspacesituation\">PermGen space situation</h6>\n\n<p>This memory space is completely removed. <br />\nThe PermSize and MaxPermSize JVM arguments are ignored and a warning is issued if present at start-up.</p>\n\n<h6 id=\"metaspacememoryallocationmodel\">Metaspace memory allocation model</h6>\n\n<p>Most allocations for the class metadata are now allocated out of native memory. <br />\nThe klasses that were used to describe class metadata have been removed.</p>\n\n<h6 id=\"metaspacecapacity\">Metaspace capacity</h6>\n\n<p>By default class metadata allocation is limited by the amount of available native memory (capacity will of course depend if you use a 32-bit JVM vs. 64-bit along with OS virtual memory availability). <br />\nA new flag is available (MaxMetaspaceSize), allowing you to limit the amount of native memory used for class metadata. If you dont specify this flag, the Metaspace will dynamically re-size depending of the application demand at runtime.</p>\n\n<h6 id=\"metaspacegarbagecollection\">Metaspace garbage collection</h6>\n\n<p>Garbage collection of the dead classes and classloaders is triggered once the class metadata usage reaches the MaxMetaspaceSize. <br />\nProper monitoring &amp; tuning of the Metaspace will obviously be required in order to limit the frequency or delay of such garbage collections. Excessive Metaspace garbage collections may be a symptom of classes, classloaders memory leak or inadequate sizing for your application.</p>\n\n<h6 id=\"javaheapspaceimpact\">Java heap space impact</h6>\n\n<p>Some miscellaneous data has been moved to the Java heap space. This means you may observe an increase of the Java heap space following a future JDK 8 upgrade.</p>\n\n<p>Metaspace monitoring</p>\n\n<p>Metaspace usage is available from the HotSpot 1.8 verbose GC log output. <br />\nJstat &amp; JVisualVM have not been updated at this point based on our testing with b75 and the old PermGen space references are still present.</p>\n\n<h6 id=\"conclusions\">Conclusions</h6>\n\n<ul>\n<li>Permanent Generation memory is no longer available in Java 8</li>\n<li>If you will try to add <strong>-XX:MaxPermSize</strong> or <strong>-XX:PermSize</strong> parameters, you will receive warning.</li>\n<li>Be default, Metaspace will automatically increase allocated memory, that's the main practical difference betweeb <strong>PermGen</strong> and <strong>MetaSpace</strong>.</li>\n<li>You can set Maxmetaspace - <strong>-XX:MaxMetaspaceSize=128m</strong></li>\n<li>If you will reach max meta space size - Java will fail with exception, similar to PermGen exception.</li>\n</ul>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-05-05T10:37:05.000Z","created_by":1,"updated_at":"2015-07-27T12:00:44.000Z","updated_by":1,"published_at":"2015-05-05T11:10:25.000Z","published_by":1},{"id":29,"uuid":"1c3f5a4e-2152-402d-a363-984495a35f75","title":"Next step in my university project","slug":"next-step-in-my-university-project","markdown":"I finally managed to finish migration of all sub projects of my university post-graduate work.For those, who don't know anything about my work - I am first year post-graduate student and I am doing admission web service for Lviv National University of Ivan Franko.\nI started working on this project from October 2014 and from that time I achieved some results - we negotiated with [softserve.ua/](http://softserve.ua/) about cooperation - thet were having their IT Academy and we were willing to have some ui work, so we found each other.\nFew words about this project - it started as admission web site for managing applicants, their orders, enrolments, etc and soon it's backend part started to cover most of the fields in the university management, so we decided to define it as UMS(University Management System).\n\nI managed to gather everything together, so now we have nice-looking github [page](https://github.com/ifnul) of Ivan Frankon University, that collects\n\n* General UMS project\n* Backend Part(Was done by my team)\n* Frontend Admission Part\n\nNice thing to hear is that I send request for [JIRA](https://www.atlassian.com/software/jira) Open Source Program - and I hope that we will receive this product.\n\nI am also happy with the fact that we will continue working with [Jenkins](https://jenkins-ci.org/) - beatiful CI system that saves my time.You can find our jenkins here - [jenkins.lnu.edu.ua](http://jenkins.lnu.edu.ua)\n\nThere is also a [website page](http://ifnul.github.io/ums/) with some information about the project.I hope, in near future we will improve this page.","mobiledoc":null,"html":"<p>I finally managed to finish migration of all sub projects of my university post-graduate work.For those, who don't know anything about my work - I am first year post-graduate student and I am doing admission web service for Lviv National University of Ivan Franko. <br />\nI started working on this project from October 2014 and from that time I achieved some results - we negotiated with <a href=\"http://softserve.ua/\">softserve.ua/</a> about cooperation - thet were having their IT Academy and we were willing to have some ui work, so we found each other. <br />\nFew words about this project - it started as admission web site for managing applicants, their orders, enrolments, etc and soon it's backend part started to cover most of the fields in the university management, so we decided to define it as UMS(University Management System).</p>\n\n<p>I managed to gather everything together, so now we have nice-looking github <a href=\"https://github.com/ifnul\">page</a> of Ivan Frankon University, that collects</p>\n\n<ul>\n<li>General UMS project</li>\n<li>Backend Part(Was done by my team)</li>\n<li>Frontend Admission Part</li>\n</ul>\n\n<p>Nice thing to hear is that I send request for <a href=\"https://www.atlassian.com/software/jira\">JIRA</a> Open Source Program - and I hope that we will receive this product.</p>\n\n<p>I am also happy with the fact that we will continue working with <a href=\"https://jenkins-ci.org/\">Jenkins</a> - beatiful CI system that saves my time.You can find our jenkins here - <a href=\"http://jenkins.lnu.edu.ua\">jenkins.lnu.edu.ua</a></p>\n\n<p>There is also a <a href=\"http://ifnul.github.io/ums/\">website page</a> with some information about the project.I hope, in near future we will improve this page.</p>","image":"","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-05-26T18:31:11.000Z","created_by":1,"updated_at":"2015-07-27T12:08:53.000Z","updated_by":1,"published_at":"2015-05-26T18:50:07.000Z","published_by":1},{"id":30,"uuid":"882a8724-373c-4a3f-9b26-427c19ee8670","title":"Portfolio","slug":"portfolio","markdown":"###**[ivanursul.elance.com](http://ivanursul.elance.com)**\n\nOn this page I put projects, where I was involved.Some of the projects were just backend web services, some were full stack applications.Anyway, if you are interested in hiring me as a frelance developer, you should learn about this projects:\n\n###Online Booking system for restaurants\n#####[www.egogso.com](http://egogso.com/restaurant/home)\n![Egogso.se](/content/images/2015/06/Screen-Shot-2015-06-03-at-10-00-34-PM.png)\nTo be honest, this was my first project, where I was as a developer.I remember, that I was just a junior java developer with some contradictory skills. The real issue was with my team - actually, nobody could review me, because I didn't have senior developers on the project.So, it was a risky project for me. Anyway, I finished him. Later, I switched to another company, so I don't much about this project for now.\n\nI use:\n\n* Java 1.7\n* Spring 3.0\n* Hibernate(Can't remember version)\n* Twitter bootstrap\n* Backbone.js \n* Digital Ocean as a cloud server\n\n###Accountant system.\n#####[as.egogso.se](as.egogso.se)\n![as.egogso.se](/content/images/2015/06/Screen-Shot-2015-06-03-at-10-13-53-PM.png)\nThis was my second project, on which I worked as a backend developer.This was a project, where I started to use unit tests, database migrations, maven multi-module structure, and so on.This also was a project, where I realized why I should use JSR and specifications\n\nI use:\n\n* Spring 4.0\n* JPA(Hibernate)\n* Maven 3\n* Jenkins\n* PostgreSQL\n* Liquibase\n\n\n###Simple Save Loyalty System\n#####[simplesave.me](http://simplesave.me/)\n\nThis was my third project, where I collected most of my knowledge of java.For that time i thought that I have solid knowledge of java.Fortunatelly, I was wrong - I still have things and frameworks to learn.\n![](/content/images/2015/06/Screen-Shot-2015-06-03-at-10-23-25-PM.png)\n\nThis is the most succesful application in my freelance career - it rapidly went to production, there is mobile clients and this app is growing - gaining new customers, and so on.\n\nI use:\n\n* Spring 4\n* JPA\n* MySQL\n* Liquibase\n* RestAssured integration framework\n* jUnit/Mockito for unit tests\n* Maven\n\n###Funny Pictures\n#####[drawmeme.com](http://drawmeme.com/#/home)\n![](/content/images/2015/06/cc73eeda-fbaa-48b7-adad-a640956c27dd.png)\n\nThis is funny project, in which I took part.\nIt's made just for fun and for frameworks investigating.\n\n* Spring 4\n* JPA(Hibernate)\n* imagemagick \n* Maven 3\n\nThe idea of this app is to generate image with thumbnails - for example, if you want to make a joke with about your friend - you can add some text at the top and bottom of picture and send this generated meme to him.\n![](/content/images/2015/06/Screen-Shot-2015-06-03-at-10-32-19-PM.png)\n\n###University management system\n#####[github.com/ifnul/ums](https://github.com/ifnul/ums)\n\nThis is an open - source project, that I am doing.\nThe idea is to make management in my university more clear.You can find more information on github page of [this](https://github.com/ifnul/ums) project.\n![](/content/images/2015/06/Screen-Shot-2015-06-03-at-10-37-01-PM.png)","mobiledoc":null,"html":"<h3 id=\"ivanursulelancecomhttpivanursulelancecom\"><strong><a href=\"http://ivanursul.elance.com\">ivanursul.elance.com</a></strong></h3>\n\n<p>On this page I put projects, where I was involved.Some of the projects were just backend web services, some were full stack applications.Anyway, if you are interested in hiring me as a frelance developer, you should learn about this projects:</p>\n\n<h3 id=\"onlinebookingsystemforrestaurants\">Online Booking system for restaurants</h3>\n\n<h5 id=\"wwwegogsocomhttpegogsocomrestauranthome\"><a href=\"http://egogso.com/restaurant/home\">www.egogso.com</a></h5>\n\n<p><img src=\"/content/images/2015/06/Screen-Shot-2015-06-03-at-10-00-34-PM.png\" alt=\"Egogso.se\" />\nTo be honest, this was my first project, where I was as a developer.I remember, that I was just a junior java developer with some contradictory skills. The real issue was with my team - actually, nobody could review me, because I didn't have senior developers on the project.So, it was a risky project for me. Anyway, I finished him. Later, I switched to another company, so I don't much about this project for now.</p>\n\n<p>I use:</p>\n\n<ul>\n<li>Java 1.7</li>\n<li>Spring 3.0</li>\n<li>Hibernate(Can't remember version)</li>\n<li>Twitter bootstrap</li>\n<li>Backbone.js </li>\n<li>Digital Ocean as a cloud server</li>\n</ul>\n\n<h3 id=\"accountantsystem\">Accountant system.</h3>\n\n<h5 id=\"asegogsoseasegogsose\"><a href=\"as.egogso.se\">as.egogso.se</a></h5>\n\n<p><img src=\"/content/images/2015/06/Screen-Shot-2015-06-03-at-10-13-53-PM.png\" alt=\"as.egogso.se\" />\nThis was my second project, on which I worked as a backend developer.This was a project, where I started to use unit tests, database migrations, maven multi-module structure, and so on.This also was a project, where I realized why I should use JSR and specifications</p>\n\n<p>I use:</p>\n\n<ul>\n<li>Spring 4.0</li>\n<li>JPA(Hibernate)</li>\n<li>Maven 3</li>\n<li>Jenkins</li>\n<li>PostgreSQL</li>\n<li>Liquibase</li>\n</ul>\n\n<h3 id=\"simplesaveloyaltysystem\">Simple Save Loyalty System</h3>\n\n<h5 id=\"simplesavemehttpsimplesaveme\"><a href=\"http://simplesave.me/\">simplesave.me</a></h5>\n\n<p>This was my third project, where I collected most of my knowledge of java.For that time i thought that I have solid knowledge of java.Fortunatelly, I was wrong - I still have things and frameworks to learn. <br />\n<img src=\"/content/images/2015/06/Screen-Shot-2015-06-03-at-10-23-25-PM.png\" alt=\"\" /></p>\n\n<p>This is the most succesful application in my freelance career - it rapidly went to production, there is mobile clients and this app is growing - gaining new customers, and so on.</p>\n\n<p>I use:</p>\n\n<ul>\n<li>Spring 4</li>\n<li>JPA</li>\n<li>MySQL</li>\n<li>Liquibase</li>\n<li>RestAssured integration framework</li>\n<li>jUnit/Mockito for unit tests</li>\n<li>Maven</li>\n</ul>\n\n<h3 id=\"funnypictures\">Funny Pictures</h3>\n\n<h5 id=\"drawmemecomhttpdrawmemecomhome\"><a href=\"http://drawmeme.com/#/home\">drawmeme.com</a></h5>\n\n<p><img src=\"/content/images/2015/06/cc73eeda-fbaa-48b7-adad-a640956c27dd.png\" alt=\"\" /></p>\n\n<p>This is funny project, in which I took part. <br />\nIt's made just for fun and for frameworks investigating.</p>\n\n<ul>\n<li>Spring 4</li>\n<li>JPA(Hibernate)</li>\n<li>imagemagick </li>\n<li>Maven 3</li>\n</ul>\n\n<p>The idea of this app is to generate image with thumbnails - for example, if you want to make a joke with about your friend - you can add some text at the top and bottom of picture and send this generated meme to him. <br />\n<img src=\"/content/images/2015/06/Screen-Shot-2015-06-03-at-10-32-19-PM.png\" alt=\"\" /></p>\n\n<h3 id=\"universitymanagementsystem\">University management system</h3>\n\n<h5 id=\"githubcomifnulumshttpsgithubcomifnulums\"><a href=\"https://github.com/ifnul/ums\">github.com/ifnul/ums</a></h5>\n\n<p>This is an open - source project, that I am doing. <br />\nThe idea is to make management in my university more clear.You can find more information on github page of <a href=\"https://github.com/ifnul/ums\">this</a> project. <br />\n<img src=\"/content/images/2015/06/Screen-Shot-2015-06-03-at-10-37-01-PM.png\" alt=\"\" /></p>","image":null,"featured":0,"page":1,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-06-03T18:54:16.000Z","created_by":1,"updated_at":"2015-07-27T11:59:54.000Z","updated_by":1,"published_at":"2015-06-03T19:19:12.000Z","published_by":1},{"id":31,"uuid":"51dc1718-baf8-4045-9160-f7d9c877ce94","title":"Getting started with Gradle","slug":"getting-started-with-gradle","markdown":"It's been a long time since gradle become so popular framework.Year ago I tried to build one project using **gradle** with my coworkers, but as for me, I didn't understand all the upsides and downsides of this framework. So, today I dedicated one day for investigating this framework and I would like to make some review of it.\n\nThe author of this beatiful framework is Hans Dockter\n\nGradle is a dynamic build tool. That's mean, that you can use this build tool not only for java, but for any language, that can provide plugin for itselft.\n\nGradle is a live project. I mean, Gradle is one of the biggest open source projects in the world, there is 1.5 million builds using gradle every day.\n\n##Agenda\n* Gradle setup\n* Short introduction to Gradle Build\n* Single Module Project setup\n\n####Gradle setup\n\nIf we are using Windows or Linux or OS X, we can install Gradle by following steps:\n\n* [Download the binaries from the downloads page.](http://gradle.org/downloads/)\n* Unpack the zip file and add the GRADLE_HOME/bin directory to the PATH environment variable.\n\nOn linux or OS X, just do following:\n\n* Open your **terminal**\n* **Type** \n> **nano .bashrc**\n\n\tAnd modify this file as in example below\n    <script src=\"https://gist.github.com/ivanursul/f1651718b4ea2a278b3e.js\"></script>\n\n* Then restart your **terminal**\n* If there is no suspicious messages in your terminal after restart, then you can **proceed** with installiation.\n\n\nWe can verify that Gradle is working properly by running the command gradle -v at command prompt. If Gradle is working properly, we should the following output (Windows and Linux users will naturally see a bit different output):\n\n\n> **gradle -v**\n \n--------------------------------------------------------\nGradle 2.3\n------------------------------------------------------------\n\nBuild time:   2015-02-16 05:09:33 UTC\nBuild number: none\nRevision:     586be72bf6e3df1ee7676d1f2a3afd9157341274\n\nGroovy:       2.3.9\nAnt:          Apache Ant(TM) version 1.9.3 compiled on December 23 2013\nJVM:          1.8.0 (Oracle Corporation 25.0-b70)\nOS:           Mac OS X 10.10.3 x86_64\n\n\n####A Short Introduction to Gradle Build\n\nGradle has two basic concepts: projects and tasks. These concepts are explained in the following:\n\n* A project is either something we build (e.g. a jar file) or do (deploy our application to production environment). A project consists of one or more tasks.\n* A task is an atomic unit work which is performed our build (e.g. compiling our project or running tests).\nSo, how are these concepts related to a Gradle build? Well, every Gradle build contains one or more projects.\n\nThe relationships between these concepts are illustrated in the following figure:\n\n![](http://d2x79bjupkp9on.cloudfront.net/wp-content/uploads/gradlebuild.jpg)\n\nWe can configure our Gradle build by using the following configuration files:\n\n* The Gradle build script (build.gradle) specifies a project and its tasks.\n* The Gradle properties file (gradle.properties) is used to configure the properties of the build.\n* The Gradle Settings file (gradle.settings) is optional in a build which has only one project. If our Gradle build has more than one projects, it is mandatory because it describes which projects participate to our build. Every multi-project build must have a settings file in the root project of the project hierarchy.\n\n#### Single Module Project setup\n\nI came from Maven world, so it's obvious for me how to develop java project using maven build tool.But I am a new man in Gradle world.Hence, I will start from describing simple single module application, and then I will enhance our project to multi module app.\n\nSo, first thing you need to do it to create your folder\n\n> cd ~/{your_workspace}\n\n> mkdir gradle-getting-started\n\nOn this step I assume that you've already installed Gradle and are ready to create projects.\n\n> gradle tasks\n\nThis command will list all the available tasks, that you can execute.\n\nBy this time you will be able to run only two tasks\n![](/content/images/2015/06/Screen-Shot-2015-06-13-at-4-11-00-PM.png)\n\n* **init**\n* **wrapper**\n\nPS - By executing gradle init, Gradle also generates wrapper.\n\nSo, execute comamnd below\n\n> **gradle init**\n\nThis command should generate such files:\n![](/content/images/2015/06/Screen-Shot-2015-06-13-at-4-14-25-PM.png)\n\nAfter you create this files, open your favotire IDE(I use **InteliJ IDEA**) and import this folder as Gradle project.In my case it looks like this\n![](/content/images/2015/06/Screen-Shot-2015-06-13-at-5-11-24-PM.png)\n\n**Next** thing you need to do it to create **src/main/java** folder. Just hit **Ctrl+Insert** and create this directories.After that mark java folder as source folder.\n![](/content/images/2015/06/Screen-Shot-2015-06-13-at-5-14-34-PM.png)\n\nNow, edit your build.gradle file - remove all comments from it and add single line\n\n> **apply plugin: 'java'**\n\nThis plugin allows to use all the capabillities of java inside your project.Now you know how easily you can import other languages inside your gradle.For instance, you can import **c** language.You can find more detailed information about language plugins [here](https://docs.gradle.org/2.4/userguide/standard_plugins.html).\n\n> **apply plugin: 'c'**\n\n###Looks great, isn't it ?:)\n![](http://blog.plotr.co.uk/wp-content/uploads/7-summer-jobs-that-look-great-on-your-CV-624x624.jpg)\n\nNow it's time to create some classes.Spring recommends that you create two classes: **HelloWorld**.**java** and **Greeter**.**java**.\n\n<script src=\"https://gist.github.com/ivanursul/6d23f6c91d5759c84cbe.js\"></script>\n\n<script src=\"https://gist.github.com/ivanursul/dbd9ac624c054ab92cfd.js\"></script>\n\nAfter just run\n\n> **gradle build**\n\nTo see the results of the build effort, take a look in the build folder. Therein youll find several directories, including these three notable folders:\n\n* **classes**. The projects compiled .class files.\nreports. \n* **reports** produced by the build (such as test reports).\n* **libs**. Assembled project libraries (usually JAR and/or WAR files).\n\nThe classes folder has .class files that are generated from compiling the Java code. Specifically, you should find HelloWorld.class and Greeter.class.\n\nAt this point, the project doesnt have any library dependencies, so theres nothing in the **dependency_cache** folder.\n\nThe reports folder should contain a report of running unit tests on the project. Because the project doesnt yet have any unit tests, that report will be uninteresting.\n\nThe libs folder should contain a JAR file that is named after the projects folder. Further down, youll see how you can specify the name of the JAR and its version.\n\n#####Adding external dependencies to our application\n\nBefore I start I would like to emphasize on importance of manual loading dependencies into Gradle app from IDEA. I spent some extra minutes in finding out why my deps are not loaded as extra jars.Finally, I realized that there needs to be some Gradle window in IDEA.\nI recommend to read this doc article for resolving this issue\n\n[https://www.jetbrains.com/idea/help/synchronizing-changes-in-gradle-project-and-intellij-idea-project.html](https://www.jetbrains.com/idea/help/synchronizing-changes-in-gradle-project-and-intellij-idea-project.html)\n\nSo, we would like to include Joda Time library into our project.Naturally, we need to find it somewhere.I am using Maven repository for that\n[http://mvnrepository.com/artifact/joda-time/joda-time/2.8](http://mvnrepository.com/artifact/joda-time/joda-time/2.8)\n\nJust transform your build.gradle file into something like this\n<script src=\"https://gist.github.com/ivanursul/2df6862250016ce18119.js\"></script>\n","mobiledoc":null,"html":"<p>It's been a long time since gradle become so popular framework.Year ago I tried to build one project using <strong>gradle</strong> with my coworkers, but as for me, I didn't understand all the upsides and downsides of this framework. So, today I dedicated one day for investigating this framework and I would like to make some review of it.</p>\n\n<p>The author of this beatiful framework is Hans Dockter</p>\n\n<p>Gradle is a dynamic build tool. That's mean, that you can use this build tool not only for java, but for any language, that can provide plugin for itselft.</p>\n\n<p>Gradle is a live project. I mean, Gradle is one of the biggest open source projects in the world, there is 1.5 million builds using gradle every day.</p>\n\n<h2 id=\"agenda\">Agenda</h2>\n\n<ul>\n<li>Gradle setup</li>\n<li>Short introduction to Gradle Build</li>\n<li>Single Module Project setup</li>\n</ul>\n\n<h4 id=\"gradlesetup\">Gradle setup</h4>\n\n<p>If we are using Windows or Linux or OS X, we can install Gradle by following steps:</p>\n\n<ul>\n<li><a href=\"http://gradle.org/downloads/\">Download the binaries from the downloads page.</a></li>\n<li>Unpack the zip file and add the GRADLE_HOME/bin directory to the PATH environment variable.</li>\n</ul>\n\n<p>On linux or OS X, just do following:</p>\n\n<ul>\n<li>Open your <strong>terminal</strong></li>\n<li><p><strong>Type</strong> </p>\n\n<blockquote>\n  <p><strong>nano .bashrc</strong></p>\n</blockquote>\n\n<p>And modify this file as in example below</p>\n\n<script src=\"https://gist.github.com/ivanursul/f1651718b4ea2a278b3e.js\"></script></li>\n<li><p>Then restart your <strong>terminal</strong></p></li>\n<li>If there is no suspicious messages in your terminal after restart, then you can <strong>proceed</strong> with installiation.</li>\n</ul>\n\n<p>We can verify that Gradle is working properly by running the command gradle -v at command prompt. If Gradle is working properly, we should the following output (Windows and Linux users will naturally see a bit different output):</p>\n\n<blockquote>\n  <p><strong>gradle -v</strong></p>\n</blockquote>\n\n<hr />\n\n<h2 id=\"gradle23\">Gradle 2.3  </h2>\n\n<p>Build time:   2015-02-16 05:09:33 UTC <br />\nBuild number: none <br />\nRevision:     586be72bf6e3df1ee7676d1f2a3afd9157341274</p>\n\n<p>Groovy:       2.3.9 <br />\nAnt:          Apache Ant(TM) version 1.9.3 compiled on December 23 2013 <br />\nJVM:          1.8.0 (Oracle Corporation 25.0-b70) <br />\nOS:           Mac OS X 10.10.3 x86_64</p>\n\n<h4 id=\"ashortintroductiontogradlebuild\">A Short Introduction to Gradle Build</h4>\n\n<p>Gradle has two basic concepts: projects and tasks. These concepts are explained in the following:</p>\n\n<ul>\n<li>A project is either something we build (e.g. a jar file) or do (deploy our application to production environment). A project consists of one or more tasks.</li>\n<li>A task is an atomic unit work which is performed our build (e.g. compiling our project or running tests).\nSo, how are these concepts related to a Gradle build? Well, every Gradle build contains one or more projects.</li>\n</ul>\n\n<p>The relationships between these concepts are illustrated in the following figure:</p>\n\n<p><img src=\"http://d2x79bjupkp9on.cloudfront.net/wp-content/uploads/gradlebuild.jpg\" alt=\"\" /></p>\n\n<p>We can configure our Gradle build by using the following configuration files:</p>\n\n<ul>\n<li>The Gradle build script (build.gradle) specifies a project and its tasks.</li>\n<li>The Gradle properties file (gradle.properties) is used to configure the properties of the build.</li>\n<li>The Gradle Settings file (gradle.settings) is optional in a build which has only one project. If our Gradle build has more than one projects, it is mandatory because it describes which projects participate to our build. Every multi-project build must have a settings file in the root project of the project hierarchy.</li>\n</ul>\n\n<h4 id=\"singlemoduleprojectsetup\">Single Module Project setup</h4>\n\n<p>I came from Maven world, so it's obvious for me how to develop java project using maven build tool.But I am a new man in Gradle world.Hence, I will start from describing simple single module application, and then I will enhance our project to multi module app.</p>\n\n<p>So, first thing you need to do it to create your folder</p>\n\n<blockquote>\n  <p>cd ~/{your_workspace}</p>\n  \n  <p>mkdir gradle-getting-started</p>\n</blockquote>\n\n<p>On this step I assume that you've already installed Gradle and are ready to create projects.</p>\n\n<blockquote>\n  <p>gradle tasks</p>\n</blockquote>\n\n<p>This command will list all the available tasks, that you can execute.</p>\n\n<p>By this time you will be able to run only two tasks <br />\n<img src=\"/content/images/2015/06/Screen-Shot-2015-06-13-at-4-11-00-PM.png\" alt=\"\" /></p>\n\n<ul>\n<li><strong>init</strong></li>\n<li><strong>wrapper</strong></li>\n</ul>\n\n<p>PS - By executing gradle init, Gradle also generates wrapper.</p>\n\n<p>So, execute comamnd below</p>\n\n<blockquote>\n  <p><strong>gradle init</strong></p>\n</blockquote>\n\n<p>This command should generate such files: <br />\n<img src=\"/content/images/2015/06/Screen-Shot-2015-06-13-at-4-14-25-PM.png\" alt=\"\" /></p>\n\n<p>After you create this files, open your favotire IDE(I use <strong>InteliJ IDEA</strong>) and import this folder as Gradle project.In my case it looks like this <br />\n<img src=\"/content/images/2015/06/Screen-Shot-2015-06-13-at-5-11-24-PM.png\" alt=\"\" /></p>\n\n<p><strong>Next</strong> thing you need to do it to create <strong>src/main/java</strong> folder. Just hit <strong>Ctrl+Insert</strong> and create this directories.After that mark java folder as source folder.\n<img src=\"/content/images/2015/06/Screen-Shot-2015-06-13-at-5-14-34-PM.png\" alt=\"\" /></p>\n\n<p>Now, edit your build.gradle file - remove all comments from it and add single line</p>\n\n<blockquote>\n  <p><strong>apply plugin: 'java'</strong></p>\n</blockquote>\n\n<p>This plugin allows to use all the capabillities of java inside your project.Now you know how easily you can import other languages inside your gradle.For instance, you can import <strong>c</strong> language.You can find more detailed information about language plugins <a href=\"https://docs.gradle.org/2.4/userguide/standard_plugins.html\">here</a>.</p>\n\n<blockquote>\n  <p><strong>apply plugin: 'c'</strong></p>\n</blockquote>\n\n<h3 id=\"looksgreatisntit\">Looks great, isn't it ?:)</h3>\n\n<p><img src=\"http://blog.plotr.co.uk/wp-content/uploads/7-summer-jobs-that-look-great-on-your-CV-624x624.jpg\" alt=\"\" /></p>\n\n<p>Now it's time to create some classes.Spring recommends that you create two classes: <strong>HelloWorld</strong>.<strong>java</strong> and <strong>Greeter</strong>.<strong>java</strong>.</p>\n\n<script src=\"https://gist.github.com/ivanursul/6d23f6c91d5759c84cbe.js\"></script>\n\n<script src=\"https://gist.github.com/ivanursul/dbd9ac624c054ab92cfd.js\"></script>\n\n<p>After just run</p>\n\n<blockquote>\n  <p><strong>gradle build</strong></p>\n</blockquote>\n\n<p>To see the results of the build effort, take a look in the build folder. Therein youll find several directories, including these three notable folders:</p>\n\n<ul>\n<li><strong>classes</strong>. The projects compiled .class files.\nreports.  </li>\n<li><strong>reports</strong> produced by the build (such as test reports).</li>\n<li><strong>libs</strong>. Assembled project libraries (usually JAR and/or WAR files).</li>\n</ul>\n\n<p>The classes folder has .class files that are generated from compiling the Java code. Specifically, you should find HelloWorld.class and Greeter.class.</p>\n\n<p>At this point, the project doesnt have any library dependencies, so theres nothing in the <strong>dependency_cache</strong> folder.</p>\n\n<p>The reports folder should contain a report of running unit tests on the project. Because the project doesnt yet have any unit tests, that report will be uninteresting.</p>\n\n<p>The libs folder should contain a JAR file that is named after the projects folder. Further down, youll see how you can specify the name of the JAR and its version.</p>\n\n<h5 id=\"addingexternaldependenciestoourapplication\">Adding external dependencies to our application</h5>\n\n<p>Before I start I would like to emphasize on importance of manual loading dependencies into Gradle app from IDEA. I spent some extra minutes in finding out why my deps are not loaded as extra jars.Finally, I realized that there needs to be some Gradle window in IDEA. <br />\nI recommend to read this doc article for resolving this issue</p>\n\n<p><a href=\"https://www.jetbrains.com/idea/help/synchronizing-changes-in-gradle-project-and-intellij-idea-project.html\">https://www.jetbrains.com/idea/help/synchronizing-changes-in-gradle-project-and-intellij-idea-project.html</a></p>\n\n<p>So, we would like to include Joda Time library into our project.Naturally, we need to find it somewhere.I am using Maven repository for that <br />\n<a href=\"http://mvnrepository.com/artifact/joda-time/joda-time/2.8\">http://mvnrepository.com/artifact/joda-time/joda-time/2.8</a></p>\n\n<p>Just transform your build.gradle file into something like this  </p>\n\n<script src=\"https://gist.github.com/ivanursul/2df6862250016ce18119.js\"></script>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-06-13T11:44:29.000Z","created_by":1,"updated_at":"2015-07-27T11:59:06.000Z","updated_by":1,"published_at":"2015-07-21T08:20:22.000Z","published_by":1},{"id":32,"uuid":"222affe0-4ca9-4784-bf34-e70c573d04d2","title":"Spring Security: Avoiding Basic Authentication window in your browser","slug":"spring-security-avoiding-basic-authentication-window-in-your-browser","markdown":"While tunnning your Spring application with Basic Authentication security you can notice that you are receiving Basic Aithentication window in your browser.\n![](/content/images/2015/07/servlet-basic-authentication-xml-secured-popup.png)\n\nThe problem is with [BasicAuthenticationEntryPoint](http://docs.spring.io/spring-security/site/docs/3.1.x/apidocs/org/springframework/security/web/authentication/www/BasicAuthenticationEntryPoint.html) that sends header \n\t`WWW-Authenticate: Basic realm=\"nmrs_m7VKmomQ2YM3:\"`\n\nSo, if you don't want to receive this window in your browser, just create Custom Entry Point:\n\n<script src=\"https://gist.github.com/ivanursul/1c475d8939f070e8cf93.js\"></script>\n\n<script src=\"https://gist.github.com/ivanursul/853f918fbd7ed47ea34d.js\"></script>","mobiledoc":null,"html":"<p>While tunnning your Spring application with Basic Authentication security you can notice that you are receiving Basic Aithentication window in your browser. <br />\n<img src=\"/content/images/2015/07/servlet-basic-authentication-xml-secured-popup.png\" alt=\"\" /></p>\n\n<p>The problem is with <a href=\"http://docs.spring.io/spring-security/site/docs/3.1.x/apidocs/org/springframework/security/web/authentication/www/BasicAuthenticationEntryPoint.html\">BasicAuthenticationEntryPoint</a> that sends header <br />\n    <code>WWW-Authenticate: Basic realm=\"nmrs_m7VKmomQ2YM3:\"</code></p>\n\n<p>So, if you don't want to receive this window in your browser, just create Custom Entry Point:</p>\n\n<script src=\"https://gist.github.com/ivanursul/1c475d8939f070e8cf93.js\"></script>\n\n<script src=\"https://gist.github.com/ivanursul/853f918fbd7ed47ea34d.js\"></script>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-07-20T04:30:06.000Z","created_by":1,"updated_at":"2015-07-27T11:59:24.000Z","updated_by":1,"published_at":"2015-07-20T04:40:36.000Z","published_by":1},{"id":33,"uuid":"dadbd9ae-8760-4eda-af7b-61cebaec0c79","title":"Always add a clean phase in your build execution","slug":"always-add-a-clean-phase-in-your-build-execution","markdown":"Today I faced an issue on Jenkins. I needed to fix jenkins job, which is getting info about sonar metrics, and received an error in job console\n![](/content/images/2015/09/Screen-Shot-2015-09-03-at-11-20-46-AM.png)\n\nIt was weird for me, because I was getting success jobs, everything went right, but as soon as I failed one job due to bad configuration - hell started, and I couldn't understand why there my job is failing all the time.So I began experiments.\n\nAfter several tries I managed to add prefix **clean** phase and everything worked.\nAnother problem was that I was building this job with different parameters and depending on specific parameter I was receiving specific console output.But then I understood that the problem was that I had constant artifacts, that were not cleaned. And I was receiving different output because this parameter was a version of plugin , so that's why I received different output.","mobiledoc":null,"html":"<p>Today I faced an issue on Jenkins. I needed to fix jenkins job, which is getting info about sonar metrics, and received an error in job console <br />\n<img src=\"/content/images/2015/09/Screen-Shot-2015-09-03-at-11-20-46-AM.png\" alt=\"\" /></p>\n\n<p>It was weird for me, because I was getting success jobs, everything went right, but as soon as I failed one job due to bad configuration - hell started, and I couldn't understand why there my job is failing all the time.So I began experiments.</p>\n\n<p>After several tries I managed to add prefix <strong>clean</strong> phase and everything worked. <br />\nAnother problem was that I was building this job with different parameters and depending on specific parameter I was receiving specific console output.But then I understood that the problem was that I had constant artifacts, that were not cleaned. And I was receiving different output because this parameter was a version of plugin , so that's why I received different output.</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-09-03T08:20:34.000Z","created_by":1,"updated_at":"2015-09-03T08:33:15.000Z","updated_by":1,"published_at":"2015-09-03T08:32:19.000Z","published_by":1},{"id":34,"uuid":"e534095c-3f24-4294-adeb-6852b6ba2fc5","title":"Multiple javax.inject.Provider implementations and their binding","slug":"multiple-javax-inject-provider-implementations-and-their-binding","markdown":"Hi everyone!\n\nRecently I became a freelancer and  started to work for one big company. It's great,  I hope to continue work there.And I also started to work on project where Dropwizard stack is used.Naturally, Guice DI framework is used ther.So, today I would like to write an article about how to bind javax.inject.Provider interfaces to their implementation. The thing is that Guice treat javax.inject.Provider specially, and you cannot just to \n\n<script src=\"https://gist.github.com/ivanursul/4d0584056338f6b282db.js\"></script>\n    \nBecause Provider is a generic interface, and google care about initializing this Providers.\nSo, to do that - just do the following\n\n<script src=\"https://gist.github.com/ivanursul/490cdbbd299b850591ee.js\"></script>\n\nThere's no special magic there, you just need to specify generic from your Provider and how to name it.And then u can do something like this\n\n<script src=\"https://gist.github.com/ivanursul/acf36fd94357fecf02e0.js\"></script>\n\nSee you!","mobiledoc":null,"html":"<p>Hi everyone!</p>\n\n<p>Recently I became a freelancer and  started to work for one big company. It's great,  I hope to continue work there.And I also started to work on project where Dropwizard stack is used.Naturally, Guice DI framework is used ther.So, today I would like to write an article about how to bind javax.inject.Provider interfaces to their implementation. The thing is that Guice treat javax.inject.Provider specially, and you cannot just to </p>\n\n<script src=\"https://gist.github.com/ivanursul/4d0584056338f6b282db.js\"></script>\n\n<p>Because Provider is a generic interface, and google care about initializing this Providers. <br />\nSo, to do that - just do the following</p>\n\n<script src=\"https://gist.github.com/ivanursul/490cdbbd299b850591ee.js\"></script>\n\n<p>There's no special magic there, you just need to specify generic from your Provider and how to name it.And then u can do something like this</p>\n\n<script src=\"https://gist.github.com/ivanursul/acf36fd94357fecf02e0.js\"></script>\n\n<p>See you!</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-09-12T17:27:10.000Z","created_by":1,"updated_at":"2015-09-12T17:38:26.000Z","updated_by":1,"published_at":"2015-09-12T17:38:26.000Z","published_by":1},{"id":35,"uuid":"e23c1efa-dee0-4c44-9ac5-dbd6e9c6beb4","title":"How to work with maven javacc plugin","slug":"how-to-work-with-maven-javacc-plugin","markdown":"I had to do some work with google-visualization-java github project.\nI needed to change some logic for correct google visualization query.\nI found, that guys were using **javacc** compiler.\n\nOfficial site says: Java Compiler Compiler tm (JavaCC tm) is the most popular parser generator for use with Java tm applications. A parser generator is a tool that reads a grammar specification and converts it to a Java program that can recognize matches to the grammar. In addition to the parser generator itself, JavaCC provides other standard capabilities related to parser generation such as tree building (via a tool called JJTree included with JavaCC), actions, debugging, etc.\n\nTo inject javacc inside your maven project, add following plugin\n\n```\n      <plugin>\n        <groupId>org.codehaus.mojo</groupId>\n        <artifactId>javacc-maven-plugin</artifactId>\n        <version>2.5</version>\n        <executions>\n          <execution>\n            <id>javacc</id>\n            <goals>\n              <goal>javacc</goal>\n            </goals>\n            <configuration>\n              <sourceDirectory>${basedir}/src/main/java</sourceDirectory>\n            </configuration>\n          </execution>\n        </executions>\n      </plugin>\n```\n\nSteps you need to do with your javacc project:\n\n* Open terminal\n* cd to project\n* mvn clean generate-sources\n\nThat's it.","mobiledoc":null,"html":"<p>I had to do some work with google-visualization-java github project. <br />\nI needed to change some logic for correct google visualization query. <br />\nI found, that guys were using <strong>javacc</strong> compiler.</p>\n\n<p>Official site says: Java Compiler Compiler tm (JavaCC tm) is the most popular parser generator for use with Java tm applications. A parser generator is a tool that reads a grammar specification and converts it to a Java program that can recognize matches to the grammar. In addition to the parser generator itself, JavaCC provides other standard capabilities related to parser generation such as tree building (via a tool called JJTree included with JavaCC), actions, debugging, etc.</p>\n\n<p>To inject javacc inside your maven project, add following plugin</p>\n\n<pre><code>      &lt;plugin&gt;\n        &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;\n        &lt;artifactId&gt;javacc-maven-plugin&lt;/artifactId&gt;\n        &lt;version&gt;2.5&lt;/version&gt;\n        &lt;executions&gt;\n          &lt;execution&gt;\n            &lt;id&gt;javacc&lt;/id&gt;\n            &lt;goals&gt;\n              &lt;goal&gt;javacc&lt;/goal&gt;\n            &lt;/goals&gt;\n            &lt;configuration&gt;\n              &lt;sourceDirectory&gt;${basedir}/src/main/java&lt;/sourceDirectory&gt;\n            &lt;/configuration&gt;\n          &lt;/execution&gt;\n        &lt;/executions&gt;\n      &lt;/plugin&gt;\n</code></pre>\n\n<p>Steps you need to do with your javacc project:</p>\n\n<ul>\n<li>Open terminal</li>\n<li>cd to project</li>\n<li>mvn clean generate-sources</li>\n</ul>\n\n<p>That's it.</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-10-22T17:36:58.000Z","created_by":1,"updated_at":"2015-10-23T05:45:19.000Z","updated_by":1,"published_at":"2015-10-23T05:45:19.000Z","published_by":1},{"id":36,"uuid":"1b87b322-43e1-42e1-97fc-fa3de6e4fc04","title":"Lazybones - a cool instrument, which can save your time during a project setup","slug":"lazybones-a-cool-instrument-which-can-save-your-time","markdown":"How long does it takes for you to create a new project ? And how often are you doing this routine job ? Do you remember, how interesting for you is to test some feature, which you want to test/try, but how hard is it to configure initial project ? \nThere's so many frameworks in java world, and for me it's extremely hard to remember initial steps and configurations for each of them. And logically, if you do al this teps by your hands, then something can potencially go wrong\n![alt](http://i.imgur.com/8eYyobe.gif)\n\nLet's start with a question \"Why you need to create a new project\":\n\n* You'd like to test something in your favorite project.\n* You're working in a microserviced worlds, and you start new microservice every day...\n* You need to create a POC and test how it works.\n* Anything else\n\nPersonally I don't want to store all this configuration steps in my head, and I prefer to use something, which can the work for me. That's why I'd like to recommend a cool instrument for creating project skeletons in extremely convenient way - lazybones. Let's demonstrate how to setup a [Gatling](http://gatling.io/) project\n\n\tlazybones create https://bintray.com/ivanursul/lazybones-templates/download_file\\?file_path\\=gatling-template-0.1.zip load-tests\n    \nQuite easy, don't you find ? Especially, when you can set your own templates, and tune them, add README.md, custom folders, create folders, etc.\n![alt](http://i.imgur.com/OX4eZnQ.gif)\n\nLazybones was born out of frustration that [Ratpack](https://ratpack.io)\ndoes not and will not have a command line tool that will bootstrap a project.\n\nI recommend to review their [README.md](https://github.com/pledbrook/lazybones/blob/master/README.md) to understand all the features, my article is just an overview on how I understand this instument.\n\n######How to install lazybones\n\n* Install [sdkman](http://sdkman.io/)\n* Install lazybones \n\t`sdk install lazybones`\n\n###### How to use it\n\nLet's create a [Ratpack](https://ratpack.io) project \n`lazybones create ratpack 1.2.0 my-rat-app`\nYou will get intro text about what this template is about.\n![alt](/content/images/2016/05/Screen-Shot-2016-05-25-at-1-16-50-PM.png)\n\nI think it's a true way of starting your new project with README for some technology, because you can coordinate yourself with a steps you need to do with this project.\n\n#####Available templates\nBy the time I find this instument, there was not so many useful templates for me. To see, what templates are available, just type\n\n\tlazybones list\n    \nThis command will list all the templates, which are available for now. You can read [more](https://github.com/pledbrook/lazybones#finding-out-what-templates-are-available) about available templates on github. It's also not so clear for me, how to list all custom templates, which users create for themselves, and push to bintray. You can read how to import custom repos [here](https://github.com/pledbrook/lazybones#the-project-templates)\n\n\n#####Cook templates for yourself\nComplete document on how to create custom templates is located on [github](https://github.com/pledbrook/lazybones#the-project-templates)\nI'll describe how I understand it:\n\n* Create account on [bintray.com](https://bintray.com/)\n* Get apiKey from settings page.\n* Create lazybones template project from lazybones template(we're lazy, don't forget it)\n\t`lazybones create lazybones-project my-lzb-templates`\n* Create a folder in templates folder. E.g - myTemplate\n* Build your folder according to [this](https://github.com/pledbrook/lazybones/wiki/Template-developers-guide#creating-a-template) document\n* `cd my-lzb-templates`\n* `./gradlew installAllTemplates`\n* `lazybones create myTemplate 1.0-SNAPSHOT my-new-project`\n","mobiledoc":null,"html":"<p>How long does it takes for you to create a new project ? And how often are you doing this routine job ? Do you remember, how interesting for you is to test some feature, which you want to test/try, but how hard is it to configure initial project ? <br />\nThere's so many frameworks in java world, and for me it's extremely hard to remember initial steps and configurations for each of them. And logically, if you do al this teps by your hands, then something can potencially go wrong <br />\n<img src=\"http://i.imgur.com/8eYyobe.gif\" alt=\"alt\" /></p>\n\n<p>Let's start with a question \"Why you need to create a new project\":</p>\n\n<ul>\n<li>You'd like to test something in your favorite project.</li>\n<li>You're working in a microserviced worlds, and you start new microservice every day...</li>\n<li>You need to create a POC and test how it works.</li>\n<li>Anything else</li>\n</ul>\n\n<p>Personally I don't want to store all this configuration steps in my head, and I prefer to use something, which can the work for me. That's why I'd like to recommend a cool instrument for creating project skeletons in extremely convenient way - lazybones. Let's demonstrate how to setup a <a href=\"http://gatling.io/\">Gatling</a> project</p>\n\n<pre><code>lazybones create https://bintray.com/ivanursul/lazybones-templates/download_file\\?file_path\\=gatling-template-0.1.zip load-tests\n</code></pre>\n\n<p>Quite easy, don't you find ? Especially, when you can set your own templates, and tune them, add README.md, custom folders, create folders, etc. <br />\n<img src=\"http://i.imgur.com/OX4eZnQ.gif\" alt=\"alt\" /></p>\n\n<p>Lazybones was born out of frustration that <a href=\"https://ratpack.io\">Ratpack</a> <br />\ndoes not and will not have a command line tool that will bootstrap a project.</p>\n\n<p>I recommend to review their <a href=\"https://github.com/pledbrook/lazybones/blob/master/README.md\">README.md</a> to understand all the features, my article is just an overview on how I understand this instument.</p>\n\n<h6 id=\"howtoinstalllazybones\">How to install lazybones</h6>\n\n<ul>\n<li>Install <a href=\"http://sdkman.io/\">sdkman</a></li>\n<li>Install lazybones \n<code>sdk install lazybones</code></li>\n</ul>\n\n<h6 id=\"howtouseit\">How to use it</h6>\n\n<p>Let's create a <a href=\"https://ratpack.io\">Ratpack</a> project <br />\n<code>lazybones create ratpack 1.2.0 my-rat-app</code>\nYou will get intro text about what this template is about. <br />\n<img src=\"/content/images/2016/05/Screen-Shot-2016-05-25-at-1-16-50-PM.png\" alt=\"alt\" /></p>\n\n<p>I think it's a true way of starting your new project with README for some technology, because you can coordinate yourself with a steps you need to do with this project.</p>\n\n<h5 id=\"availabletemplates\">Available templates</h5>\n\n<p>By the time I find this instument, there was not so many useful templates for me. To see, what templates are available, just type</p>\n\n<pre><code>lazybones list\n</code></pre>\n\n<p>This command will list all the templates, which are available for now. You can read <a href=\"https://github.com/pledbrook/lazybones#finding-out-what-templates-are-available\">more</a> about available templates on github. It's also not so clear for me, how to list all custom templates, which users create for themselves, and push to bintray. You can read how to import custom repos <a href=\"https://github.com/pledbrook/lazybones#the-project-templates\">here</a></p>\n\n<h5 id=\"cooktemplatesforyourself\">Cook templates for yourself</h5>\n\n<p>Complete document on how to create custom templates is located on <a href=\"https://github.com/pledbrook/lazybones#the-project-templates\">github</a> <br />\nI'll describe how I understand it:</p>\n\n<ul>\n<li>Create account on <a href=\"https://bintray.com/\">bintray.com</a></li>\n<li>Get apiKey from settings page.</li>\n<li>Create lazybones template project from lazybones template(we're lazy, don't forget it)\n<code>lazybones create lazybones-project my-lzb-templates</code></li>\n<li>Create a folder in templates folder. E.g - myTemplate</li>\n<li>Build your folder according to <a href=\"https://github.com/pledbrook/lazybones/wiki/Template-developers-guide#creating-a-template\">this</a> document</li>\n<li><code>cd my-lzb-templates</code></li>\n<li><code>./gradlew installAllTemplates</code></li>\n<li><code>lazybones create myTemplate 1.0-SNAPSHOT my-new-project</code></li>\n</ul>","image":"/content/images/2016/05/lazy-bear.jpg","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-05-25T08:41:08.000Z","created_by":1,"updated_at":"2016-05-26T06:28:46.000Z","updated_by":1,"published_at":"2016-05-25T11:03:04.000Z","published_by":1},{"id":37,"uuid":"cd573343-afe5-42cf-8e96-adc874a311ba","title":"Do we really know our application performance behaviour ?","slug":"performance-testing-explained","markdown":"Why do performance testing ? Are you asking this question ? You maybe asking \"are we ready to go live?\". You have enough functional tests, they are working well, your business logic is well tested, and you are sure you won't have any troubles on your production servers. On the other side, you have lot's of infrastructure work, which is not covered by your tests. Let's say, you have few applications, couple of databases, cached layer, and of course, load balancer layer. \n\nWhat about failover, are our load balancer working correctly ? Oh, by the way, what if we run our load test for a long period of time, what will happen ? Will you notice some performance degradation after ? \n\nAnother thing, your application was successful enough to double its transactions, will it's performance behave the same after ? \n\nPerformance testing can answer this questions, and if you don't know the answers for them before running on production, then, eventually, your customers will answer them. Testing is all about risk - you have a choice - do it, or skip. And if you are lucky enough to properly write your application without writing a single test - then you can save a lot of money. In practice, you're a human, which more or less, has some issues besides your work, and, honestly, you can't write a software without mistakes. That's why I advocate for writing performance tests, and analysing them in the future.\n\n### <a href=\"#whatis\" name=\"whatis\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> What is performance testing ?\n\nThere're a lot of synonyms: load testing, stress testing, performance testing, capacity testing, volume testing, non-functional testing. The goal is to evaluate end user experience in realistic scenarios. It can allow you to find out some interesting facts - let's say, you can find, that your application server can't handle more that 10k requests per second, and you need to do something with it. Performance testing is based on real business requirements, which mean, that sending 10k concurrent requests to a single home page isn't a true performance test, because your application more likely behaves not like that. \n\n### <a href=\"#criteria\" name=\"criteria\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Criteria of a good performance test\n\nMy understanding is that you should always have a concurrent users, which run different relevant scenarios. Each independent user should have natural speed, and should execute requests as in the real world. As a result, you will receive unit of answer - response time, which will be analysed later, and this analysis should answer most of the questions about your future production infrastructure. Normally, per given unit of time, you should know what load did you made, and what average response time did you get.\n\n### <a href=\"#preface\" name=\"preface\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Preface\n\nI'm going to share some examples of performance testing, and essentially, I need to share some examples. I'm going to use [gatling stress tool](http://gatling.io/) for this. It's written in scala, and has wonderful DSL for writing tests. Gatling has good [documentation](http://gatling.io/docs/2.2.2/), so feel free to read it in your free time.\n\n### <a href=\"#type\" name=\"type\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Type of tests\n\n\n![](/content/images/2016/07/performance00123844D5EB--1-.png)\n\n### <a href=\"#stress\" name=\"stress\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Stress test\n\nThe idea is to find the limits of your system. You'll find your maximum design capacity, which is one of the answers. According to your **mdc** your application can be planned accordingly, and without troubles. If often comes, that your maximum design capacity is not enough, so you need to tune your application, maybe find some bottlenecks, fix them, and run your stress test once again. \n\n### <a href=\"#peak\" name=\"peak\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Peak Load Test\n\nA peak load test is something you use to simulate some peak load over some period of time, which often happen in your application. Let's say, you have more visits over a weekends. The key goal is to make sure, that your application has no degradation during peak test. Remember, peak load is not the average load, it's something called **maximum** or **peak** **load** You should find out it during performance planning phase, and build your tests accordingly to make sure there is no degradation.\n\n### <a href=\"#soak\" name=\"soak\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Soak Test\n\nSoak testing is something you run for a long period of time to make sure you have no memory leaks in your application. This happen if you write your application with an statefull style, and some objects in your application start to grow over some period of time.\n\nLet's say, you have some endpoint, which you want to test. If you will test your application with an average load for a longer period of time, you should notice memory leak and appropriate problems in your application.\n\n```\nController {\n\n    list requests\n\n    @'/endpoint'\n    endpoint(HttpRequest request) {\n         // storing request in memory\n         // Don't ask why\n         requests.add(request);\n         var something = ...\n         return something;\n    } \n\n}\n```\n\nIn example above, your app won't be able to store enough requests after some time, and will fail with outofmemory exception.\n\n**Practical example**: We have three endpoints, and I'll continuosly send requests to them over 24 hours. If you have memory leaks, this test should show the problems.\n\n<script src=\"https://gist.github.com/ivanursul/712d8af0e7dc5e3c25ec7b96195d6c01.js\"></script>\n\nPay attention to simulation setup section:\n\n> constantUsersPerSec(5) during(24 hours)\n\nWe have the same load over all period of time, so we don't expect any spikes, and so on, we only concentrate on getting long running problems.\n\n### <a href=\"#spike\" name=\"spike\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Spike Test\n\nThe only purpose of spike test is to find out if your system can survive, if your test load will be greater, than maximum design capacity. The most positive result of spike test is that your service average response time will be big and not acceptable for end users. The worst option - your service will fall down, which mean you need to do something with it.\n\n### <a href=\"#rethink\" name=\"rethink\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Rethink\n\nThe importance of writing tests had been proofed by many project failures. It's a common knowledge, that writing tests is an essential part of successful project. One good think about tests is that you will no longer need to check everything manually, because you'll have a regression history and will fully rely on previous test result. Performance test regression is a good thing to have in your project, believe me.\n\n### <a href=\"#links\" name=\"links\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Links\n\n\n* [Performance Basics: Peak Load Testing  Dont Suck like Bank of America](https://www.joecolantonio.com/2011/10/15/performance-basics-peak-load-testing-don%E2%80%99t-suck-like-bank-of-america/)\n\n* [What is soak testing ?](http://www.tutorialspoint.com/software_testing_dictionary/soak_testing.htm)","mobiledoc":null,"html":"<p>Why do performance testing ? Are you asking this question ? You maybe asking \"are we ready to go live?\". You have enough functional tests, they are working well, your business logic is well tested, and you are sure you won't have any troubles on your production servers. On the other side, you have lot's of infrastructure work, which is not covered by your tests. Let's say, you have few applications, couple of databases, cached layer, and of course, load balancer layer. </p>\n\n<p>What about failover, are our load balancer working correctly ? Oh, by the way, what if we run our load test for a long period of time, what will happen ? Will you notice some performance degradation after ? </p>\n\n<p>Another thing, your application was successful enough to double its transactions, will it's performance behave the same after ? </p>\n\n<p>Performance testing can answer this questions, and if you don't know the answers for them before running on production, then, eventually, your customers will answer them. Testing is all about risk - you have a choice - do it, or skip. And if you are lucky enough to properly write your application without writing a single test - then you can save a lot of money. In practice, you're a human, which more or less, has some issues besides your work, and, honestly, you can't write a software without mistakes. That's why I advocate for writing performance tests, and analysing them in the future.</p>\n\n<h3 id=\"ahrefwhatisnamewhatisiclassanchorfafalinkariahiddentrueiawhatisperformancetesting\"><a href=\"#whatis\" name=\"whatis\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> What is performance testing ?</h3>\n\n<p>There're a lot of synonyms: load testing, stress testing, performance testing, capacity testing, volume testing, non-functional testing. The goal is to evaluate end user experience in realistic scenarios. It can allow you to find out some interesting facts - let's say, you can find, that your application server can't handle more that 10k requests per second, and you need to do something with it. Performance testing is based on real business requirements, which mean, that sending 10k concurrent requests to a single home page isn't a true performance test, because your application more likely behaves not like that. </p>\n\n<h3 id=\"ahrefcriterianamecriteriaiclassanchorfafalinkariahiddentrueiacriteriaofagoodperformancetest\"><a href=\"#criteria\" name=\"criteria\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Criteria of a good performance test</h3>\n\n<p>My understanding is that you should always have a concurrent users, which run different relevant scenarios. Each independent user should have natural speed, and should execute requests as in the real world. As a result, you will receive unit of answer - response time, which will be analysed later, and this analysis should answer most of the questions about your future production infrastructure. Normally, per given unit of time, you should know what load did you made, and what average response time did you get.</p>\n\n<h3 id=\"ahrefprefacenameprefaceiclassanchorfafalinkariahiddentrueiapreface\"><a href=\"#preface\" name=\"preface\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Preface</h3>\n\n<p>I'm going to share some examples of performance testing, and essentially, I need to share some examples. I'm going to use <a href=\"http://gatling.io/\">gatling stress tool</a> for this. It's written in scala, and has wonderful DSL for writing tests. Gatling has good <a href=\"http://gatling.io/docs/2.2.2/\">documentation</a>, so feel free to read it in your free time.</p>\n\n<h3 id=\"ahreftypenametypeiclassanchorfafalinkariahiddentrueiatypeoftests\"><a href=\"#type\" name=\"type\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Type of tests</h3>\n\n<p><img src=\"/content/images/2016/07/performance00123844D5EB--1-.png\" alt=\"\" /></p>\n\n<h3 id=\"ahrefstressnamestressiclassanchorfafalinkariahiddentrueiastresstest\"><a href=\"#stress\" name=\"stress\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Stress test</h3>\n\n<p>The idea is to find the limits of your system. You'll find your maximum design capacity, which is one of the answers. According to your <strong>mdc</strong> your application can be planned accordingly, and without troubles. If often comes, that your maximum design capacity is not enough, so you need to tune your application, maybe find some bottlenecks, fix them, and run your stress test once again. </p>\n\n<h3 id=\"ahrefpeaknamepeakiclassanchorfafalinkariahiddentrueiapeakloadtest\"><a href=\"#peak\" name=\"peak\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Peak Load Test</h3>\n\n<p>A peak load test is something you use to simulate some peak load over some period of time, which often happen in your application. Let's say, you have more visits over a weekends. The key goal is to make sure, that your application has no degradation during peak test. Remember, peak load is not the average load, it's something called <strong>maximum</strong> or <strong>peak</strong> <strong>load</strong> You should find out it during performance planning phase, and build your tests accordingly to make sure there is no degradation.</p>\n\n<h3 id=\"ahrefsoaknamesoakiclassanchorfafalinkariahiddentrueiasoaktest\"><a href=\"#soak\" name=\"soak\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Soak Test</h3>\n\n<p>Soak testing is something you run for a long period of time to make sure you have no memory leaks in your application. This happen if you write your application with an statefull style, and some objects in your application start to grow over some period of time.</p>\n\n<p>Let's say, you have some endpoint, which you want to test. If you will test your application with an average load for a longer period of time, you should notice memory leak and appropriate problems in your application.</p>\n\n<pre><code>Controller {\n\n    list requests\n\n    @'/endpoint'\n    endpoint(HttpRequest request) {\n         // storing request in memory\n         // Don't ask why\n         requests.add(request);\n         var something = ...\n         return something;\n    } \n\n}\n</code></pre>\n\n<p>In example above, your app won't be able to store enough requests after some time, and will fail with outofmemory exception.</p>\n\n<p><strong>Practical example</strong>: We have three endpoints, and I'll continuosly send requests to them over 24 hours. If you have memory leaks, this test should show the problems.</p>\n\n<script src=\"https://gist.github.com/ivanursul/712d8af0e7dc5e3c25ec7b96195d6c01.js\"></script>\n\n<p>Pay attention to simulation setup section:</p>\n\n<blockquote>\n  <p>constantUsersPerSec(5) during(24 hours)</p>\n</blockquote>\n\n<p>We have the same load over all period of time, so we don't expect any spikes, and so on, we only concentrate on getting long running problems.</p>\n\n<h3 id=\"ahrefspikenamespikeiclassanchorfafalinkariahiddentrueiaspiketest\"><a href=\"#spike\" name=\"spike\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Spike Test</h3>\n\n<p>The only purpose of spike test is to find out if your system can survive, if your test load will be greater, than maximum design capacity. The most positive result of spike test is that your service average response time will be big and not acceptable for end users. The worst option - your service will fall down, which mean you need to do something with it.</p>\n\n<h3 id=\"ahrefrethinknamerethinkiclassanchorfafalinkariahiddentrueiarethink\"><a href=\"#rethink\" name=\"rethink\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Rethink</h3>\n\n<p>The importance of writing tests had been proofed by many project failures. It's a common knowledge, that writing tests is an essential part of successful project. One good think about tests is that you will no longer need to check everything manually, because you'll have a regression history and will fully rely on previous test result. Performance test regression is a good thing to have in your project, believe me.</p>\n\n<h3 id=\"ahreflinksnamelinksiclassanchorfafalinkariahiddentrueialinks\"><a href=\"#links\" name=\"links\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Links</h3>\n\n<ul>\n<li><p><a href=\"https://www.joecolantonio.com/2011/10/15/performance-basics-peak-load-testing-don%E2%80%99t-suck-like-bank-of-america/\">Performance Basics: Peak Load Testing  Dont Suck like Bank of America</a></p></li>\n<li><p><a href=\"http://www.tutorialspoint.com/software_testing_dictionary/soak_testing.htm\">What is soak testing ?</a></p></li>\n</ul>","image":null,"featured":1,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":"Do we really know our application performance behaviour ?","meta_description":"This article is about importance of performance tests. I tried to describe how performance tests can help us to write better software.","author_id":1,"created_at":"2016-05-25T14:05:48.000Z","created_by":1,"updated_at":"2016-12-01T10:36:11.000Z","updated_by":1,"published_at":"2016-07-17T08:20:00.000Z","published_by":1},{"id":38,"uuid":"c1862d8b-17a5-4e40-980e-450753e30d0e","title":"Dockerizing your apps","slug":"dockerizing-your-app","markdown":"###### Preface\nAs some of you may already know, from the last autumn-2015 I'm working as a software engineer in startup company called [Upwork](https://upwork.com), in one of teams. From the time I started working there, I realized, that working on freelance basis is completely different from office work, even if you're working on a good quality freelance job. Why ? At least, because you cant share some knowledge with your coworkers by cuf of coffee in your lunch time. So I decided to convert my blog to some R&D investigation. That's why I'd like to warn you, that everything here is just an investigation, which I do on my free time, so don't take it so critically, if you find something, which you don't agree with, just comment, and let's discuss. Technologies are changing every day, and we need to follow the trend, don't we ?\n\n###### Why containers  ?\nBefore I can proceed with explanation in my head what is the definition of container, let's return to the near part times, where we did everything manually: the dyno age, where it was ok to assemble your proeject [war](https://en.wikipedia.org/wiki/WAR_(file_format)) manually, and deploy it to the server in the same way.\n![alt](https://i.chzbgr.com/full/5328393728/h178595FF/)\nThen, the new era started, and we got build systems. It was cool, because we now could build and deploy our software using build commands. But it wasn't the true way it shold be. Why ? Because it was clear for us, developers, how to build our software, but was it clear for [devops](https://en.wikipedia.org/wiki/DevOps) ? Of course, not, because there was lot's of build systems, and for each of them devops needed to find aproach. It was not ideal, because developers wrote dozens of deployment instructions. \n\nFrom the prospective of past times, now I understand, that if developer had to write some instructions for deployment or even scripts, then he could easily follow some standard or specification - a formal contract, which is known for developer and devops engineer: developer knows how to write deployment instuctions and devops know how to deploy this instuctions.\n\nThis resulted in so called application containers - a strucute, where your application is running, and it's easy to deploy it. One of this containers is [Docker](https://www.docker.com/). Why it's cool ? Because you can deploy application without knowing what's inside. Perfect.\n\n###### What is docker ?\n[Wiki](https://en.wikipedia.org/wiki/Docker_(software)) page describes docker as an \"open-source project that automates the deployment of applications inside software containers\". So, it's a thing called container. \nDocker containers wrap up a piece of software in a complete filesystem that contains everything it needs to run: code, runtime, system tools, system libraries  anything you can install on a server. This guarantees that it will always run the same, regardless of the environment it is running in. So, instead of creating some instructions - you create a container.\n\n###### How it works ?\nOn a high level, you have a **Dockerfile** for your project, and you it describes how container should be started. Docker has a good documentation, and there's no sense on copying words from there. So, just refer to [this](https://docs.docker.com/engine/reference/builder/) part of documentation, and it should be clear for you.\n\nHow this Dockerfile looks like ?\n\n<script src=\"https://gist.github.com/ivanursul/6f13f9e5f3c343d73db07c0903e0d684.js\"></script>\n\n###### How can you I it ?\n\n    docker build -t learning/getting-started .\n\n###### How can I start it ?\n\n    docker run -ti --rm learning/getting-started bash\n\n`-t` means docker will attach preudo-container, \n`-i` means it's running in interactive mode,\n`--rm` means it'll be removed as soon as you quit interactive mode.\n\n\n###### Docker Hub\nIt's a place, where all docker images are located. Think about hub as a version control system. The same here, you create a new version of your image, you push it to hub. Visit [hub.docker.com](https://hub.docker.com/) for more information. Try doing following command to understand how it works\n       \n    docker search ${some-image-name}\n    docker search ubuntu // example\n\n\n\n###### How can it help me in my daily job ?\nImagin you have a new person on your project. You project is a complex system, which deploys not in a trivial way, and you have to show your new developer how he should start your project. Of course, you can say, that you can write a good README file, and your devs will be happy to read it and start working. And there'll be no failures or errors. True story, what else to say.\n![alt](http://i.makeagif.com/media/10-01-2015/hN7OrQ.gif)\nInstead of introducing your project in such insufficient way, delegate this work docker. Let him know how to start everything.\n\n###### We love os x. How to work with docker on it ?\nThe recommended way to work with Docker is to use [Docker Toolbox](https://www.docker.com/products/docker-toolbox). \n\n###### Okay, if I'll fully rely on docker, how can I use it in all my services, including load balancers, etc ?\nYou can use [docker-compose](https://docs.docker.com/compose/). The idea is that you write one file called docker-compose.yml and describe which services do you need. You can describe all the containers, that you need, including load balancers, service discoveries, etc...\n\nHere's how docker-compose.yml should look like:\n```\nweb:\n  build: .\n  ports:\n   - \"8000:8000\"\n  links:\n   - redis\nredis:\n  image: redis\n```\nTake a look at following line\n\n\tbuild .\n\nIt means, that web is a custom container, and you should look for **Dockerfile** in the same folder, from which you start your compose.\n\nAnother interesting part of compose file is **links**. What are they ?\n\nthen you do\n\n\tdocker-compose up\nunder root folder, which contains docker-compose.yml and it should\nbuild your system.\n\nPS - Following [article](https://examples.javacodegeeks.com/devops/docker/docker-compose-example/) works fine as an example.\n\n![alt](/content/images/2016/05/Screen-Shot-2016-05-26-at-1-44-39-PM.png)\n\n###### Docker Compose. What beast is it ?\n\n###### Can I run docker on production servers ? How? Is it worth doing that ?\n\nAt the moment, I see two options:\n\n* Deploy your Docker image to Docker Hub, Install Docker on your prod machine, pull your image from Docker Hub and start it\n* User Docker Swarm\n\n\n###### Links\n* https://examples.javacodegeeks.com/devops/docker/introduction-docker-java-developers/\n* https://examples.javacodegeeks.com/devops/docker/docker-compose-example/\n* http://blog.hypriot.com/post/docker-compose-nodejs-haproxy/\n* https://docs.docker.com/compose/compose-file/\n\n","mobiledoc":null,"html":"<h6 id=\"preface\">Preface</h6>\n\n<p>As some of you may already know, from the last autumn-2015 I'm working as a software engineer in startup company called <a href=\"https://upwork.com\">Upwork</a>, in one of teams. From the time I started working there, I realized, that working on freelance basis is completely different from office work, even if you're working on a good quality freelance job. Why ? At least, because you cant share some knowledge with your coworkers by cuf of coffee in your lunch time. So I decided to convert my blog to some R&amp;D investigation. That's why I'd like to warn you, that everything here is just an investigation, which I do on my free time, so don't take it so critically, if you find something, which you don't agree with, just comment, and let's discuss. Technologies are changing every day, and we need to follow the trend, don't we ?</p>\n\n<h6 id=\"whycontainers\">Why containers  ?</h6>\n\n<p>Before I can proceed with explanation in my head what is the definition of container, let's return to the near part times, where we did everything manually: the dyno age, where it was ok to assemble your proeject <a href=\"https://en.wikipedia.org/wiki/WAR_(file_format)\">war</a> manually, and deploy it to the server in the same way. <br />\n<img src=\"https://i.chzbgr.com/full/5328393728/h178595FF/\" alt=\"alt\" />\nThen, the new era started, and we got build systems. It was cool, because we now could build and deploy our software using build commands. But it wasn't the true way it shold be. Why ? Because it was clear for us, developers, how to build our software, but was it clear for <a href=\"https://en.wikipedia.org/wiki/DevOps\">devops</a> ? Of course, not, because there was lot's of build systems, and for each of them devops needed to find aproach. It was not ideal, because developers wrote dozens of deployment instructions. </p>\n\n<p>From the prospective of past times, now I understand, that if developer had to write some instructions for deployment or even scripts, then he could easily follow some standard or specification - a formal contract, which is known for developer and devops engineer: developer knows how to write deployment instuctions and devops know how to deploy this instuctions.</p>\n\n<p>This resulted in so called application containers - a strucute, where your application is running, and it's easy to deploy it. One of this containers is <a href=\"https://www.docker.com/\">Docker</a>. Why it's cool ? Because you can deploy application without knowing what's inside. Perfect.</p>\n\n<h6 id=\"whatisdocker\">What is docker ?</h6>\n\n<p><a href=\"https://en.wikipedia.org/wiki/Docker_(software)\">Wiki</a> page describes docker as an \"open-source project that automates the deployment of applications inside software containers\". So, it's a thing called container. \nDocker containers wrap up a piece of software in a complete filesystem that contains everything it needs to run: code, runtime, system tools, system libraries  anything you can install on a server. This guarantees that it will always run the same, regardless of the environment it is running in. So, instead of creating some instructions - you create a container.</p>\n\n<h6 id=\"howitworks\">How it works ?</h6>\n\n<p>On a high level, you have a <strong>Dockerfile</strong> for your project, and you it describes how container should be started. Docker has a good documentation, and there's no sense on copying words from there. So, just refer to <a href=\"https://docs.docker.com/engine/reference/builder/\">this</a> part of documentation, and it should be clear for you.</p>\n\n<p>How this Dockerfile looks like ?</p>\n\n<script src=\"https://gist.github.com/ivanursul/6f13f9e5f3c343d73db07c0903e0d684.js\"></script>\n\n<h6 id=\"howcanyouiit\">How can you I it ?</h6>\n\n<pre><code>docker build -t learning/getting-started .\n</code></pre>\n\n<h6 id=\"howcanistartit\">How can I start it ?</h6>\n\n<pre><code>docker run -ti --rm learning/getting-started bash\n</code></pre>\n\n<p><code>-t</code> means docker will attach preudo-container, \n<code>-i</code> means it's running in interactive mode,\n<code>--rm</code> means it'll be removed as soon as you quit interactive mode.</p>\n\n<h6 id=\"dockerhub\">Docker Hub</h6>\n\n<p>It's a place, where all docker images are located. Think about hub as a version control system. The same here, you create a new version of your image, you push it to hub. Visit <a href=\"https://hub.docker.com/\">hub.docker.com</a> for more information. Try doing following command to understand how it works</p>\n\n<pre><code>docker search ${some-image-name}\ndocker search ubuntu // example\n</code></pre>\n\n<h6 id=\"howcanithelpmeinmydailyjob\">How can it help me in my daily job ?</h6>\n\n<p>Imagin you have a new person on your project. You project is a complex system, which deploys not in a trivial way, and you have to show your new developer how he should start your project. Of course, you can say, that you can write a good README file, and your devs will be happy to read it and start working. And there'll be no failures or errors. True story, what else to say. <br />\n<img src=\"http://i.makeagif.com/media/10-01-2015/hN7OrQ.gif\" alt=\"alt\" />\nInstead of introducing your project in such insufficient way, delegate this work docker. Let him know how to start everything.</p>\n\n<h6 id=\"weloveosxhowtoworkwithdockeronit\">We love os x. How to work with docker on it ?</h6>\n\n<p>The recommended way to work with Docker is to use <a href=\"https://www.docker.com/products/docker-toolbox\">Docker Toolbox</a>. </p>\n\n<h6 id=\"okayifillfullyrelyondockerhowcaniuseitinallmyservicesincludingloadbalancersetc\">Okay, if I'll fully rely on docker, how can I use it in all my services, including load balancers, etc ?</h6>\n\n<p>You can use <a href=\"https://docs.docker.com/compose/\">docker-compose</a>. The idea is that you write one file called docker-compose.yml and describe which services do you need. You can describe all the containers, that you need, including load balancers, service discoveries, etc...</p>\n\n<p>Here's how docker-compose.yml should look like:  </p>\n\n<pre><code>web:  \n  build: .\n  ports:\n   - \"8000:8000\"\n  links:\n   - redis\nredis:  \n  image: redis\n</code></pre>\n\n<p>Take a look at following line</p>\n\n<pre><code>build .\n</code></pre>\n\n<p>It means, that web is a custom container, and you should look for <strong>Dockerfile</strong> in the same folder, from which you start your compose.</p>\n\n<p>Another interesting part of compose file is <strong>links</strong>. What are they ?</p>\n\n<p>then you do</p>\n\n<pre><code>docker-compose up\n</code></pre>\n\n<p>under root folder, which contains docker-compose.yml and it should <br />\nbuild your system.</p>\n\n<p>PS - Following <a href=\"https://examples.javacodegeeks.com/devops/docker/docker-compose-example/\">article</a> works fine as an example.</p>\n\n<p><img src=\"/content/images/2016/05/Screen-Shot-2016-05-26-at-1-44-39-PM.png\" alt=\"alt\" /></p>\n\n<h6 id=\"dockercomposewhatbeastisit\">Docker Compose. What beast is it ?</h6>\n\n<h6 id=\"canirundockeronproductionservershowisitworthdoingthat\">Can I run docker on production servers ? How? Is it worth doing that ?</h6>\n\n<p>At the moment, I see two options:</p>\n\n<ul>\n<li>Deploy your Docker image to Docker Hub, Install Docker on your prod machine, pull your image from Docker Hub and start it</li>\n<li>User Docker Swarm</li>\n</ul>\n\n<h6 id=\"links\">Links</h6>\n\n<ul>\n<li><a href=\"https://examples.javacodegeeks.com/devops/docker/introduction-docker-java-developers/\">https://examples.javacodegeeks.com/devops/docker/introduction-docker-java-developers/</a></li>\n<li><a href=\"https://examples.javacodegeeks.com/devops/docker/docker-compose-example/\">https://examples.javacodegeeks.com/devops/docker/docker-compose-example/</a></li>\n<li><a href=\"http://blog.hypriot.com/post/docker-compose-nodejs-haproxy/\">http://blog.hypriot.com/post/docker-compose-nodejs-haproxy/</a></li>\n<li><a href=\"https://docs.docker.com/compose/compose-file/\">https://docs.docker.com/compose/compose-file/</a></li>\n</ul>","image":"/content/images/2016/05/Screen-Shot-2016-05-26-at-12-03-09-AM.png","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-05-25T14:46:56.000Z","created_by":1,"updated_at":"2016-06-03T15:40:26.000Z","updated_by":1,"published_at":"2016-05-26T11:16:54.000Z","published_by":1},{"id":39,"uuid":"bd6ed179-4de3-4ebb-b4ba-a09e1ac9870f","title":"Spectacleapp - forget about your mouse. OSX only.","slug":"spectacleapp-forget-about-your-mouse","markdown":"If you're interested in reducing interaction with your mouse - then use this program. Link - [https://www.spectacleapp.com/](https://www.spectacleapp.com/)\n\nIf you want easily locate your terminals - then it's a good instrument for you\n![alt](/content/images/2016/06/Screen-Shot-2016-06-03-at-1-46-49-PM.png)\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/CI4x2WK5RuY\" frameborder=\"0\" allowfullscreen></iframe>\n\nPS - it's especially useful to move app between your physical monitors. And in case of many Desktops. I found this on their github https://github.com/eczarny/spectacle/issues/15","mobiledoc":null,"html":"<p>If you're interested in reducing interaction with your mouse - then use this program. Link - <a href=\"https://www.spectacleapp.com/\">https://www.spectacleapp.com/</a></p>\n\n<p>If you want easily locate your terminals - then it's a good instrument for you <br />\n<img src=\"/content/images/2016/06/Screen-Shot-2016-06-03-at-1-46-49-PM.png\" alt=\"alt\" /></p>\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/CI4x2WK5RuY\" frameborder=\"0\" allowfullscreen></iframe>\n\n<p>PS - it's especially useful to move app between your physical monitors. And in case of many Desktops. I found this on their github <a href=\"https://github.com/eczarny/spectacle/issues/15\">https://github.com/eczarny/spectacle/issues/15</a></p>","image":"/content/images/2016/06/Screen-Shot-2016-06-03-at-1-42-06-PM.png","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-06-03T10:37:14.000Z","created_by":1,"updated_at":"2016-06-03T10:48:51.000Z","updated_by":1,"published_at":"2016-06-03T10:39:43.000Z","published_by":1},{"id":40,"uuid":"aed268b3-0321-4115-8f7a-b951412f8fb0","title":"tx,context, aop...How to read spring xml namespace magic ?","slug":"tx-context-aop-how-to-read-spring-xml-namespace-magic","markdown":"For those of you who worked with spring application without spring boot, and remembers those times, when you had to make your configuration using xml - did you ever question yourself how the magic is done ? For instance, if you had to create a component scan, you had to do \n`<context:component-scan base-package=\"com.yourpackage.blablabla\" />` how it really did this component scan ?\n\n######spring.handlers\nAnswer is simple - if you really would like to know how **context** or any other spring namespace is working - always start from **spring.handlers** file. It's present in each spring jar file, which have some namespaces. The more spring dependencies you add - the more spring.handlers files you will have \n\n![alt](/content/images/2016/06/Screen-Shot-2016-06-15-at-8-33-29-AM.png)\n\nIn case of spring-context jar, you will have something like this\n\n```\nhttp\\://www.springframework.org/schema/context=org.springframework.context.config.ContextNamespaceHandler\nhttp\\://www.springframework.org/schema/jee=org.springframework.ejb.config.JeeNamespaceHandler\nhttp\\://www.springframework.org/schema/lang=org.springframework.scripting.config.LangNamespaceHandler\nhttp\\://www.springframework.org/schema/task=org.springframework.scheduling.config.TaskNamespaceHandler\nhttp\\://www.springframework.org/schema/cache=org.springframework.cache.config.CacheNamespaceHandler\n```\n\nSo, let's choose **component-scan** attribute.\n\n######component-scan\ncomponent-scan is an attribute of context namespace, so, we need **ContextNamespaceHandler**\n\n```\n/*\n * Copyright 2002-2012 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.springframework.context.config;\n\nimport org.springframework.beans.factory.xml.NamespaceHandlerSupport;\nimport org.springframework.context.annotation.AnnotationConfigBeanDefinitionParser;\nimport org.springframework.context.annotation.ComponentScanBeanDefinitionParser;\n\n/**\n * {@link org.springframework.beans.factory.xml.NamespaceHandler}\n * for the '{@code context}' namespace.\n *\n * @author Mark Fisher\n * @author Juergen Hoeller\n * @since 2.5\n */\npublic class ContextNamespaceHandler extends NamespaceHandlerSupport {\n\n\t@Override\n\tpublic void init() {\n\t\tregisterBeanDefinitionParser(\"property-placeholder\", new PropertyPlaceholderBeanDefinitionParser());\n\t\tregisterBeanDefinitionParser(\"property-override\", new PropertyOverrideBeanDefinitionParser());\n\t\tregisterBeanDefinitionParser(\"annotation-config\", new AnnotationConfigBeanDefinitionParser());\n\t\tregisterBeanDefinitionParser(\"component-scan\", new ComponentScanBeanDefinitionParser());\n\t\tregisterBeanDefinitionParser(\"load-time-weaver\", new LoadTimeWeaverBeanDefinitionParser());\n\t\tregisterBeanDefinitionParser(\"spring-configured\", new SpringConfiguredBeanDefinitionParser());\n\t\tregisterBeanDefinitionParser(\"mbean-export\", new MBeanExportBeanDefinitionParser());\n\t\tregisterBeanDefinitionParser(\"mbean-server\", new MBeanServerBeanDefinitionParser());\n\t}\n\n}\n```\n\nStill nothing magical, just something about **[BeanDefinitionParser](http://docs.spring.io/spring/docs/current/javadoc-api/org/springframework/beans/factory/xml/BeanDefinitionParser.html)**\n\nWe need this line\n\n```\nregisterBeanDefinitionParser(\"component-scan\", new ComponentScanBeanDefinitionParser());\n```\n\nThe key thing is [ComponentScanBeanDefinitionParser](http://docs.spring.io/spring/docs/current/javadoc-api/org/springframework/context/annotation/ComponentScanBeanDefinitionParser.html) and his **parse** method\n\n```\n\t@Override\n\tpublic BeanDefinition parse(Element element, ParserContext parserContext) {\n\t\tString[] basePackages = StringUtils.tokenizeToStringArray(element.getAttribute(BASE_PACKAGE_ATTRIBUTE),\n\t\t\t\tConfigurableApplicationContext.CONFIG_LOCATION_DELIMITERS);\n\n\t\t// Actually scan for bean definitions and register them.\n\t\tClassPathBeanDefinitionScanner scanner = configureScanner(parserContext, element);\n\t\tSet<BeanDefinitionHolder> beanDefinitions = scanner.doScan(basePackages);\n\t\tregisterComponents(parserContext.getReaderContext(), beanDefinitions, element);\n\n\t\treturn null;\n\t}\n```\n\nYou can, of course, dig deeper, and find many spring rules and constraints, for example one about scope-proxy attribute\n\n```\n\t\tif (element.hasAttribute(SCOPED_PROXY_ATTRIBUTE)) {\n\t\t\tString mode = element.getAttribute(SCOPED_PROXY_ATTRIBUTE);\n\t\t\tif (\"targetClass\".equals(mode)) {\n\t\t\t\tscanner.setScopedProxyMode(ScopedProxyMode.TARGET_CLASS);\n\t\t\t}\n\t\t\telse if (\"interfaces\".equals(mode)) {\n\t\t\t\tscanner.setScopedProxyMode(ScopedProxyMode.INTERFACES);\n\t\t\t}\n\t\t\telse if (\"no\".equals(mode)) {\n\t\t\t\tscanner.setScopedProxyMode(ScopedProxyMode.NO);\n\t\t\t}\n\t\t\telse {\n\t\t\t\tthrow new IllegalArgumentException(\"scoped-proxy only supports 'no', 'interfaces' and 'targetClass'\");\n\t\t\t}\n\t\t}\n```\n\n\n######Conclusion\nThere's a chance, that you won't need this article, because the common way now to configure your project is java config. However, next time when you will have a chance to configure some spring xml application - try it for yourself, do your own investigation.\n\n\n######Links\nhttp://stackoverflow.com/questions/11174286/spring-xml-namespaces-how-do-i-find-what-are-the-implementing-classes-behind-t","mobiledoc":null,"html":"<p>For those of you who worked with spring application without spring boot, and remembers those times, when you had to make your configuration using xml - did you ever question yourself how the magic is done ? For instance, if you had to create a component scan, you had to do <br />\n<code>&lt;context:component-scan base-package=\"com.yourpackage.blablabla\" /&gt;</code> how it really did this component scan ?</p>\n\n<h6 id=\"springhandlers\">spring.handlers</h6>\n\n<p>Answer is simple - if you really would like to know how <strong>context</strong> or any other spring namespace is working - always start from <strong>spring.handlers</strong> file. It's present in each spring jar file, which have some namespaces. The more spring dependencies you add - the more spring.handlers files you will have </p>\n\n<p><img src=\"/content/images/2016/06/Screen-Shot-2016-06-15-at-8-33-29-AM.png\" alt=\"alt\" /></p>\n\n<p>In case of spring-context jar, you will have something like this</p>\n\n<pre><code>http\\://www.springframework.org/schema/context=org.springframework.context.config.ContextNamespaceHandler  \nhttp\\://www.springframework.org/schema/jee=org.springframework.ejb.config.JeeNamespaceHandler  \nhttp\\://www.springframework.org/schema/lang=org.springframework.scripting.config.LangNamespaceHandler  \nhttp\\://www.springframework.org/schema/task=org.springframework.scheduling.config.TaskNamespaceHandler  \nhttp\\://www.springframework.org/schema/cache=org.springframework.cache.config.CacheNamespaceHandler  \n</code></pre>\n\n<p>So, let's choose <strong>component-scan</strong> attribute.</p>\n\n<h6 id=\"componentscan\">component-scan</h6>\n\n<p>component-scan is an attribute of context namespace, so, we need <strong>ContextNamespaceHandler</strong></p>\n\n<pre><code>/*\n * Copyright 2002-2012 the original author or authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.springframework.context.config;\n\nimport org.springframework.beans.factory.xml.NamespaceHandlerSupport;  \nimport org.springframework.context.annotation.AnnotationConfigBeanDefinitionParser;  \nimport org.springframework.context.annotation.ComponentScanBeanDefinitionParser;\n\n/**\n * {@link org.springframework.beans.factory.xml.NamespaceHandler}\n * for the '{@code context}' namespace.\n *\n * @author Mark Fisher\n * @author Juergen Hoeller\n * @since 2.5\n */\npublic class ContextNamespaceHandler extends NamespaceHandlerSupport {\n\n    @Override\n    public void init() {\n        registerBeanDefinitionParser(\"property-placeholder\", new PropertyPlaceholderBeanDefinitionParser());\n        registerBeanDefinitionParser(\"property-override\", new PropertyOverrideBeanDefinitionParser());\n        registerBeanDefinitionParser(\"annotation-config\", new AnnotationConfigBeanDefinitionParser());\n        registerBeanDefinitionParser(\"component-scan\", new ComponentScanBeanDefinitionParser());\n        registerBeanDefinitionParser(\"load-time-weaver\", new LoadTimeWeaverBeanDefinitionParser());\n        registerBeanDefinitionParser(\"spring-configured\", new SpringConfiguredBeanDefinitionParser());\n        registerBeanDefinitionParser(\"mbean-export\", new MBeanExportBeanDefinitionParser());\n        registerBeanDefinitionParser(\"mbean-server\", new MBeanServerBeanDefinitionParser());\n    }\n\n}\n</code></pre>\n\n<p>Still nothing magical, just something about <strong><a href=\"http://docs.spring.io/spring/docs/current/javadoc-api/org/springframework/beans/factory/xml/BeanDefinitionParser.html\">BeanDefinitionParser</a></strong></p>\n\n<p>We need this line</p>\n\n<pre><code>registerBeanDefinitionParser(\"component-scan\", new ComponentScanBeanDefinitionParser());  \n</code></pre>\n\n<p>The key thing is <a href=\"http://docs.spring.io/spring/docs/current/javadoc-api/org/springframework/context/annotation/ComponentScanBeanDefinitionParser.html\">ComponentScanBeanDefinitionParser</a> and his <strong>parse</strong> method</p>\n\n<pre><code>    @Override\n    public BeanDefinition parse(Element element, ParserContext parserContext) {\n        String[] basePackages = StringUtils.tokenizeToStringArray(element.getAttribute(BASE_PACKAGE_ATTRIBUTE),\n                ConfigurableApplicationContext.CONFIG_LOCATION_DELIMITERS);\n\n        // Actually scan for bean definitions and register them.\n        ClassPathBeanDefinitionScanner scanner = configureScanner(parserContext, element);\n        Set&lt;BeanDefinitionHolder&gt; beanDefinitions = scanner.doScan(basePackages);\n        registerComponents(parserContext.getReaderContext(), beanDefinitions, element);\n\n        return null;\n    }\n</code></pre>\n\n<p>You can, of course, dig deeper, and find many spring rules and constraints, for example one about scope-proxy attribute</p>\n\n<pre><code>        if (element.hasAttribute(SCOPED_PROXY_ATTRIBUTE)) {\n            String mode = element.getAttribute(SCOPED_PROXY_ATTRIBUTE);\n            if (\"targetClass\".equals(mode)) {\n                scanner.setScopedProxyMode(ScopedProxyMode.TARGET_CLASS);\n            }\n            else if (\"interfaces\".equals(mode)) {\n                scanner.setScopedProxyMode(ScopedProxyMode.INTERFACES);\n            }\n            else if (\"no\".equals(mode)) {\n                scanner.setScopedProxyMode(ScopedProxyMode.NO);\n            }\n            else {\n                throw new IllegalArgumentException(\"scoped-proxy only supports 'no', 'interfaces' and 'targetClass'\");\n            }\n        }\n</code></pre>\n\n<h6 id=\"conclusion\">Conclusion</h6>\n\n<p>There's a chance, that you won't need this article, because the common way now to configure your project is java config. However, next time when you will have a chance to configure some spring xml application - try it for yourself, do your own investigation.</p>\n\n<h6 id=\"links\">Links</h6>\n\n<p><a href=\"http://stackoverflow.com/questions/11174286/spring-xml-namespaces-how-do-i-find-what-are-the-implementing-classes-behind-t\">http://stackoverflow.com/questions/11174286/spring-xml-namespaces-how-do-i-find-what-are-the-implementing-classes-behind-t</a></p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-06-07T08:54:39.000Z","created_by":1,"updated_at":"2016-06-18T14:36:55.000Z","updated_by":1,"published_at":"2016-06-15T05:16:22.000Z","published_by":1},{"id":41,"uuid":"530d78a6-1e58-4de1-a3f8-30f58de3fd9a","title":"Spring DATA  Mongo. Testing your  mongo using in-memory db","slug":"spring-data-mongo-testing-using-in-memory-db","markdown":"######Embeddable\nWhen I was a bit younger, I couldn't understand why more senior engineers stood for embedding builds as much as it's possible. For instance, you have a database, and you're running your integration tests only with in memory databases. It was very unusual for me, why having a working database on your local machine, you're using some weird in memory things ? The time have passed, and I understand now, that embedding your builds is a good practice, because:\n\n* It reduce your build time\n* It decouples your build phase from any environments\n* Even if you don't have any database/thirdparty tool installed on your local machine, your build will finish successfully, and after then you can start installing all required third party instruments.\n\n###### Fongo + NoSQl-Unit\n\nBy this article I'd like to show how to effectively test your Spring DATA repositories using [Fongo](https://github.com/foursquare/fongo) - an in-memory implementation.\n\nI'm not going to explain how Spring Data works, you can read their documentation [here](http://docs.spring.io/spring-data/commons/docs/current/reference/html/)\n\nSay, you have following repository\n\n```\npackage org.example.repository;\nimport org.springframework.data.mongodb.repository.MongoRepository;\nimport org.springframework.data.mongodb.repository.Query;\nimport org.startup.queue.domain.Establishment;\n\nimport java.util.Optional;\n\npublic interface SomethingRepository extends MongoRepository<Establishment, String> {\n\n    Optional<Something> findByCol1(String col1);\n\n}\n```\n\nNext, test it:\n\n```\npackage org.example.repository;\n\nimport com.github.fakemongo.Fongo;\nimport com.lordofthejars.nosqlunit.annotation.UsingDataSet;\nimport com.lordofthejars.nosqlunit.core.LoadStrategyEnum;\nimport com.lordofthejars.nosqlunit.mongodb.MongoDbRule;\nimport com.mongodb.Mongo;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.context.ApplicationContext;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.ComponentScan;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.context.annotation.PropertySource;\nimport org.springframework.data.mongodb.config.AbstractMongoConfiguration;\nimport org.springframework.data.mongodb.repository.config.EnableMongoRepositories;\nimport org.springframework.test.context.ContextConfiguration;\nimport org.springframework.test.context.junit4.SpringJUnit4ClassRunner;\nimport org.startup.queue.domain.Establishment;\nimport org.startup.queue.domain.Table;\n\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Optional;\n\nimport static com.lordofthejars.nosqlunit.mongodb.MongoDbRule.MongoDbRuleBuilder.newMongoDbRule;\nimport static org.junit.Assert.assertEquals;\n\n@RunWith(SpringJUnit4ClassRunner.class)\n@ContextConfiguration\npublic class SomethingRepositoryTest {\n\n    // Don't forget to add this field\n    @Autowired\n    private ApplicationContext applicationContext;\n\n    @Rule\n    public MongoDbRule mongoDbRule = newMongoDbRule().defaultSpringMongoDb(\"demo-test\");\n\n    @Autowired\n    private SomethingRepository unit;\n\n    @Test\n    @UsingDataSet(locations = \"somethings.json\", loadStrategy = LoadStrategyEnum.CLEAN_INSERT)\n    public void testFindByTablesQr() throws Exception {\n        // Given\n        Something expected = new Something();\n        ... data from somethings.json\n\n        // When\n        Optional<Something> actual = unit.findByCol1(col1);\n\n        // Then\n        assertEquals(expected, actual.get());\n    }\n\n\n    @Configuration\n    @EnableMongoRepositories\n    @ComponentScan(basePackageClasses = {SomethingRepository.class})\n    static class SomethingRepositoryConfiguration extends AbstractMongoConfiguration {\n\n\n        @Override\n        protected String getDatabaseName() {\n            return \"demo-test\";\n        }\n\n        @Bean\n        public Mongo mongo() {\n            Fongo queued = new Fongo(\"something\");\n            return queued.getMongo();\n        }\n\n        @Override\n        protected String getMappingBasePackage() {\n            return \"org.startup.queue.repository\";\n        }\n\n    }\n}\n```\n\nLet's explain:\n\n```\n    @Autowired\n    private ApplicationContext applicationContext;\n```\nBy this line you are forcing Spring Context to fully load under this class. If you'll skip this line - `mongoDbRule` won't work.\n\n```\n    @Rule\n    public MongoDbRule mongoDbRule = newMongoDbRule().defaultSpringMongoDb(\"demo-test\");\n```\n\nBy adding this line you're making sure, that you can use NoSQL-Unit. I mean, adding this line will allow you to use\n`@UsingDataSet` annotation. Of course, you can use **Fongo** without NoSQL-Unit. In this case you will need to manually add records into your nosql storage. Refer to github of following project to understand all possibilities of this project.\n\n\n```\n    @UsingDataSet(locations = \"somethings.json\", loadStrategy = LoadStrategyEnum.CLEAN_INSERT)\n```\n\nBy line above you are loading your collection data into your in memory database.\nPS - your something.json should be under `src/test/resources/org/example/repository/something.json` folder.\n\n\n```\n        @Bean\n        public Mongo mongo() {\n            Fongo queued = new Fongo(\"something\");\n            return queued.getMongo();\n        }\n```\nHow your in memory database is being created ? You need to override spring bean, which stands for creating Mongo object.\n\nThat's all what you need to do to test your mongo repository. I've noticed, that testing Spring Data using in-container mode is the only reasonable way to test them.\n\n###### Links\n* https://github.com/lordofthejars/nosql-unit\n* https://github.com/foursquare/fongo\n","mobiledoc":null,"html":"<h6 id=\"embeddable\">Embeddable</h6>\n\n<p>When I was a bit younger, I couldn't understand why more senior engineers stood for embedding builds as much as it's possible. For instance, you have a database, and you're running your integration tests only with in memory databases. It was very unusual for me, why having a working database on your local machine, you're using some weird in memory things ? The time have passed, and I understand now, that embedding your builds is a good practice, because:</p>\n\n<ul>\n<li>It reduce your build time</li>\n<li>It decouples your build phase from any environments</li>\n<li>Even if you don't have any database/thirdparty tool installed on your local machine, your build will finish successfully, and after then you can start installing all required third party instruments.</li>\n</ul>\n\n<h6 id=\"fongonosqlunit\">Fongo + NoSQl-Unit</h6>\n\n<p>By this article I'd like to show how to effectively test your Spring DATA repositories using <a href=\"https://github.com/foursquare/fongo\">Fongo</a> - an in-memory implementation.</p>\n\n<p>I'm not going to explain how Spring Data works, you can read their documentation <a href=\"http://docs.spring.io/spring-data/commons/docs/current/reference/html/\">here</a></p>\n\n<p>Say, you have following repository</p>\n\n<pre><code>package org.example.repository;  \nimport org.springframework.data.mongodb.repository.MongoRepository;  \nimport org.springframework.data.mongodb.repository.Query;  \nimport org.startup.queue.domain.Establishment;\n\nimport java.util.Optional;\n\npublic interface SomethingRepository extends MongoRepository&lt;Establishment, String&gt; {\n\n    Optional&lt;Something&gt; findByCol1(String col1);\n\n}\n</code></pre>\n\n<p>Next, test it:</p>\n\n<pre><code>package org.example.repository;\n\nimport com.github.fakemongo.Fongo;  \nimport com.lordofthejars.nosqlunit.annotation.UsingDataSet;  \nimport com.lordofthejars.nosqlunit.core.LoadStrategyEnum;  \nimport com.lordofthejars.nosqlunit.mongodb.MongoDbRule;  \nimport com.mongodb.Mongo;  \nimport org.junit.Rule;  \nimport org.junit.Test;  \nimport org.junit.runner.RunWith;  \nimport org.springframework.beans.factory.annotation.Autowired;  \nimport org.springframework.context.ApplicationContext;  \nimport org.springframework.context.annotation.Bean;  \nimport org.springframework.context.annotation.ComponentScan;  \nimport org.springframework.context.annotation.Configuration;  \nimport org.springframework.context.annotation.PropertySource;  \nimport org.springframework.data.mongodb.config.AbstractMongoConfiguration;  \nimport org.springframework.data.mongodb.repository.config.EnableMongoRepositories;  \nimport org.springframework.test.context.ContextConfiguration;  \nimport org.springframework.test.context.junit4.SpringJUnit4ClassRunner;  \nimport org.startup.queue.domain.Establishment;  \nimport org.startup.queue.domain.Table;\n\nimport java.util.Collections;  \nimport java.util.List;  \nimport java.util.Optional;\n\nimport static com.lordofthejars.nosqlunit.mongodb.MongoDbRule.MongoDbRuleBuilder.newMongoDbRule;  \nimport static org.junit.Assert.assertEquals;\n\n@RunWith(SpringJUnit4ClassRunner.class)\n@ContextConfiguration\npublic class SomethingRepositoryTest {\n\n    // Don't forget to add this field\n    @Autowired\n    private ApplicationContext applicationContext;\n\n    @Rule\n    public MongoDbRule mongoDbRule = newMongoDbRule().defaultSpringMongoDb(\"demo-test\");\n\n    @Autowired\n    private SomethingRepository unit;\n\n    @Test\n    @UsingDataSet(locations = \"somethings.json\", loadStrategy = LoadStrategyEnum.CLEAN_INSERT)\n    public void testFindByTablesQr() throws Exception {\n        // Given\n        Something expected = new Something();\n        ... data from somethings.json\n\n        // When\n        Optional&lt;Something&gt; actual = unit.findByCol1(col1);\n\n        // Then\n        assertEquals(expected, actual.get());\n    }\n\n\n    @Configuration\n    @EnableMongoRepositories\n    @ComponentScan(basePackageClasses = {SomethingRepository.class})\n    static class SomethingRepositoryConfiguration extends AbstractMongoConfiguration {\n\n\n        @Override\n        protected String getDatabaseName() {\n            return \"demo-test\";\n        }\n\n        @Bean\n        public Mongo mongo() {\n            Fongo queued = new Fongo(\"something\");\n            return queued.getMongo();\n        }\n\n        @Override\n        protected String getMappingBasePackage() {\n            return \"org.startup.queue.repository\";\n        }\n\n    }\n}\n</code></pre>\n\n<p>Let's explain:</p>\n\n<pre><code>    @Autowired\n    private ApplicationContext applicationContext;\n</code></pre>\n\n<p>By this line you are forcing Spring Context to fully load under this class. If you'll skip this line - <code>mongoDbRule</code> won't work.</p>\n\n<pre><code>    @Rule\n    public MongoDbRule mongoDbRule = newMongoDbRule().defaultSpringMongoDb(\"demo-test\");\n</code></pre>\n\n<p>By adding this line you're making sure, that you can use NoSQL-Unit. I mean, adding this line will allow you to use <br />\n<code>@UsingDataSet</code> annotation. Of course, you can use <strong>Fongo</strong> without NoSQL-Unit. In this case you will need to manually add records into your nosql storage. Refer to github of following project to understand all possibilities of this project.</p>\n\n<pre><code>    @UsingDataSet(locations = \"somethings.json\", loadStrategy = LoadStrategyEnum.CLEAN_INSERT)\n</code></pre>\n\n<p>By line above you are loading your collection data into your in memory database. <br />\nPS - your something.json should be under <code>src/test/resources/org/example/repository/something.json</code> folder.</p>\n\n<pre><code>        @Bean\n        public Mongo mongo() {\n            Fongo queued = new Fongo(\"something\");\n            return queued.getMongo();\n        }\n</code></pre>\n\n<p>How your in memory database is being created ? You need to override spring bean, which stands for creating Mongo object.</p>\n\n<p>That's all what you need to do to test your mongo repository. I've noticed, that testing Spring Data using in-container mode is the only reasonable way to test them.</p>\n\n<h6 id=\"links\">Links</h6>\n\n<ul>\n<li><a href=\"https://github.com/lordofthejars/nosql-unit\">https://github.com/lordofthejars/nosql-unit</a></li>\n<li><a href=\"https://github.com/foursquare/fongo\">https://github.com/foursquare/fongo</a></li>\n</ul>","image":"https://webassets.mongodb.com/_com_assets/cms/MongoDB-Logo-5c3a7405a85675366beb3a5ec4c032348c390b3f142f5e6dddf1d78e2df5cb5c.png","featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-06-10T14:14:20.000Z","created_by":1,"updated_at":"2016-06-19T08:55:15.000Z","updated_by":1,"published_at":"2016-06-10T14:46:02.000Z","published_by":1},{"id":42,"uuid":"316bc92a-ce90-4d8d-a91b-fa17ca3f7afa","title":"Combining Spring Integration Testing with Mockito","slug":"combining-spring-integration-tests-with-mockito","markdown":"Integration tests...they are perfect for testing your data flows. You send some request to your application and can control how data is being processed throughout your application. You see how request is received by your controller, then it's sent to service, dao or other layers, that you have in your application. Sometimes you don't want some layer to do real work in Spring. For instance, your dao layer is using some native queries to get data from database, and some embedded database doesn't support some query syntax. Naturally, you still want to have your integration tests, but without a real call to database. What can we do in this situation ? I'd suggest to mock this dao layer, using mockito. Let's demonstrate how it works ?\n\n######Project setup\nI use [Spring Initialzr](https://start.spring.io) to setup projects, so let's create a simple Spring Boot application. Code can be found [here](https://github.com/ivanursul/spring-integration-mockito).\n\n######Project structure\n```\n README.md\n build.gradle\n gradle\n  wrapper\n      gradle-wrapper.jar\n      gradle-wrapper.properties\n gradlew\n gradlew.bat\n spring-integration-mockito.iml\n src\n     main\n      java\n       org\n           springmockito\n               demo\n                   DemoApplication.java\n                   ExampleDao.java\n                   ExampleEntity.java\n                   ExampleService.java\n                   TestConfiguration.java\n      resources\n          application.properties\n     test\n         java\n             org\n                 springmockito\n                     demo\n                         ExampleServiceTest.java\n```\n\nLet's try to explain step by step how it should work\n\n######Roles\n\n* **ExampleDao** - class, which will be mocked. Let's say, ExampleDao is doing very hard database operation, and we don't want to waste time on it. Instead of waiting, you can mock it.\n\n* **ExampleService** - class, which will have **ExampleDao** as a field. Nothing special, just some delegation to Dao layer.\n\n* **ExampleServiceTest** - place, where we will use Spring with Mockito.\n\n```\n\n@ActiveProfiles(\"test\")\n@RunWith(SpringJUnit4ClassRunner.class)\n@SpringApplicationConfiguration(classes = DemoApplication.class)\npublic class ExampleServiceTest {\n\n    @Autowired\n    private ExampleDao exampleDao;\n\n    @Autowired\n    private ExampleService unit;\n\n    @Test\n    public void testGetEntity() throws Exception {\n        // Given\n        Long id = 1L;\n        ExampleEntity expected = new ExampleEntity(1L, \"Mocked name\");\n\n        // When\n        doReturn(expected)\n                .when(exampleDao).longRunnintGetById(id);\n\n        ExampleEntity actual = unit.getEntity(id);\n\n        // Then\n        assertEquals(expected, actual);\n    }\n}\n```\n\nIf I were you, I'd have two questions: why exampleDao is missing annotation `@Mock` and what is `@ActiveProfiles(\"test\")`Because we are createa mock using Spring dependency injection, we don't need to annotate our mock field with any @Mock annotations.\n\n* **TestConfiguration** - test configuration, which has one method of ExampleDao returning type. Notice, how it's returned.\n\n```\n    @Bean\n    public ExampleDao exampleDao() {\n        return mock(ExampleDao.class);\n    }\n```\n\n**TestConfiguration** also has `@Profile(\"test\")` annotation, which is an indicator, that this configuration should run only on test profile. `@Profile`, together with `@ActiveProfiles` are loading **TestConfiguration** inside our test. That's why ExampleDao is annotated as a spring bean, because we need to get it from context.","mobiledoc":null,"html":"<p>Integration tests...they are perfect for testing your data flows. You send some request to your application and can control how data is being processed throughout your application. You see how request is received by your controller, then it's sent to service, dao or other layers, that you have in your application. Sometimes you don't want some layer to do real work in Spring. For instance, your dao layer is using some native queries to get data from database, and some embedded database doesn't support some query syntax. Naturally, you still want to have your integration tests, but without a real call to database. What can we do in this situation ? I'd suggest to mock this dao layer, using mockito. Let's demonstrate how it works ?</p>\n\n<h6 id=\"projectsetup\">Project setup</h6>\n\n<p>I use <a href=\"https://start.spring.io\">Spring Initialzr</a> to setup projects, so let's create a simple Spring Boot application. Code can be found <a href=\"https://github.com/ivanursul/spring-integration-mockito\">here</a>.</p>\n\n<h6 id=\"projectstructure\">Project structure</h6>\n\n<pre><code> README.md\n build.gradle\n gradle\n  wrapper\n      gradle-wrapper.jar\n      gradle-wrapper.properties\n gradlew\n gradlew.bat\n spring-integration-mockito.iml\n src\n     main\n      java\n       org\n           springmockito\n               demo\n                   DemoApplication.java\n                   ExampleDao.java\n                   ExampleEntity.java\n                   ExampleService.java\n                   TestConfiguration.java\n      resources\n          application.properties\n     test\n         java\n             org\n                 springmockito\n                     demo\n                         ExampleServiceTest.java\n</code></pre>\n\n<p>Let's try to explain step by step how it should work</p>\n\n<h6 id=\"roles\">Roles</h6>\n\n<ul>\n<li><p><strong>ExampleDao</strong> - class, which will be mocked. Let's say, ExampleDao is doing very hard database operation, and we don't want to waste time on it. Instead of waiting, you can mock it.</p></li>\n<li><p><strong>ExampleService</strong> - class, which will have <strong>ExampleDao</strong> as a field. Nothing special, just some delegation to Dao layer.</p></li>\n<li><p><strong>ExampleServiceTest</strong> - place, where we will use Spring with Mockito.</p></li>\n</ul>\n\n<pre><code>@ActiveProfiles(\"test\")\n@RunWith(SpringJUnit4ClassRunner.class)\n@SpringApplicationConfiguration(classes = DemoApplication.class)\npublic class ExampleServiceTest {\n\n    @Autowired\n    private ExampleDao exampleDao;\n\n    @Autowired\n    private ExampleService unit;\n\n    @Test\n    public void testGetEntity() throws Exception {\n        // Given\n        Long id = 1L;\n        ExampleEntity expected = new ExampleEntity(1L, \"Mocked name\");\n\n        // When\n        doReturn(expected)\n                .when(exampleDao).longRunnintGetById(id);\n\n        ExampleEntity actual = unit.getEntity(id);\n\n        // Then\n        assertEquals(expected, actual);\n    }\n}\n</code></pre>\n\n<p>If I were you, I'd have two questions: why exampleDao is missing annotation <code>@Mock</code> and what is <code>@ActiveProfiles(\"test\")</code>Because we are createa mock using Spring dependency injection, we don't need to annotate our mock field with any @Mock annotations.</p>\n\n<ul>\n<li><strong>TestConfiguration</strong> - test configuration, which has one method of ExampleDao returning type. Notice, how it's returned.</li>\n</ul>\n\n<pre><code>    @Bean\n    public ExampleDao exampleDao() {\n        return mock(ExampleDao.class);\n    }\n</code></pre>\n\n<p><strong>TestConfiguration</strong> also has <code>@Profile(\"test\")</code> annotation, which is an indicator, that this configuration should run only on test profile. <code>@Profile</code>, together with <code>@ActiveProfiles</code> are loading <strong>TestConfiguration</strong> inside our test. That's why ExampleDao is annotated as a spring bean, because we need to get it from context.</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":"","meta_description":null,"author_id":1,"created_at":"2016-06-18T12:32:20.000Z","created_by":1,"updated_at":"2016-06-18T16:12:43.000Z","updated_by":1,"published_at":"2016-06-18T13:51:58.000Z","published_by":1},{"id":43,"uuid":"e1e9f4ca-75cf-40e9-8abd-aad87d970757","title":"Deploying your application to cloud using docker-machine","slug":"deploying-your-application-to-cloud-using-docker-machine","markdown":"#### Problem\nAs a part of my investigation of what docker is, I want to do a simple and useful thing - deploy my application in a completely convenient manner. Let's say, I'm using [Digital Ocean](https://www.digitalocean.com/) as a cloud provider. Because my application is too little to think about complex deployment infrastructure, I'd like to be able to deploy everything from my laptop using digitalocean token.\n\n#### Solution\nI'll try to deploy everything using docker-machine, together with digitalocean cloud provider. I'll describe in step by step how to do this.\n\n\n##### Setup a new digitalocean token.\n\nGo to [cloud.digitalocean.com/settings/api/tokens](https://cloud.digitalocean.com/settings/api/tokens) and generate a new token\n\n![](/content/images/2016/06/Screen-Shot-2016-06-25-at-3-36-49-PM.png)\n\nThen, got your token, and execute following commands to export your instructions for further commands. You will understand why do we need them later.\n\n```\nexport DIGITALOCEAN_ACCESS_TOKEN=${newly-generated-token}\nexport DIGITALOCEAN_PRIVATE_NETWORKING=true\nexport DIGITALOCEAN_IMAGE=debian-8-x64\n```\n\n##### Create new machine\n\nAs simple, as it can be - I'll create a new digitalocean instance. Open your terminal, and type following command.\n\n```\ndocker-machine create \\\n  -d digitalocean \\\n  my-application\n```\nFew explanations, `-d digitalocean` means, that you will use digitalocean for deployment. Out of the box, digitalocean will use exports, that we set some minutes ago.\nWhile we're waiting till our console end up deploying to docker, let's open our digitalocean droplets page, and see how it's beatiful, we are using console, and ignore any ui cloud tools.\n\n![](/content/images/2016/06/Screen-Shot-2016-06-25-at-3-40-18-PM.png)\n\n##### Connecting to docker\nSo, our console shows following\n```\n  queued-rest-api git:(master)  docker-machine create \\\n>   -d digitalocean \\\n>   my-application\nRunning pre-create checks...\nCreating machine...\n(my-application) Creating SSH key...\n(my-application) Creating Digital Ocean droplet...\n(my-application) Waiting for IP address to be assigned to the Droplet...\nWaiting for machine to be running, this may take a few minutes...\nDetecting operating system of created instance...\nWaiting for SSH to be available...\nDetecting the provisioner...\nProvisioning with debian...\nCopying certs to the local machine directory...\nCopying certs to the remote machine...\nSetting Docker configuration on the remote daemon...\nChecking connection to Docker...\nDocker is up and running!\nTo see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: docker-machine env my-application\n```\n\nLast line is the line that you should pay attention with - it. \n\n```\nTo see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: docker-machine env my-application\n```\n\nLet's run `docker-machine env my-application`\n\n```\nexport DOCKER_TLS_VERIFY=\"1\"\nexport DOCKER_HOST=\"tcp://104.131.4.132:2376\"\nexport DOCKER_CERT_PATH=\"/Users/ivanursul/.docker/machine/machines/my-application\"\nexport DOCKER_MACHINE_NAME=\"my-application\"\n# Run this command to configure your shell: \n# eval $(docker-machine env my-application)\n```\n\nSo, by running `eval $(docker-machine env my-application)` we can connect to our docker instance, right ? Let's do it. There's no input after running this command, so, don't worry. Think about it, as about switching contexts. You switch your docker context to another machine, for this case, in the cloud. After command above, you are working with docker, which is located in the cloud, and can potentially, run your web application.\n\n##### What's next ?\nSo , we have connected to docker straightly and can start executing any commands. If we have some folder with our application, we need to write instructions about how to deploy it. It's called Dockerfile.\n\n##### Running our app inside docker instance\nI'm not going to write anything about [Dockerfile](https://docs.docker.com/engine/reference/builder/), feel free to investigate it by yourself. Let's suppose, you have Dockerfile. So, go to your terminal, cd to folder, where **Dockerfile** is present and execute following command:\n\n```\ndocker build -t \"my-application\" .\n```\n\nIt should result in similar output\n\n```\nSending build context to Docker daemon 29.58 MB\nStep 1 : FROM java:8\n8: Pulling from library/java\n\n5c90d4a2d1a8: Pull complete \nab30c63719b1: Pull complete \nc6072700a242: Pull complete \n5f444d070427: Pull complete \n620b5227cf38: Pull complete \n3cfd33220efa: Pull complete \n864a98a84dd2: Pull complete \n734cc28150de: Pull complete \nDigest: sha256:2b840b021b8753dd18da3491d362999980e6636b4a3064ff57bf17ea6dbce42f\nStatus: Downloaded newer image for java:8\n ---> 264282a59a95\nStep 2 : ENV APP_DIR /srv\n ---> Running in f303ecc96d60\n ---> 6ba44f5d96db\nRemoving intermediate container f303ecc96d60\nStep 3 : WORKDIR $APP_DIR\n ---> Running in 48988746fa2b\n ---> 72154b0c43b2\nRemoving intermediate container 48988746fa2b\nStep 4 : ADD ./build/libs/queued-0.1.jar $APP_DIR/queued.jar\n ---> c88d3ddabf0e\nRemoving intermediate container 1f7d9997ed39\nStep 5 : CMD java -Djava.security.egd=file:/dev/./urandom -jar queued.jar\n ---> Running in d9d377cd1bc3\n ---> dedd6291688f\nRemoving intermediate container d9d377cd1bc3\nSuccessfully built dedd6291688f\n```\n\nSo, we build a image with tag 'my-application', let's run it ?\n\n##### Running docker image on server\nJust execute this command\n\n```\ndocker run -d \\\n  -h my-application \\\n  -p 8080:8080 \\\n  --restart always \\\n  my-application\n```\n\n##### Testing my-application\n\nFirst of all, let's ignore any ui, and try to get as much information from console, as we can. We need to get ip address of the machine, so, this command will do the work\n\n```\ndocker-machine ip my-application\n```\n\n```\n  queued-rest-api git:(master)  docker-machine ip my-application\n104.131.4.132\n```\n\nBelieve me, or not, but then I enter postman app, sent request, and received message.\n\n![](/content/images/2016/06/Screen-Shot-2016-06-25-at-4-04-54-PM.png)\n\nOf course, it's not a proof, so, I encourage you to try this steps for yourself.\n\n##### Don't forget to remove your docker instance\n\nAs you remember, you are playing with cloud, and pay money for machines, so don't forget to remove your machine\n\n```\ndocker-machine rm my-application\n```\n\nMake sure you don't see any digitalocean droplets with name 'my-application'.\n\n\n\n\n","mobiledoc":null,"html":"<h4 id=\"problem\">Problem</h4>\n\n<p>As a part of my investigation of what docker is, I want to do a simple and useful thing - deploy my application in a completely convenient manner. Let's say, I'm using <a href=\"https://www.digitalocean.com/\">Digital Ocean</a> as a cloud provider. Because my application is too little to think about complex deployment infrastructure, I'd like to be able to deploy everything from my laptop using digitalocean token.</p>\n\n<h4 id=\"solution\">Solution</h4>\n\n<p>I'll try to deploy everything using docker-machine, together with digitalocean cloud provider. I'll describe in step by step how to do this.</p>\n\n<h5 id=\"setupanewdigitaloceantoken\">Setup a new digitalocean token.</h5>\n\n<p>Go to <a href=\"https://cloud.digitalocean.com/settings/api/tokens\">cloud.digitalocean.com/settings/api/tokens</a> and generate a new token</p>\n\n<p><img src=\"/content/images/2016/06/Screen-Shot-2016-06-25-at-3-36-49-PM.png\" alt=\"\" /></p>\n\n<p>Then, got your token, and execute following commands to export your instructions for further commands. You will understand why do we need them later.</p>\n\n<pre><code>export DIGITALOCEAN_ACCESS_TOKEN=${newly-generated-token}  \nexport DIGITALOCEAN_PRIVATE_NETWORKING=true  \nexport DIGITALOCEAN_IMAGE=debian-8-x64  \n</code></pre>\n\n<h5 id=\"createnewmachine\">Create new machine</h5>\n\n<p>As simple, as it can be - I'll create a new digitalocean instance. Open your terminal, and type following command.</p>\n\n<pre><code>docker-machine create \\  \n  -d digitalocean \\\n  my-application\n</code></pre>\n\n<p>Few explanations, <code>-d digitalocean</code> means, that you will use digitalocean for deployment. Out of the box, digitalocean will use exports, that we set some minutes ago. <br />\nWhile we're waiting till our console end up deploying to docker, let's open our digitalocean droplets page, and see how it's beatiful, we are using console, and ignore any ui cloud tools.</p>\n\n<p><img src=\"/content/images/2016/06/Screen-Shot-2016-06-25-at-3-40-18-PM.png\" alt=\"\" /></p>\n\n<h5 id=\"connectingtodocker\">Connecting to docker</h5>\n\n<p>So, our console shows following  </p>\n\n<pre><code>  queued-rest-api git:(master)  docker-machine create \\\n&gt;   -d digitalocean \\\n&gt;   my-application\nRunning pre-create checks...  \nCreating machine...  \n(my-application) Creating SSH key...\n(my-application) Creating Digital Ocean droplet...\n(my-application) Waiting for IP address to be assigned to the Droplet...\nWaiting for machine to be running, this may take a few minutes...  \nDetecting operating system of created instance...  \nWaiting for SSH to be available...  \nDetecting the provisioner...  \nProvisioning with debian...  \nCopying certs to the local machine directory...  \nCopying certs to the remote machine...  \nSetting Docker configuration on the remote daemon...  \nChecking connection to Docker...  \nDocker is up and running!  \nTo see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: docker-machine env my-application  \n</code></pre>\n\n<p>Last line is the line that you should pay attention with - it. </p>\n\n<pre><code>To see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: docker-machine env my-application  \n</code></pre>\n\n<p>Let's run <code>docker-machine env my-application</code></p>\n\n<pre><code>export DOCKER_TLS_VERIFY=\"1\"  \nexport DOCKER_HOST=\"tcp://104.131.4.132:2376\"  \nexport DOCKER_CERT_PATH=\"/Users/ivanursul/.docker/machine/machines/my-application\"  \nexport DOCKER_MACHINE_NAME=\"my-application\"  \n# Run this command to configure your shell: \n# eval $(docker-machine env my-application)\n</code></pre>\n\n<p>So, by running <code>eval $(docker-machine env my-application)</code> we can connect to our docker instance, right ? Let's do it. There's no input after running this command, so, don't worry. Think about it, as about switching contexts. You switch your docker context to another machine, for this case, in the cloud. After command above, you are working with docker, which is located in the cloud, and can potentially, run your web application.</p>\n\n<h5 id=\"whatsnext\">What's next ?</h5>\n\n<p>So , we have connected to docker straightly and can start executing any commands. If we have some folder with our application, we need to write instructions about how to deploy it. It's called Dockerfile.</p>\n\n<h5 id=\"runningourappinsidedockerinstance\">Running our app inside docker instance</h5>\n\n<p>I'm not going to write anything about <a href=\"https://docs.docker.com/engine/reference/builder/\">Dockerfile</a>, feel free to investigate it by yourself. Let's suppose, you have Dockerfile. So, go to your terminal, cd to folder, where <strong>Dockerfile</strong> is present and execute following command:</p>\n\n<pre><code>docker build -t \"my-application\" .  \n</code></pre>\n\n<p>It should result in similar output</p>\n\n<pre><code>Sending build context to Docker daemon 29.58 MB  \nStep 1 : FROM java:8  \n8: Pulling from library/java\n\n5c90d4a2d1a8: Pull complete  \nab30c63719b1: Pull complete  \nc6072700a242: Pull complete  \n5f444d070427: Pull complete  \n620b5227cf38: Pull complete  \n3cfd33220efa: Pull complete  \n864a98a84dd2: Pull complete  \n734cc28150de: Pull complete  \nDigest: sha256:2b840b021b8753dd18da3491d362999980e6636b4a3064ff57bf17ea6dbce42f  \nStatus: Downloaded newer image for java:8  \n ---&gt; 264282a59a95\nStep 2 : ENV APP_DIR /srv  \n ---&gt; Running in f303ecc96d60\n ---&gt; 6ba44f5d96db\nRemoving intermediate container f303ecc96d60  \nStep 3 : WORKDIR $APP_DIR  \n ---&gt; Running in 48988746fa2b\n ---&gt; 72154b0c43b2\nRemoving intermediate container 48988746fa2b  \nStep 4 : ADD ./build/libs/queued-0.1.jar $APP_DIR/queued.jar  \n ---&gt; c88d3ddabf0e\nRemoving intermediate container 1f7d9997ed39  \nStep 5 : CMD java -Djava.security.egd=file:/dev/./urandom -jar queued.jar  \n ---&gt; Running in d9d377cd1bc3\n ---&gt; dedd6291688f\nRemoving intermediate container d9d377cd1bc3  \nSuccessfully built dedd6291688f  \n</code></pre>\n\n<p>So, we build a image with tag 'my-application', let's run it ?</p>\n\n<h5 id=\"runningdockerimageonserver\">Running docker image on server</h5>\n\n<p>Just execute this command</p>\n\n<pre><code>docker run -d \\  \n  -h my-application \\\n  -p 8080:8080 \\\n  --restart always \\\n  my-application\n</code></pre>\n\n<h5 id=\"testingmyapplication\">Testing my-application</h5>\n\n<p>First of all, let's ignore any ui, and try to get as much information from console, as we can. We need to get ip address of the machine, so, this command will do the work</p>\n\n<pre><code>docker-machine ip my-application  \n</code></pre>\n\n<pre><code>  queued-rest-api git:(master)  docker-machine ip my-application\n104.131.4.132  \n</code></pre>\n\n<p>Believe me, or not, but then I enter postman app, sent request, and received message.</p>\n\n<p><img src=\"/content/images/2016/06/Screen-Shot-2016-06-25-at-4-04-54-PM.png\" alt=\"\" /></p>\n\n<p>Of course, it's not a proof, so, I encourage you to try this steps for yourself.</p>\n\n<h5 id=\"dontforgettoremoveyourdockerinstance\">Don't forget to remove your docker instance</h5>\n\n<p>As you remember, you are playing with cloud, and pay money for machines, so don't forget to remove your machine</p>\n\n<pre><code>docker-machine rm my-application  \n</code></pre>\n\n<p>Make sure you don't see any digitalocean droplets with name 'my-application'.</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-06-25T12:18:41.000Z","created_by":1,"updated_at":"2016-06-25T16:25:11.000Z","updated_by":1,"published_at":"2016-06-25T13:10:06.000Z","published_by":1},{"id":44,"uuid":"8015edaa-df8f-42cd-87dc-6facf7f852a9","title":"Tee Streaming","slug":"tee-streaming","markdown":"Few days ago I faced an issue with using Java InputStream in parallel.\nImagine following situation: you have [InputStream](https://docs.oracle.com/javase/7/docs/api/java/io/InputStream.html), which you need to use in parallel. First thing - you can't use it in parallel, because InputStream keeps some pointer, which store information about where stream position is. More realistic scenario is to make first call asynchronous, and leave second as it is. But again, if we are working with streams, after we read it fully, there shouldn't be anything to read again, right ? So, this article is about problem of parallel read and how to fix them.\nWatch this example to understand why parallel stream read is a bad idea\n<script src=\"https://gist.github.com/ivanursul/f4cd44a4460452a5ee8ebb6fc1eaff98.js\"></script>\n\n\nThe output will be similar to:\n\n```\nmain thread line: Number1 Number2 Number3 Number4 Number5 Number6 Number7 Number8 Number9 Number...\nt1 line: Number831 Number832 Number833 Number834 Number835 Number836 Number837 Number838 Number8...\n```\n\nAs we see, some of the numbers are in the first line, and some of - in the second.\n\n###### Main thread could be executed first\n\n<script src=\"https://gist.github.com/ivanursul/8b6561a0cd9c0604f80e0b861502389d.js\"></script>\n\nThe result is even funnier:\n```\nmain thread line: Number1 Number2 Number3 Number4 Numbe\nt1 line: \n```\n\nBecause main thread read everything first, there was nothing to read for t1 thread.\n\n######TeeInputStream\n\n<script src=\"https://gist.github.com/ivanursul/71e53b3ed4bae44388d22ac6280b06a5.js\"></script>\n\nThe idea is quite simple\nhttps://en.wikipedia.org/wiki/Tee_(command)\n\n![alt](https://upload.wikimedia.org/wikipedia/commons/thumb/2/24/Tee.svg/400px-Tee.svg.png)\n\nYou read from InputStream, but in parallel, you write to another source.\n\nSo in our case, we read from InputStream, but write to ByteArrayOutputStream, which will later be used to get bytes from it. In java, we have [TeeInputStream](https://commons.apache.org/proper/commons-io/javadocs/api-1.4/org/apache/commons/io/input/TeeInputStream.html), class, which is doing exactly the same thing as Linux tee command does. You should use [Apache Commons IO](https://commons.apache.org/proper/commons-io/)\n\n```\n    /**\n     * Creates a TeeInputStream that proxies the given {@link InputStream}\n     * and copies all read bytes to the given {@link OutputStream}. The given\n     * output stream will be closed when this stream gets closed if the\n     * closeBranch parameter is {@code true}.\n     *\n     * @param input input stream to be proxied\n     * @param branch output stream that will receive a copy of all bytes read\n     * @param closeBranch flag for closing also the output stream when this\n     *                    stream is closed\n     */\n    public TeeInputStream(\n            InputStream input, OutputStream branch, boolean closeBranch) {\n        super(input);\n        this.branch = branch;\n        this.closeBranch = closeBranch;\n    }\n```","mobiledoc":null,"html":"<p>Few days ago I faced an issue with using Java InputStream in parallel. <br />\nImagine following situation: you have <a href=\"https://docs.oracle.com/javase/7/docs/api/java/io/InputStream.html\">InputStream</a>, which you need to use in parallel. First thing - you can't use it in parallel, because InputStream keeps some pointer, which store information about where stream position is. More realistic scenario is to make first call asynchronous, and leave second as it is. But again, if we are working with streams, after we read it fully, there shouldn't be anything to read again, right ? So, this article is about problem of parallel read and how to fix them. <br />\nWatch this example to understand why parallel stream read is a bad idea  </p>\n\n<script src=\"https://gist.github.com/ivanursul/f4cd44a4460452a5ee8ebb6fc1eaff98.js\"></script>\n\n<p>The output will be similar to:</p>\n\n<pre><code>main thread line: Number1 Number2 Number3 Number4 Number5 Number6 Number7 Number8 Number9 Number...  \nt1 line: Number831 Number832 Number833 Number834 Number835 Number836 Number837 Number838 Number8...  \n</code></pre>\n\n<p>As we see, some of the numbers are in the first line, and some of - in the second.</p>\n\n<h6 id=\"mainthreadcouldbeexecutedfirst\">Main thread could be executed first</h6>\n\n<script src=\"https://gist.github.com/ivanursul/8b6561a0cd9c0604f80e0b861502389d.js\"></script>\n\n<p>The result is even funnier:  </p>\n\n<pre><code>main thread line: Number1 Number2 Number3 Number4 Numbe  \nt1 line:  \n</code></pre>\n\n<p>Because main thread read everything first, there was nothing to read for t1 thread.</p>\n\n<h6 id=\"teeinputstream\">TeeInputStream</h6>\n\n<script src=\"https://gist.github.com/ivanursul/71e53b3ed4bae44388d22ac6280b06a5.js\"></script>\n\n<p>The idea is quite simple <br />\n<a href=\"https://en.wikipedia.org/wiki/Tee_(command\">https://en.wikipedia.org/wiki/Tee_(command</a>)</p>\n\n<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/24/Tee.svg/400px-Tee.svg.png\" alt=\"alt\" /></p>\n\n<p>You read from InputStream, but in parallel, you write to another source.</p>\n\n<p>So in our case, we read from InputStream, but write to ByteArrayOutputStream, which will later be used to get bytes from it. In java, we have <a href=\"https://commons.apache.org/proper/commons-io/javadocs/api-1.4/org/apache/commons/io/input/TeeInputStream.html\">TeeInputStream</a>, class, which is doing exactly the same thing as Linux tee command does. You should use <a href=\"https://commons.apache.org/proper/commons-io/\">Apache Commons IO</a></p>\n\n<pre><code>    /**\n     * Creates a TeeInputStream that proxies the given {@link InputStream}\n     * and copies all read bytes to the given {@link OutputStream}. The given\n     * output stream will be closed when this stream gets closed if the\n     * closeBranch parameter is {@code true}.\n     *\n     * @param input input stream to be proxied\n     * @param branch output stream that will receive a copy of all bytes read\n     * @param closeBranch flag for closing also the output stream when this\n     *                    stream is closed\n     */\n    public TeeInputStream(\n            InputStream input, OutputStream branch, boolean closeBranch) {\n        super(input);\n        this.branch = branch;\n        this.closeBranch = closeBranch;\n    }\n</code></pre>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-06-29T16:35:02.000Z","created_by":1,"updated_at":"2016-08-14T06:28:27.000Z","updated_by":1,"published_at":"2016-06-29T18:09:02.000Z","published_by":1},{"id":45,"uuid":"baa33d61-dd3b-421f-9f5e-9e6f3d6af74b","title":"Apache Kafka. Basics","slug":"apache-kafka-basics","markdown":"#### Preface\n\nToday, we live in a world, which defines no ip addresses and is dynamically changing minute by minute. As amount of services increase every day, we receive new problems.\n\n#### Story\nJust imagine, that you have a monolithic application, which you want to rewrite into microservices architecture. You start with a single service, which, let's say, maintains profile functionality. You use MongoDB as a database.\n![Profile service](/content/images/2016/07/Screen-Shot-2016-07-03-at-2-55-13-PM.png)\n\nAt this step you don't have any troubles, because, it's a single one, and it doesn't interact with other world. You developed some required amount of endpoints, everything works fine. Then imagine, that your next step is to start doing another service, let's say, billing service, which uses PostgreSQL for storing transactions. Besides that, you need to know, when Profile service receives new put request and updates some profile.\n\n![billing service](/content/images/2016/07/Screen-Shot-2016-07-03-at-3-06-25-PM.png)\n\nSo you have two options - develop an external API for your case or work with PostgreSQL db straightly. So, you chose to work with db, and your problems begin with this point: you coupled profile and billing service together. You are signing a contract, that from now on, you need to think about two databases. And this sucks. Here's why\n\n![Microservices coupling](/content/images/2016/07/Screen-Shot-2016-07-03-at-3-09-49-PM.png)\n\nAfter some time you receive a mess with all your structure, and you can't control it, because everything is coupled. You analyse your Profile service and see, that he is doing interactions with other db's just because he needs to keep consistency of their state\n\n![Profile usage](/content/images/2016/07/Screen-Shot-2016-07-03-at-3-13-14-PM.png)\n\nIt's not the job of Profile service to keep other state consistent. That's why it should be eliminated.\n\n#### Event Hub\n\nYou need to somehow decouple your services, right ? I see one problem - if you are using secondary database for your work, you are breaking [Database per service](http://microservices.io/patterns/data/database-per-service.html) pattern, which explains why it's bad. \n\nWhat I propose is to think about profile update, as some event. I think, that Profile service shouldn't work with other services internal structures, but instead, he should notify them by some event. And this events should be in some event bus. Every service, who wants to know when Profile is updated, should listen to this event, and when profile get's updated - event should be fired, and everyone should get notification.\n\n![](/content/images/2016/07/Screen-Shot-2016-07-03-at-3-23-12-PM.png)\n\nSo, when you are going to speak about Kafka ? Ok, what I propose is to use Kafka as an event bus - when profile get's updated - he push a message into Kafka. In this case, Profile service is a [Kafka Producer](http://kafka.apache.org/documentation.html#intro_producers). Profile service sends this message to this event bus, which in terms of Kafka is called [Kafka Topic](http://kafka.apache.org/documentation.html#intro_topics).\nMessages are appended in Kafka Topic, and they don't disappear after being consumed. That's why Kafka is a commit log. Topics are stored on Kafka Server, which is sometimes called Kafka Broker. \n![](/content/images/2016/07/Screen-Shot-2016-07-03-at-3-40-23-PM.png)\n\n#### Kafka Brokers\n\nThere can be multiple Kafka Brokers. Each Kafka Server can have multiple Leader and Follower partitions. There's a Zookeper, which stores information about topics, brokers, consumers and producers.\n![](/content/images/2016/07/Screen-Shot-2016-07-03-at-3-44-49-PM.png)\n\n#### Anatomy of topic.\n\n###### Partition\nEach topic consists of one or more partitions. Partitions could be either Leader or Follower. If Partitions acts as a leader, then it handles all read and writes to the given partition. If it's a follower, then it acts as a replication, and, only in case leader will fall down, some of followers will became a leader.\n\n###### Guarantees\n\nAt a high-level Kafka gives the following guarantees:\n\n* Messages sent by a producer to a particular topic partition will be appended in the order they are sent. That is, if a message M1 is sent by the same producer as a message M2, and M1 is sent first, then M1 will have a lower offset than M2 and appear earlier in the log.\n* A consumer instance sees messages in the order they are stored in the log.\n* Kafka tolerate up to N-1 server failures without losing any messages committed to the log, where N - replication factor.\n\n\n#### Consumers\n\n###### Consumer groups\n\nWait, if we are speaking about messaging service, then we know, that there are two ways of consuming event messages: by using queue, when a single message is delivered to a single consumer, and pub/sub, when message is broadcast to all consumers. How Kafka achieve this ? By using Consumer groups. \n\nIf you want to have **Queue** model - you need to have all your consumers in a single consumer group. \n![](/content/images/2016/07/Screen-Shot-2016-07-03-at-4-14-53-PM.png)\n\nIf you want to have **Publish/Subscribe** - you need to have all consumers in a different consumer groups.\n\n![](/content/images/2016/07/Screen-Shot-2016-07-03-at-4-17-06-PM.png)\n\n###### Conclusion\n\nI'll try to write more practical article soon, for now, it's enough to start understanding kafka.\n\n#### Links\n\n* [Kafka Documentation](https://kafka.apache.org/documentation.html)\n* [Introduction to Apache Kafka by Joe Stein](https://www.youtube.com/watch?v=qc33qMUvR7c)\n* [How does Apache Kafka work?](https://www.youtube.com/watch?v=EiWsPd6JDoo)","mobiledoc":null,"html":"<h4 id=\"preface\">Preface</h4>\n\n<p>Today, we live in a world, which defines no ip addresses and is dynamically changing minute by minute. As amount of services increase every day, we receive new problems.</p>\n\n<h4 id=\"story\">Story</h4>\n\n<p>Just imagine, that you have a monolithic application, which you want to rewrite into microservices architecture. You start with a single service, which, let's say, maintains profile functionality. You use MongoDB as a database. <br />\n<img src=\"/content/images/2016/07/Screen-Shot-2016-07-03-at-2-55-13-PM.png\" alt=\"Profile service\" /></p>\n\n<p>At this step you don't have any troubles, because, it's a single one, and it doesn't interact with other world. You developed some required amount of endpoints, everything works fine. Then imagine, that your next step is to start doing another service, let's say, billing service, which uses PostgreSQL for storing transactions. Besides that, you need to know, when Profile service receives new put request and updates some profile.</p>\n\n<p><img src=\"/content/images/2016/07/Screen-Shot-2016-07-03-at-3-06-25-PM.png\" alt=\"billing service\" /></p>\n\n<p>So you have two options - develop an external API for your case or work with PostgreSQL db straightly. So, you chose to work with db, and your problems begin with this point: you coupled profile and billing service together. You are signing a contract, that from now on, you need to think about two databases. And this sucks. Here's why</p>\n\n<p><img src=\"/content/images/2016/07/Screen-Shot-2016-07-03-at-3-09-49-PM.png\" alt=\"Microservices coupling\" /></p>\n\n<p>After some time you receive a mess with all your structure, and you can't control it, because everything is coupled. You analyse your Profile service and see, that he is doing interactions with other db's just because he needs to keep consistency of their state</p>\n\n<p><img src=\"/content/images/2016/07/Screen-Shot-2016-07-03-at-3-13-14-PM.png\" alt=\"Profile usage\" /></p>\n\n<p>It's not the job of Profile service to keep other state consistent. That's why it should be eliminated.</p>\n\n<h4 id=\"eventhub\">Event Hub</h4>\n\n<p>You need to somehow decouple your services, right ? I see one problem - if you are using secondary database for your work, you are breaking <a href=\"http://microservices.io/patterns/data/database-per-service.html\">Database per service</a> pattern, which explains why it's bad. </p>\n\n<p>What I propose is to think about profile update, as some event. I think, that Profile service shouldn't work with other services internal structures, but instead, he should notify them by some event. And this events should be in some event bus. Every service, who wants to know when Profile is updated, should listen to this event, and when profile get's updated - event should be fired, and everyone should get notification.</p>\n\n<p><img src=\"/content/images/2016/07/Screen-Shot-2016-07-03-at-3-23-12-PM.png\" alt=\"\" /></p>\n\n<p>So, when you are going to speak about Kafka ? Ok, what I propose is to use Kafka as an event bus - when profile get's updated - he push a message into Kafka. In this case, Profile service is a <a href=\"http://kafka.apache.org/documentation.html#intro_producers\">Kafka Producer</a>. Profile service sends this message to this event bus, which in terms of Kafka is called <a href=\"http://kafka.apache.org/documentation.html#intro_topics\">Kafka Topic</a>. <br />\nMessages are appended in Kafka Topic, and they don't disappear after being consumed. That's why Kafka is a commit log. Topics are stored on Kafka Server, which is sometimes called Kafka Broker. <br />\n<img src=\"/content/images/2016/07/Screen-Shot-2016-07-03-at-3-40-23-PM.png\" alt=\"\" /></p>\n\n<h4 id=\"kafkabrokers\">Kafka Brokers</h4>\n\n<p>There can be multiple Kafka Brokers. Each Kafka Server can have multiple Leader and Follower partitions. There's a Zookeper, which stores information about topics, brokers, consumers and producers. <br />\n<img src=\"/content/images/2016/07/Screen-Shot-2016-07-03-at-3-44-49-PM.png\" alt=\"\" /></p>\n\n<h4 id=\"anatomyoftopic\">Anatomy of topic.</h4>\n\n<h6 id=\"partition\">Partition</h6>\n\n<p>Each topic consists of one or more partitions. Partitions could be either Leader or Follower. If Partitions acts as a leader, then it handles all read and writes to the given partition. If it's a follower, then it acts as a replication, and, only in case leader will fall down, some of followers will became a leader.</p>\n\n<h6 id=\"guarantees\">Guarantees</h6>\n\n<p>At a high-level Kafka gives the following guarantees:</p>\n\n<ul>\n<li>Messages sent by a producer to a particular topic partition will be appended in the order they are sent. That is, if a message M1 is sent by the same producer as a message M2, and M1 is sent first, then M1 will have a lower offset than M2 and appear earlier in the log.</li>\n<li>A consumer instance sees messages in the order they are stored in the log.</li>\n<li>Kafka tolerate up to N-1 server failures without losing any messages committed to the log, where N - replication factor.</li>\n</ul>\n\n<h4 id=\"consumers\">Consumers</h4>\n\n<h6 id=\"consumergroups\">Consumer groups</h6>\n\n<p>Wait, if we are speaking about messaging service, then we know, that there are two ways of consuming event messages: by using queue, when a single message is delivered to a single consumer, and pub/sub, when message is broadcast to all consumers. How Kafka achieve this ? By using Consumer groups. </p>\n\n<p>If you want to have <strong>Queue</strong> model - you need to have all your consumers in a single consumer group. <br />\n<img src=\"/content/images/2016/07/Screen-Shot-2016-07-03-at-4-14-53-PM.png\" alt=\"\" /></p>\n\n<p>If you want to have <strong>Publish/Subscribe</strong> - you need to have all consumers in a different consumer groups.</p>\n\n<p><img src=\"/content/images/2016/07/Screen-Shot-2016-07-03-at-4-17-06-PM.png\" alt=\"\" /></p>\n\n<h6 id=\"conclusion\">Conclusion</h6>\n\n<p>I'll try to write more practical article soon, for now, it's enough to start understanding kafka.</p>\n\n<h4 id=\"links\">Links</h4>\n\n<ul>\n<li><a href=\"https://kafka.apache.org/documentation.html\">Kafka Documentation</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=qc33qMUvR7c\">Introduction to Apache Kafka by Joe Stein</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=EiWsPd6JDoo\">How does Apache Kafka work?</a></li>\n</ul>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-07-03T11:38:21.000Z","created_by":1,"updated_at":"2016-07-10T09:46:46.000Z","updated_by":1,"published_at":"2016-07-10T09:33:31.000Z","published_by":1},{"id":46,"uuid":"89363f73-b2a0-4e4e-8f96-47e77d0c46d0","title":"Kafka differences: topics, partitions, publish/subscribe and queue.","slug":"kafka-differences-topics-partitions-publish-subscribe-and-queue","markdown":"When I first started to look at Kafka as a event publishing system, my first question was 'How to create pub/sub and queue type of events?'. Unfortunately, there was no quick answers for that, because the way Kafka works. My previous article was about [Kafka basics](http://ivanursul.com/apache-kafka-basics/), which you can, of course, read and get more information about this cool [commit log](http://kafka.apache.org/documentation.html#introduction). By this article I'll try to explain why Kafka is different from other similar system, how it differs, and will try to answer to all interesting questions, which I had in the beginning.\n\n###### Why Kafka is a commit log?\nSimply, because Kafka works different to other pub/sub systems. It's a commit log, where new messages are being appended constantly.\n![](https://content.linkedin.com/content/dam/engineering/en-us/blog/migrated/log.png)\n\nEach message has it's own unique id, which is called [offset](http://kafka.apache.org/documentation.html#intro_topics)\n\n###### Okay, and how to consume this so called 'commit log' ?\n\nConsumer stores one thing - offset, and he is responsible for reading messages. Consider this console consumer\n\n> bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning\n\nAs you see, this consumer will read all log from the beginning.\n\n###### Are messages being deleted ?\n\nYes, after some time, there's a retention policy.\n\n###### So, how to create pub/sub model ?\n\nEvery consumer should be in a unique consumer group.\n\n###### What about queue model ?\n\nYou set all consumers in a single consumer group and message will be delivered to someone from the group.\n\n###### A strange thing, don't you think ?\n\nMe not. The reason for that is because each partition is will be consumed per exactly one consumer in a consumer group. That's hard, yes. Partition is a unit of parallelism inside consumer group. When you have one partition and lot of consumer groups - you won't get any troubles. But when you have a single consumer group and only one partion, then ....\n\n###### Remember to make sure you have enough partitions in your topic.\n\n> bin/zookeeper-server-start.sh config/zookeeper.properties\n\n> bin/kafka-server-start.sh config/server.properties\n\n> bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test\n\n> bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --consumer.config config/consumer-1.properties\n\n> bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --consumer.config config/consumer-2.properties\n\n**consumer-1.properties:**\n```\ngroup.id=group-1\n```\n\n**consumer-2.properties:**\n```\ngroup.id=group-2\n```\n\n> bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test\n\nTry to type some messages. You'll see, that messages will be consumed by both consumers.\n\nNow, let's change something. Shutdown consumers, edit consumer properties and restart consumers.\n\n**consumer-1.properties:**\n```\ngroup.id=group-1\n```\n\n**consumer-2.properties:**\n```\ngroup.id=group-1\n```\n\nYou will get a similar message\n```\n[2016-07-13 22:11:40,584] WARN No broker partitions consumed by consumer thread test-consumer-group_Ivans-MacBook-Pro.local-1468437100356-aa97cd2a-0 for topic test (kafka.consumer.RangeAssignor)\n```\n\nThat's mean, that you don't have enough partitions. Remember, partition is used to parallelise things inside topic.\n\nPossible options - create topic with more partitions and restart\n\n> bin/kafka-topics.sh --create --zookeeper localhost:2181 --partitions 2 --topic test\n\n###### What are the guarantees ? \nAt a high-level Kafka gives the following guarantees:\n\n* Messages sent by a producer to a particular topic partition will be appended in the order they are sent. That is, if a message M1 is sent by the same producer as a message M2, and M1 is sent first, then M1 will have a lower offset than M2 and appear earlier in the log.\n* A consumer instance sees messages in the order they are stored in the log.\n* For a topic with replication factor N, we will tolerate up to N-1 server failures without losing any messages committed to the log.\n\nSource: Apache Kafka [documentation](http://kafka.apache.org/documentation.html#intro_guarantees)\n\n###### How not to loose messages ?\n\nRead about replication factor. In a nutshell, you need to deploy multiple Kafka brokers and distribute replicas across multiple brokers. In a single partition, there's always a leader, and a follower. Leader partition is responsible for all read/write requests within some broker. If a leader will fail - then there'll be an election and a new leader will be elected.\n\n###### Can a broker hold multiple leaders ?\n\nSure, for example, if you have a single broker.\n\n\nLooks like, that's it, I',m finished, but I'd be glad to get more suggested questions from you. Feel free to post them in your messages and let's answer them together.\n\n","mobiledoc":null,"html":"<p>When I first started to look at Kafka as a event publishing system, my first question was 'How to create pub/sub and queue type of events?'. Unfortunately, there was no quick answers for that, because the way Kafka works. My previous article was about <a href=\"http://ivanursul.com/apache-kafka-basics/\">Kafka basics</a>, which you can, of course, read and get more information about this cool <a href=\"http://kafka.apache.org/documentation.html#introduction\">commit log</a>. By this article I'll try to explain why Kafka is different from other similar system, how it differs, and will try to answer to all interesting questions, which I had in the beginning.</p>\n\n<h6 id=\"whykafkaisacommitlog\">Why Kafka is a commit log?</h6>\n\n<p>Simply, because Kafka works different to other pub/sub systems. It's a commit log, where new messages are being appended constantly. <br />\n<img src=\"https://content.linkedin.com/content/dam/engineering/en-us/blog/migrated/log.png\" alt=\"\" /></p>\n\n<p>Each message has it's own unique id, which is called <a href=\"http://kafka.apache.org/documentation.html#intro_topics\">offset</a></p>\n\n<h6 id=\"okayandhowtoconsumethissocalledcommitlog\">Okay, and how to consume this so called 'commit log' ?</h6>\n\n<p>Consumer stores one thing - offset, and he is responsible for reading messages. Consider this console consumer</p>\n\n<blockquote>\n  <p>bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning</p>\n</blockquote>\n\n<p>As you see, this consumer will read all log from the beginning.</p>\n\n<h6 id=\"aremessagesbeingdeleted\">Are messages being deleted ?</h6>\n\n<p>Yes, after some time, there's a retention policy.</p>\n\n<h6 id=\"sohowtocreatepubsubmodel\">So, how to create pub/sub model ?</h6>\n\n<p>Every consumer should be in a unique consumer group.</p>\n\n<h6 id=\"whataboutqueuemodel\">What about queue model ?</h6>\n\n<p>You set all consumers in a single consumer group and message will be delivered to someone from the group.</p>\n\n<h6 id=\"astrangethingdontyouthink\">A strange thing, don't you think ?</h6>\n\n<p>Me not. The reason for that is because each partition is will be consumed per exactly one consumer in a consumer group. That's hard, yes. Partition is a unit of parallelism inside consumer group. When you have one partition and lot of consumer groups - you won't get any troubles. But when you have a single consumer group and only one partion, then ....</p>\n\n<h6 id=\"remembertomakesureyouhaveenoughpartitionsinyourtopic\">Remember to make sure you have enough partitions in your topic.</h6>\n\n<blockquote>\n  <p>bin/zookeeper-server-start.sh config/zookeeper.properties</p>\n  \n  <p>bin/kafka-server-start.sh config/server.properties</p>\n  \n  <p>bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test</p>\n  \n  <p>bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --consumer.config config/consumer-1.properties</p>\n  \n  <p>bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --consumer.config config/consumer-2.properties</p>\n</blockquote>\n\n<p><strong>consumer-1.properties:</strong></p>\n\n<pre><code>group.id=group-1  \n</code></pre>\n\n<p><strong>consumer-2.properties:</strong></p>\n\n<pre><code>group.id=group-2  \n</code></pre>\n\n<blockquote>\n  <p>bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test</p>\n</blockquote>\n\n<p>Try to type some messages. You'll see, that messages will be consumed by both consumers.</p>\n\n<p>Now, let's change something. Shutdown consumers, edit consumer properties and restart consumers.</p>\n\n<p><strong>consumer-1.properties:</strong></p>\n\n<pre><code>group.id=group-1  \n</code></pre>\n\n<p><strong>consumer-2.properties:</strong></p>\n\n<pre><code>group.id=group-1  \n</code></pre>\n\n<p>You will get a similar message  </p>\n\n<pre><code>[2016-07-13 22:11:40,584] WARN No broker partitions consumed by consumer thread test-consumer-group_Ivans-MacBook-Pro.local-1468437100356-aa97cd2a-0 for topic test (kafka.consumer.RangeAssignor)\n</code></pre>\n\n<p>That's mean, that you don't have enough partitions. Remember, partition is used to parallelise things inside topic.</p>\n\n<p>Possible options - create topic with more partitions and restart</p>\n\n<blockquote>\n  <p>bin/kafka-topics.sh --create --zookeeper localhost:2181 --partitions 2 --topic test</p>\n</blockquote>\n\n<h6 id=\"whataretheguarantees\">What are the guarantees ?</h6>\n\n<p>At a high-level Kafka gives the following guarantees:</p>\n\n<ul>\n<li>Messages sent by a producer to a particular topic partition will be appended in the order they are sent. That is, if a message M1 is sent by the same producer as a message M2, and M1 is sent first, then M1 will have a lower offset than M2 and appear earlier in the log.</li>\n<li>A consumer instance sees messages in the order they are stored in the log.</li>\n<li>For a topic with replication factor N, we will tolerate up to N-1 server failures without losing any messages committed to the log.</li>\n</ul>\n\n<p>Source: Apache Kafka <a href=\"http://kafka.apache.org/documentation.html#intro_guarantees\">documentation</a></p>\n\n<h6 id=\"hownottoloosemessages\">How not to loose messages ?</h6>\n\n<p>Read about replication factor. In a nutshell, you need to deploy multiple Kafka brokers and distribute replicas across multiple brokers. In a single partition, there's always a leader, and a follower. Leader partition is responsible for all read/write requests within some broker. If a leader will fail - then there'll be an election and a new leader will be elected.</p>\n\n<h6 id=\"canabrokerholdmultipleleaders\">Can a broker hold multiple leaders ?</h6>\n\n<p>Sure, for example, if you have a single broker.</p>\n\n<p>Looks like, that's it, I',m finished, but I'd be glad to get more suggested questions from you. Feel free to post them in your messages and let's answer them together.</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-07-12T19:41:10.000Z","created_by":1,"updated_at":"2016-07-18T19:28:19.000Z","updated_by":1,"published_at":"2016-07-13T19:27:46.000Z","published_by":1},{"id":47,"uuid":"95ec627d-fa33-49aa-b991-bc044003fa98","title":"cv","slug":"cv","markdown":"","mobiledoc":null,"html":"","image":null,"featured":0,"page":1,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-07-13T21:12:00.000Z","created_by":1,"updated_at":"2016-10-09T05:59:39.000Z","updated_by":1,"published_at":"2016-07-13T21:12:00.000Z","published_by":1},{"id":48,"uuid":"d399f810-1b1c-4260-8f21-d8164cbe3632","title":"Trying new JUnit 5 - let's extend everything","slug":"trying-new-junit-5-lets-extend-everything","markdown":"<a href=\"#generalidea\" name=\"generalidea\">\n#### General idea\n</a>\n\nRecently, new JUnit version has been released. I found many useful things. Besides, there're lot's of useless features. At least, I think something won't be used in new JUnit version. By the way, here the new version - [JUnit 5](http://junit.org/junit5/)\n\n<a href=\"#goal\" name=\"goal\">\n#### JUnit Goal\n</a>\n\nIt's obvious, that JUnit decided to make their product more opensource - by releasing instruments, which will allow junit users to create lot's of [extensions](#extensions) \n\n<a href=\"#restrictions\" name=\"restictions\">\n#### Restrictions\n</a>\n\n* JUnit 5 is running on java > 1.8\n* JUnit 5 has lot's of new features, which are not working in JUnit 3 or 4. However, there is a project [JUnit Vintage](#howtomigrate) which allows older versions work on JUnit 5.\n\n<a href=\"#storyaboutanotherpackage\" name=\"storyaboutanotherpackage\">\n#### Story about another package\n</a>\n\nNew version has new package - `org.junit.jupiter.*`. This was done, mostly, for separating new version from previous versions, which completely differs.\n\n<a href=\"#transormations\" name=\"transformations\">\n#### Transformations\n</a>\n\nAll core annotation are located under `org.junit.jupiter.api` \n\n* **@Test** - not this annotation comes from completely new package\n\n* **@TestFactory** - comparing to something oldes, TestFactory is a replacement of [parameterized tests](https://github.com/junit-team/junit4/wiki/parameterized-tests)\n\n* **@BeforeEach** - new version of **@Before**. I wonder, why it was done.\n\n* **@AfterEach** - new version of @After.\n\n* **@BeforeAll** - replacement of **@BeforeClass**. Must be static, as usual.\n\n* **@AfterAll** - replacement of **@AfterClass**. Must be static.\n\n* **@Tag** - new version of **@Category**\n\n* **@Disabled** - new **@Ignore** annotation\n\n* **@ExtendWith** - something related to **@RunWith**, but the implementation is completely different\n\nAs you can see, JUnit made a lot of work on renaming it's annotation\n\n<a href=\"#nesting\" name=\"nesting\">\n#### Nested tests\n</a>\n\nNow, you can write tests classes inside your test classes. Purposes ? Don't know :)\n\n```\n@DisplayName(\"A stack\")\nclass TestingAStackDemo {\n\n    Stack<Object> stack;\n    boolean isRun = false;\n\n    @Test\n    @DisplayName(\"is instantiated with new Stack()\")\n    void isInstantiatedWithNew() {\n        new Stack<Object>();\n    }\n\n    @Nested\n    @DisplayName(\"when new\")\n    class WhenNew {\n\n        @BeforeEach\n        void init() {\n            stack = new Stack<Object>();\n        }\n\n        @Test\n        @DisplayName(\"is empty\")\n        void isEmpty() {\n            Assertions.assertTrue(stack.isEmpty());\n        }\n\n        @Test\n        @DisplayName(\"throws EmptyStackException when popped\")\n        void throwsExceptionWhenPopped() {\n            Assertions.expectThrows(EmptyStackException.class, () -> stack.pop());\n        }\n\n        @Test\n        @DisplayName(\"throws EmptyStackException when peeked\")\n        void throwsExceptionWhenPeeked() {\n            Assertions.expectThrows(EmptyStackException.class, () -> stack.peek());\n        }\n\n        @Nested\n        @DisplayName(\"after pushing an element\")\n        class AfterPushing {\n\n            String anElement = \"an element\";\n\n            @BeforeEach\n            void init() {\n                stack.push(anElement);\n            }\n\n            @Test\n            @DisplayName(\"it is no longer empty\")\n            void isEmpty() {\n                Assertions.assertFalse(stack.isEmpty());\n            }\n\n            @Test\n            @DisplayName(\"returns the element when popped and is empty\")\n            void returnElementWhenPopped() {\n                Assertions.assertEquals(anElement, stack.pop());\n                Assertions.assertTrue(stack.isEmpty());\n            }\n\n            @Test\n            @DisplayName(\"returns the element when peeked but remains not empty\")\n            void returnElementWhenPeeked() {\n                Assertions.assertEquals(anElement, stack.peek());\n                Assertions.assertFalse(stack.isEmpty());\n            }\n        }\n    }\n}\n```\n\n<a href=\"#extensions\" name=\"extensions\">\n##### Extensions instead of abstract before/after classes\n</a>\n\nThe idea of extensions is to provide an abstractions for operations, which had place before or after test execution in JUnit 3,4. This allows you to move all non-business logic from before/after sections to extensions\n\n\nThe following interfaces define the APIs for extending tests at various points in the test execution lifecycle. Consult the following sections for examples and the Javadoc for each of these interfaces in the org.junit.jupiter.api.extension package for further details:\n\n* [BeforeAllCallback](http://junit.org/junit5/docs/current/api/org/junit/jupiter/api/extension/BeforeAllCallback.html)\n\n* [BeforeEachCallback](http://junit.org/junit5/docs/current/api/org/junit/jupiter/api/extension/BeforeEachCallback.html)\n    \n    * [BeforeTestExecutionCallback](http://junit.org/junit5/docs/current/api/org/junit/jupiter/api/extension/BeforeTestExecutionCallback.html)\n    * [AfterTestExecutionCallback](http://junit.org/junit5/docs/current/api/org/junit/jupiter/api/extension/AfterTestExecutionCallback.html)\n\n* [AfterEachCallback](http://junit.org/junit5/docs/current/api/org/junit/jupiter/api/extension/AfterEachCallback.html)\n\n* [AfterAllCallback](http://junit.org/junit5/docs/current/api/org/junit/jupiter/api/extension/AfterAllCallback.html)\n\n<a href=\"#di\" name=\"di\">\n##### Dependency injection. Injection test context\n</a>\n\nThis version of Junit brings [dependency injection for constructors and methods](http://junit.org/junit5/docs/current/user-guide/#writing-tests-dependency-injection). [ParameterResolver](http://junit.org/junit5/docs/current/api/org/junit/jupiter/api/extension/ParameterResolver.html) is an API you should implement for your extension in order to use Dependency injection. Junit 5 ships with two pre-build parameter resolvers:\n\n* [TestInfoParameterResolver](https://github.com/junit-team/junit5/blob/r5.0.0-M2/junit-jupiter-engine/src/main/java/org/junit/jupiter/engine/extension/TestInfoParameterResolver.java)\n\n* [TestReporterParameterResolver](https://github.com/junit-team/junit5/blob/r5.0.0-M2/junit-jupiter-engine/src/main/java/org/junit/jupiter/engine/extension/TestReporterParameterResolver.java)\n\n<a href=\"#howtomigrate\" name=\"howtomigrate\">\n#### How to migrate. JUnit Vintage.\n</a>\n\nYou should have junit-vintage jar in your project. Juniter Platform Launcer will automaticalyy pick up all tests then.\n\n<a href=\"#links\" name=\"links\">\n#### Links\n</a>\n\n* [Documentation](http://junit.org/junit5/docs/current/user-guide/)\n* [Samples](https://github.com/junit-team/junit5-samples)\n\n![]()","mobiledoc":null,"html":"<p><a href=\"#generalidea\" name=\"generalidea\">  </p>\n\n<h4 id=\"generalidea\">General idea</h4>\n\n<p></a></p>\n\n<p>Recently, new JUnit version has been released. I found many useful things. Besides, there're lot's of useless features. At least, I think something won't be used in new JUnit version. By the way, here the new version - <a href=\"http://junit.org/junit5/\">JUnit 5</a></p>\n\n<p><a href=\"#goal\" name=\"goal\">  </p>\n\n<h4 id=\"junitgoal\">JUnit Goal</h4>\n\n<p></a></p>\n\n<p>It's obvious, that JUnit decided to make their product more opensource - by releasing instruments, which will allow junit users to create lot's of <a href=\"#extensions\">extensions</a> </p>\n\n<p><a href=\"#restrictions\" name=\"restictions\">  </p>\n\n<h4 id=\"restrictions\">Restrictions</h4>\n\n<p></a></p>\n\n<ul>\n<li>JUnit 5 is running on java > 1.8</li>\n<li>JUnit 5 has lot's of new features, which are not working in JUnit 3 or 4. However, there is a project <a href=\"#howtomigrate\">JUnit Vintage</a> which allows older versions work on JUnit 5.</li>\n</ul>\n\n<p><a href=\"#storyaboutanotherpackage\" name=\"storyaboutanotherpackage\">  </p>\n\n<h4 id=\"storyaboutanotherpackage\">Story about another package</h4>\n\n<p></a></p>\n\n<p>New version has new package - <code>org.junit.jupiter.*</code>. This was done, mostly, for separating new version from previous versions, which completely differs.</p>\n\n<p><a href=\"#transormations\" name=\"transformations\">  </p>\n\n<h4 id=\"transformations\">Transformations</h4>\n\n<p></a></p>\n\n<p>All core annotation are located under <code>org.junit.jupiter.api</code> </p>\n\n<ul>\n<li><p><strong>@Test</strong> - not this annotation comes from completely new package</p></li>\n<li><p><strong>@TestFactory</strong> - comparing to something oldes, TestFactory is a replacement of <a href=\"https://github.com/junit-team/junit4/wiki/parameterized-tests\">parameterized tests</a></p></li>\n<li><p><strong>@BeforeEach</strong> - new version of <strong>@Before</strong>. I wonder, why it was done.</p></li>\n<li><p><strong>@AfterEach</strong> - new version of @After.</p></li>\n<li><p><strong>@BeforeAll</strong> - replacement of <strong>@BeforeClass</strong>. Must be static, as usual.</p></li>\n<li><p><strong>@AfterAll</strong> - replacement of <strong>@AfterClass</strong>. Must be static.</p></li>\n<li><p><strong>@Tag</strong> - new version of <strong>@Category</strong></p></li>\n<li><p><strong>@Disabled</strong> - new <strong>@Ignore</strong> annotation</p></li>\n<li><p><strong>@ExtendWith</strong> - something related to <strong>@RunWith</strong>, but the implementation is completely different</p></li>\n</ul>\n\n<p>As you can see, JUnit made a lot of work on renaming it's annotation</p>\n\n<p><a href=\"#nesting\" name=\"nesting\">  </p>\n\n<h4 id=\"nestedtests\">Nested tests</h4>\n\n<p></a></p>\n\n<p>Now, you can write tests classes inside your test classes. Purposes ? Don't know :)</p>\n\n<pre><code>@DisplayName(\"A stack\")\nclass TestingAStackDemo {\n\n    Stack&lt;Object&gt; stack;\n    boolean isRun = false;\n\n    @Test\n    @DisplayName(\"is instantiated with new Stack()\")\n    void isInstantiatedWithNew() {\n        new Stack&lt;Object&gt;();\n    }\n\n    @Nested\n    @DisplayName(\"when new\")\n    class WhenNew {\n\n        @BeforeEach\n        void init() {\n            stack = new Stack&lt;Object&gt;();\n        }\n\n        @Test\n        @DisplayName(\"is empty\")\n        void isEmpty() {\n            Assertions.assertTrue(stack.isEmpty());\n        }\n\n        @Test\n        @DisplayName(\"throws EmptyStackException when popped\")\n        void throwsExceptionWhenPopped() {\n            Assertions.expectThrows(EmptyStackException.class, () -&gt; stack.pop());\n        }\n\n        @Test\n        @DisplayName(\"throws EmptyStackException when peeked\")\n        void throwsExceptionWhenPeeked() {\n            Assertions.expectThrows(EmptyStackException.class, () -&gt; stack.peek());\n        }\n\n        @Nested\n        @DisplayName(\"after pushing an element\")\n        class AfterPushing {\n\n            String anElement = \"an element\";\n\n            @BeforeEach\n            void init() {\n                stack.push(anElement);\n            }\n\n            @Test\n            @DisplayName(\"it is no longer empty\")\n            void isEmpty() {\n                Assertions.assertFalse(stack.isEmpty());\n            }\n\n            @Test\n            @DisplayName(\"returns the element when popped and is empty\")\n            void returnElementWhenPopped() {\n                Assertions.assertEquals(anElement, stack.pop());\n                Assertions.assertTrue(stack.isEmpty());\n            }\n\n            @Test\n            @DisplayName(\"returns the element when peeked but remains not empty\")\n            void returnElementWhenPeeked() {\n                Assertions.assertEquals(anElement, stack.peek());\n                Assertions.assertFalse(stack.isEmpty());\n            }\n        }\n    }\n}\n</code></pre>\n\n<p><a href=\"#extensions\" name=\"extensions\">  </p>\n\n<h5 id=\"extensionsinsteadofabstractbeforeafterclasses\">Extensions instead of abstract before/after classes</h5>\n\n<p></a></p>\n\n<p>The idea of extensions is to provide an abstractions for operations, which had place before or after test execution in JUnit 3,4. This allows you to move all non-business logic from before/after sections to extensions</p>\n\n<p>The following interfaces define the APIs for extending tests at various points in the test execution lifecycle. Consult the following sections for examples and the Javadoc for each of these interfaces in the org.junit.jupiter.api.extension package for further details:</p>\n\n<ul>\n<li><p><a href=\"http://junit.org/junit5/docs/current/api/org/junit/jupiter/api/extension/BeforeAllCallback.html\">BeforeAllCallback</a></p></li>\n<li><p><a href=\"http://junit.org/junit5/docs/current/api/org/junit/jupiter/api/extension/BeforeEachCallback.html\">BeforeEachCallback</a></p>\n\n<ul><li><a href=\"http://junit.org/junit5/docs/current/api/org/junit/jupiter/api/extension/BeforeTestExecutionCallback.html\">BeforeTestExecutionCallback</a></li>\n<li><a href=\"http://junit.org/junit5/docs/current/api/org/junit/jupiter/api/extension/AfterTestExecutionCallback.html\">AfterTestExecutionCallback</a></li></ul></li>\n<li><p><a href=\"http://junit.org/junit5/docs/current/api/org/junit/jupiter/api/extension/AfterEachCallback.html\">AfterEachCallback</a></p></li>\n<li><p><a href=\"http://junit.org/junit5/docs/current/api/org/junit/jupiter/api/extension/AfterAllCallback.html\">AfterAllCallback</a></p></li>\n</ul>\n\n<p><a href=\"#di\" name=\"di\">  </p>\n\n<h5 id=\"dependencyinjectioninjectiontestcontext\">Dependency injection. Injection test context</h5>\n\n<p></a></p>\n\n<p>This version of Junit brings <a href=\"http://junit.org/junit5/docs/current/user-guide/#writing-tests-dependency-injection\">dependency injection for constructors and methods</a>. <a href=\"http://junit.org/junit5/docs/current/api/org/junit/jupiter/api/extension/ParameterResolver.html\">ParameterResolver</a> is an API you should implement for your extension in order to use Dependency injection. Junit 5 ships with two pre-build parameter resolvers:</p>\n\n<ul>\n<li><p><a href=\"https://github.com/junit-team/junit5/blob/r5.0.0-M2/junit-jupiter-engine/src/main/java/org/junit/jupiter/engine/extension/TestInfoParameterResolver.java\">TestInfoParameterResolver</a></p></li>\n<li><p><a href=\"https://github.com/junit-team/junit5/blob/r5.0.0-M2/junit-jupiter-engine/src/main/java/org/junit/jupiter/engine/extension/TestReporterParameterResolver.java\">TestReporterParameterResolver</a></p></li>\n</ul>\n\n<p><a href=\"#howtomigrate\" name=\"howtomigrate\">  </p>\n\n<h4 id=\"howtomigratejunitvintage\">How to migrate. JUnit Vintage.</h4>\n\n<p></a></p>\n\n<p>You should have junit-vintage jar in your project. Juniter Platform Launcer will automaticalyy pick up all tests then.</p>\n\n<p><a href=\"#links\" name=\"links\">  </p>\n\n<h4 id=\"links\">Links</h4>\n\n<p></a></p>\n\n<ul>\n<li><a href=\"http://junit.org/junit5/docs/current/user-guide/\">Documentation</a></li>\n<li><a href=\"https://github.com/junit-team/junit5-samples\">Samples</a></li>\n</ul>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-07-18T19:43:28.000Z","created_by":1,"updated_at":"2016-08-17T15:38:18.000Z","updated_by":1,"published_at":"2016-07-19T07:50:14.000Z","published_by":1},{"id":49,"uuid":"5a6f1b2f-9f94-40ee-a715-ae9d07dd6daf","title":"Monitoring  your Spring application using Dropwizard metrics module","slug":"monitoring-your-spring-application-using-dropwizard-metrics-module","markdown":"Yes, it's true, that Spring is better, than Dropwizard. I've worked with both frameworks, and can truly say, that Dropwizard has poor Guice dependency injection, Jersey, which I don't like at all, and other things. But there's one thing, which I like in Dropwizard, and which I'd be happy to see in Spring Framework - [Dropwizard Metrics](http://metrics.dropwizard.io/3.1.0/) module. it has very rich number of instruments, which can help you to understand your application behaviour: different gauges, timers, counters, histograms, timers, and healthcheks.\n\n### <a href=\"#thequestion\" name=\"thequestion\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> The question is how to import Dropwizard Metrics into Spring? \n\nBasically, you need to add 3 dependencies:one as an adapter for dropwizard metrics, another is for jvm metrics and last one for Filter, which you will use to get this metrics using http.\n\n> [com.ryantenney.metrics:metrics-spring:3.1.2](https://www.google.com.ua/search?q=com.ryantenney.metrics%3Ametrics-spring%3A3.1.2&oq=com.ryantenney.metrics%3Ametrics-spring%3A3.1.2&aqs=chrome..69i57.248j0j7&sourceid=chrome&ie=UTF-8)\n\n> [io.dropwizard.metrics:metrics-jvm:3.1.2](https://mvnrepository.com/artifact/io.dropwizard.metrics/metrics-jvm/3.1.2)\n\n> [io.dropwizard.metrics:metrics-servlets:3.1.2](https://mvnrepository.com/artifact/io.dropwizard.metrics/metrics-servlets/3.1.0)\n\nDepending on what build tool you have in your application - include those libraries. I use Gradle, so my **build.gradle** looks like following: \n\n<script src=\"https://gist.github.com/ivanursul/5da9a51ec38d31635aa16b4522a40ad7.js\"></script>\n\n### <a href=\"#metricsListener\" name=\"metricsListener\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Create Metrics Listener \n\nOne thing you need to know about Dropwizard Metrics module is that it's required to have [MetricRegistry](http://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html) and [HealthCheckRegistry](http://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/health/HealthCheckRegistry.html) classes to be instantiated under ServletContext. That's why we'll get it from constructor, and initialise context.\n\n<script src=\"https://gist.github.com/ivanursul/e8301df33eebd9a2d3631db7f5cb0e8b.js\"></script>\n\nWe will create a separate bean for this listener soon.\n\n### <a href=\"#configclass\" name=\"configclass\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Creating configuration class \n\nNow, when you have everything ready to launch your metrics, you need to inject it somewhere. I propose to do a separate configuration class for that. Let's call it **MonitoringConfiguration**\n\n<script src=\"https://gist.github.com/ivanursul/2c056cfa657db78412a3aa6cfb6279fa.js\"></script>\n\nI bet, you've noticed couple of things:\n\n* `@EnableMetrics` annotation. Yes, you need to add it, don't forget.\n\n* **MetricsServletContextListener** initialisation. I don't have **web.xml** descriptor anymore in my app, so the true way to do it is by injecting it in spring context. Spring will take care of it.\n\n* Extending from **MetricsConfigurerAdapter**. This class comes from [com.ryantenney.metrics:metrics-spring:3.1.2](https://www.google.com.ua/search?q=com.ryantenney.metrics%3Ametrics-spring%3A3.1.2&oq=com.ryantenney.metrics%3Ametrics-spring%3A3.1.2&aqs=chrome..69i57.248j0j7&sourceid=chrome&ie=UTF-8) dependency\n\n* Implemented **configureReporters** method. By this method we decide what to do with our metrics. Later, in this article, I'll show you how to poll this metrics to some UI tools, which will visualise our results.\n\n### <a href=\"#tunecontroller\" name=\"tunecontroller\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Tune your controller methods \n\nIf you want to monitor your endpoints, you need to add additional annotations to them. Add **@Timed** and **@ExceptionMetered** annotations. **@Timed** will take all the measures, while **@ExceptionMetered** will measure all exceptional situations with your endpoint.\n\n<script src=\"https://gist.github.com/ivanursul/738d3af3567588139de2bfad50db6190.js\"></script>\n\n##### That's it\n\nNow, you are ready to access your application on `http://localhost:8080/dropwizard/metrics`:\n\n<script src=\"https://gist.github.com/ivanursul/ec73821fa78b61e4c7c412b504ed7a2d.js\"></script>\n\nSee how more descriptive it is, comparing to what 'metrics' we have in Spring.\n\n`AdminServlet` has another bunch of endpoints, which you can use:\n\n* /healthcheck: HealthCheckServlet\n* /metrics: MetricsServlet\n* /ping: PingServlet\n* /threads: ThreadDumpServlet\n\n##### Spring power together with Dropwizard metrics <a href=\"#springpower\" name=\"springpower\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a>\n\nNow as we have Spring here, we have the power of doing different BeanFactoryPostProcessor's for new metrics. It's much more easy, than doing it in Guice DI, which comes with Dropwizard. \n\n### <a href=\"#visualising\" name=\"visualising\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Visualising metrics \n\nWe can use [Graphite](https://graphiteapp.org) for that. [Here's how](https://gist.github.com/relaxdiego/7539911) you can install it on mac. For other OS's, find appropriate instuctions somewhere.\n\n![](https://lh6.ggpht.com/-Hb-nvEzQjk8/UUndShnqRKI/AAAAAAAAQP4/tznXzGXcUE0/image_thumb%25255B2%25255D.png?imgmax=800)\n\nAdd additional dependency:\n\n> [io.dropwizard.metrics:metrics-graphite:3.1.2](https://mvnrepository.com/artifact/io.dropwizard.metrics/metrics-graphite/3.1.2)\n\nThen, modify our configuration class for that:\n\n<script src=\"https://gist.github.com/ivanursul/b54ea49881ed1a086192214a9e0e2b0a.js\"></script>\n\n**application.properties:**\n\n<script src=\"https://gist.github.com/ivanursul/6401380148075ef919667ceaf65024dc.js\"></script>\n\n### <a href=\"#load\" name=\"load\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Give some load to your endpoints \n\nStart your Spring application, turn on Graphite on your local machine, and give some load to some of your endpoints. I prefer to do it [using load tests](http://ivanursul.com/performance-testing-explained/).\n\nAfter some load, you can see, how your application behaves: I showed m1,m5 and m15 rates.\n\n![](/content/images/2016/08/Screen-Shot-2016-08-13-at-9-21-23-PM.png)\n\nAs you can see, you're now able to get more information from your application. The information you have now is much more descriptive, than you had before. I usually speak with my colleagues from different company and ask how they measure and monitor their application activity. Most of the people answer, that they sent their logs somewhere and in case of incidents, they ask their devops to get information about their app. It's a big limitation, and I think each developer should have access to metrics dashboard.\n\n### <a href=\"#links\" name=\"links\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Links \n\n* [Dropwizard Metrics](http://metrics.dropwizard.io/3.1.0/)\n\n* [Ryan Tenney metrics module for Spring](http://metrics.ryantenney.com/)\n\n* [Spring Integration in Dropwizard](http://metrics.dropwizard.io/2.2.0/manual/spring/)\n\n\n","mobiledoc":null,"html":"<p>Yes, it's true, that Spring is better, than Dropwizard. I've worked with both frameworks, and can truly say, that Dropwizard has poor Guice dependency injection, Jersey, which I don't like at all, and other things. But there's one thing, which I like in Dropwizard, and which I'd be happy to see in Spring Framework - <a href=\"http://metrics.dropwizard.io/3.1.0/\">Dropwizard Metrics</a> module. it has very rich number of instruments, which can help you to understand your application behaviour: different gauges, timers, counters, histograms, timers, and healthcheks.</p>\n\n<h3 id=\"ahrefthequestionnamethequestioniclassanchorfafalinkariahiddentrueiathequestionishowtoimportdropwizardmetricsintospring\"><a href=\"#thequestion\" name=\"thequestion\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> The question is how to import Dropwizard Metrics into Spring?</h3>\n\n<p>Basically, you need to add 3 dependencies:one as an adapter for dropwizard metrics, another is for jvm metrics and last one for Filter, which you will use to get this metrics using http.</p>\n\n<blockquote>\n  <p><a href=\"https://www.google.com.ua/search?q=com.ryantenney.metrics%3Ametrics-spring%3A3.1.2&amp;oq=com.ryantenney.metrics%3Ametrics-spring%3A3.1.2&amp;aqs=chrome..69i57.248j0j7&amp;sourceid=chrome&amp;ie=UTF-8\">com.ryantenney.metrics:metrics-spring:3.1.2</a></p>\n  \n  <p><a href=\"https://mvnrepository.com/artifact/io.dropwizard.metrics/metrics-jvm/3.1.2\">io.dropwizard.metrics:metrics-jvm:3.1.2</a></p>\n  \n  <p><a href=\"https://mvnrepository.com/artifact/io.dropwizard.metrics/metrics-servlets/3.1.0\">io.dropwizard.metrics:metrics-servlets:3.1.2</a></p>\n</blockquote>\n\n<p>Depending on what build tool you have in your application - include those libraries. I use Gradle, so my <strong>build.gradle</strong> looks like following: </p>\n\n<script src=\"https://gist.github.com/ivanursul/5da9a51ec38d31635aa16b4522a40ad7.js\"></script>\n\n<h3 id=\"ahrefmetricslistenernamemetricslistenericlassanchorfafalinkariahiddentrueiacreatemetricslistener\"><a href=\"#metricsListener\" name=\"metricsListener\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Create Metrics Listener</h3>\n\n<p>One thing you need to know about Dropwizard Metrics module is that it's required to have <a href=\"http://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/MetricRegistry.html\">MetricRegistry</a> and <a href=\"http://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/health/HealthCheckRegistry.html\">HealthCheckRegistry</a> classes to be instantiated under ServletContext. That's why we'll get it from constructor, and initialise context.</p>\n\n<script src=\"https://gist.github.com/ivanursul/e8301df33eebd9a2d3631db7f5cb0e8b.js\"></script>\n\n<p>We will create a separate bean for this listener soon.</p>\n\n<h3 id=\"ahrefconfigclassnameconfigclassiclassanchorfafalinkariahiddentrueiacreatingconfigurationclass\"><a href=\"#configclass\" name=\"configclass\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Creating configuration class</h3>\n\n<p>Now, when you have everything ready to launch your metrics, you need to inject it somewhere. I propose to do a separate configuration class for that. Let's call it <strong>MonitoringConfiguration</strong></p>\n\n<script src=\"https://gist.github.com/ivanursul/2c056cfa657db78412a3aa6cfb6279fa.js\"></script>\n\n<p>I bet, you've noticed couple of things:</p>\n\n<ul>\n<li><p><code>@EnableMetrics</code> annotation. Yes, you need to add it, don't forget.</p></li>\n<li><p><strong>MetricsServletContextListener</strong> initialisation. I don't have <strong>web.xml</strong> descriptor anymore in my app, so the true way to do it is by injecting it in spring context. Spring will take care of it.</p></li>\n<li><p>Extending from <strong>MetricsConfigurerAdapter</strong>. This class comes from <a href=\"https://www.google.com.ua/search?q=com.ryantenney.metrics%3Ametrics-spring%3A3.1.2&amp;oq=com.ryantenney.metrics%3Ametrics-spring%3A3.1.2&amp;aqs=chrome..69i57.248j0j7&amp;sourceid=chrome&amp;ie=UTF-8\">com.ryantenney.metrics:metrics-spring:3.1.2</a> dependency</p></li>\n<li><p>Implemented <strong>configureReporters</strong> method. By this method we decide what to do with our metrics. Later, in this article, I'll show you how to poll this metrics to some UI tools, which will visualise our results.</p></li>\n</ul>\n\n<h3 id=\"ahreftunecontrollernametunecontrollericlassanchorfafalinkariahiddentrueiatuneyourcontrollermethods\"><a href=\"#tunecontroller\" name=\"tunecontroller\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Tune your controller methods</h3>\n\n<p>If you want to monitor your endpoints, you need to add additional annotations to them. Add <strong>@Timed</strong> and <strong>@ExceptionMetered</strong> annotations. <strong>@Timed</strong> will take all the measures, while <strong>@ExceptionMetered</strong> will measure all exceptional situations with your endpoint.</p>\n\n<script src=\"https://gist.github.com/ivanursul/738d3af3567588139de2bfad50db6190.js\"></script>\n\n<h5 id=\"thatsit\">That's it</h5>\n\n<p>Now, you are ready to access your application on <code>http://localhost:8080/dropwizard/metrics</code>:</p>\n\n<script src=\"https://gist.github.com/ivanursul/ec73821fa78b61e4c7c412b504ed7a2d.js\"></script>\n\n<p>See how more descriptive it is, comparing to what 'metrics' we have in Spring.</p>\n\n<p><code>AdminServlet</code> has another bunch of endpoints, which you can use:</p>\n\n<ul>\n<li>/healthcheck: HealthCheckServlet</li>\n<li>/metrics: MetricsServlet</li>\n<li>/ping: PingServlet</li>\n<li>/threads: ThreadDumpServlet</li>\n</ul>\n\n<h5 id=\"springpowertogetherwithdropwizardmetricsahrefspringpowernamespringpowericlassanchorfafalinkariahiddentrueia\">Spring power together with Dropwizard metrics <a href=\"#springpower\" name=\"springpower\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a></h5>\n\n<p>Now as we have Spring here, we have the power of doing different BeanFactoryPostProcessor's for new metrics. It's much more easy, than doing it in Guice DI, which comes with Dropwizard. </p>\n\n<h3 id=\"ahrefvisualisingnamevisualisingiclassanchorfafalinkariahiddentrueiavisualisingmetrics\"><a href=\"#visualising\" name=\"visualising\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Visualising metrics</h3>\n\n<p>We can use <a href=\"https://graphiteapp.org\">Graphite</a> for that. <a href=\"https://gist.github.com/relaxdiego/7539911\">Here's how</a> you can install it on mac. For other OS's, find appropriate instuctions somewhere.</p>\n\n<p><img src=\"https://lh6.ggpht.com/-Hb-nvEzQjk8/UUndShnqRKI/AAAAAAAAQP4/tznXzGXcUE0/image_thumb%25255B2%25255D.png?imgmax=800\" alt=\"\" /></p>\n\n<p>Add additional dependency:</p>\n\n<blockquote>\n  <p><a href=\"https://mvnrepository.com/artifact/io.dropwizard.metrics/metrics-graphite/3.1.2\">io.dropwizard.metrics:metrics-graphite:3.1.2</a></p>\n</blockquote>\n\n<p>Then, modify our configuration class for that:</p>\n\n<script src=\"https://gist.github.com/ivanursul/b54ea49881ed1a086192214a9e0e2b0a.js\"></script>\n\n<p><strong>application.properties:</strong></p>\n\n<script src=\"https://gist.github.com/ivanursul/6401380148075ef919667ceaf65024dc.js\"></script>\n\n<h3 id=\"ahrefloadnameloadiclassanchorfafalinkariahiddentrueiagivesomeloadtoyourendpoints\"><a href=\"#load\" name=\"load\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Give some load to your endpoints</h3>\n\n<p>Start your Spring application, turn on Graphite on your local machine, and give some load to some of your endpoints. I prefer to do it <a href=\"http://ivanursul.com/performance-testing-explained/\">using load tests</a>.</p>\n\n<p>After some load, you can see, how your application behaves: I showed m1,m5 and m15 rates.</p>\n\n<p><img src=\"/content/images/2016/08/Screen-Shot-2016-08-13-at-9-21-23-PM.png\" alt=\"\" /></p>\n\n<p>As you can see, you're now able to get more information from your application. The information you have now is much more descriptive, than you had before. I usually speak with my colleagues from different company and ask how they measure and monitor their application activity. Most of the people answer, that they sent their logs somewhere and in case of incidents, they ask their devops to get information about their app. It's a big limitation, and I think each developer should have access to metrics dashboard.</p>\n\n<h3 id=\"ahreflinksnamelinksiclassanchorfafalinkariahiddentrueialinks\"><a href=\"#links\" name=\"links\"><i class=\"anchor fa fa-link\" aria-hidden=\"true\"></i></a> Links</h3>\n\n<ul>\n<li><p><a href=\"http://metrics.dropwizard.io/3.1.0/\">Dropwizard Metrics</a></p></li>\n<li><p><a href=\"http://metrics.ryantenney.com/\">Ryan Tenney metrics module for Spring</a></p></li>\n<li><p><a href=\"http://metrics.dropwizard.io/2.2.0/manual/spring/\">Spring Integration in Dropwizard</a></p></li>\n</ul>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":"Monitoring your Spring Application using Dropwizard Metrics","meta_description":"Monitoring your Spring Application using Dropwizard Metrics. m1,m5,m15 rates, 99th,75th percentile, and a lot more in articlet","author_id":1,"created_at":"2016-08-13T18:29:06.000Z","created_by":1,"updated_at":"2016-12-01T09:58:59.000Z","updated_by":1,"published_at":"2016-08-13T18:29:28.000Z","published_by":1},{"id":50,"uuid":"6edb3534-0016-4c0f-a404-f1e8920abf84","title":"How to log requests and their payloads in Spring","slug":"how-to-log-requests-and-their-payloads-in-spring","markdown":"From time to time we may need to log our requests in order to get some information. Personally, I'm writing this short article, because we need to find out the reason why jackson throws 400 error status.\n\nLuckily, it's very easy to log your requests. Spring has a class `AbstractRequestLoggingFilter`, which has three concrete classes, which you can potentially use:\n\n* `ServletContextRequestLoggingFilter`\n* ~~`Log4jNestedDiagnosticContextFilter`~~ \n* **`CommonsRequestLoggingFilter`**\n\nThe last one is the guy we need. It's pretty straightforward how to configure this class: just declare it in your context:\n\n```\n    @Bean\n    public CommonsRequestLoggingFilter requestLoggingFilter() {\n        CommonsRequestLoggingFilter loggingFilter = new CommonsRequestLoggingFilter();\n        loggingFilter.setIncludeClientInfo(true);\n        loggingFilter.setIncludeQueryString(true);\n        loggingFilter.setIncludePayload(true);\n        return loggingFilter;\n    }\n```\n\nAnd yes, don't forget to add additional line in your `application.properties` file:\n```\nlogging.level.org.springframework.web.filter.CommonsRequestLoggingFilter=DEBUG\n```\n\nAnd voila, you have your requests being logged:\n```\n2016-10-24 21:50:45.520 DEBUG 83061 --- [nio-8080-exec-3] o.s.w.f.CommonsRequestLoggingFilter      : Before request [uri=/api/requests;client=0:0:0:0:0:0:0:1]\n... // My logs :)\n\n2016-10-24 21:50:45.526 DEBUG 83061 --- [nio-8080-exec-3] o.s.w.f.CommonsRequestLoggingFilter      : After request [uri=/api/requests;client=0:0:0:0:0:0:0:1;payload={\n  \"title\": \"...\"}]\n```\n","mobiledoc":null,"html":"<p>From time to time we may need to log our requests in order to get some information. Personally, I'm writing this short article, because we need to find out the reason why jackson throws 400 error status.</p>\n\n<p>Luckily, it's very easy to log your requests. Spring has a class <code>AbstractRequestLoggingFilter</code>, which has three concrete classes, which you can potentially use:</p>\n\n<ul>\n<li><code>ServletContextRequestLoggingFilter</code></li>\n<li><del><code>Log4jNestedDiagnosticContextFilter</code></del> </li>\n<li><strong><code>CommonsRequestLoggingFilter</code></strong></li>\n</ul>\n\n<p>The last one is the guy we need. It's pretty straightforward how to configure this class: just declare it in your context:</p>\n\n<pre><code>    @Bean\n    public CommonsRequestLoggingFilter requestLoggingFilter() {\n        CommonsRequestLoggingFilter loggingFilter = new CommonsRequestLoggingFilter();\n        loggingFilter.setIncludeClientInfo(true);\n        loggingFilter.setIncludeQueryString(true);\n        loggingFilter.setIncludePayload(true);\n        return loggingFilter;\n    }\n</code></pre>\n\n<p>And yes, don't forget to add additional line in your <code>application.properties</code> file:  </p>\n\n<pre><code>logging.level.org.springframework.web.filter.CommonsRequestLoggingFilter=DEBUG  \n</code></pre>\n\n<p>And voila, you have your requests being logged:  </p>\n\n<pre><code>2016-10-24 21:50:45.520 DEBUG 83061 --- [nio-8080-exec-3] o.s.w.f.CommonsRequestLoggingFilter      : Before request [uri=/api/requests;client=0:0:0:0:0:0:0:1]  \n... // My logs :)\n\n2016-10-24 21:50:45.526 DEBUG 83061 --- [nio-8080-exec-3] o.s.w.f.CommonsRequestLoggingFilter      : After request [uri=/api/requests;client=0:0:0:0:0:0:0:1;payload={  \n  \"title\": \"...\"}]\n</code></pre>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-10-24T18:59:38.000Z","created_by":1,"updated_at":"2016-10-25T20:01:46.000Z","updated_by":1,"published_at":"2016-10-24T19:09:35.000Z","published_by":1},{"id":51,"uuid":"6161fe86-de2e-4dea-afb5-2dd88adaca7c","title":"Better application deployment with DigitalOcean, Terraform,  Ansible and Docker. Creating basic terraform instances","slug":"better-application-deployment-with-terraform-ansible-and-docker-part-1","markdown":"### <a href=\"#argumentation\" name=\"argumentation\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Argumentation\nIt's not a rare examples, when people need to deploy their applications somewhere in the cloud, without having mature infrastructure, and just for testing purpose. Let's say, you're writing a backend part for mobile application, and your android developer needs to have backend right and now. Your actions ? Of course, create EC2/Droplet instance, **scp** all binaries, connect via **ssh**, run them. You can even write some bash script, which will do all the work. And what should you do, if some day you will decide to go live ? The same thing, you will order some instances in your favorite cloud, configure load balancer, add 1-5 instances and start running your application. But how to scale ? What, if you will decide to add couple servers, and balance them ? By the time you will need to do that, you will be the author of many scripts, which potentially have bugs. You should have something better in your arsenal. I spend couple of weeks to find an instrument, which will fit my needs, and want to share it here, in my blog. It's not claims to be the best solution, I'm sure, there's plenty of them, but by following series I'll explain what I've achieved.\n\n### <a href=\"#whom\" name=\"whom\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Whom this article is for ?\n\nThis is for software engineers, who wants to improve their devops processes. I don't expect, that true devops will find something useful here, because they should already have a good bundle of instruments.\n\n### <a href=\"#genearlidea\" name=\"generalidea\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> General idea\n\nYou will use three projects:\n\n* [Terraform](https://www.terraform.io/) - Cloud Creation. Since we want to reduce our mouse clicks, we're going to create instances automatically. \n\n* [Ansible](https://www.ansible.com/) - Instance configuration. The only thing we will need is ssh key.\n\n* [Docker](https://www.docker.com/) - App packaging and running. There's lots of different languages and frameworks around. Why don't we make an abstraction and intoroduce docker in our infrastructure ? It's not that hard.\n\n### <a href=\"#terraform\" name=\"terrafom\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Terraform\n\nWe will use Terraform to create instances in the cloud. I will use [digitalocean.com](https://digitalocean.com), it's less mature cloud provider, than [amazon.com](https://www.amazon.com), but I love it, and want to deploy my apps there. Feel free to try EC2 for yourself.\n\n\n### <a href=\"#preparation\" name=\"preparations\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Preparations\n\nHere're the steps you need to do before creating cloud instances:\n\n*  Create digitalocean access token. Go to [https://cloud.digitalocean.com/settings/api/tokens](https://cloud.digitalocean.com/settings/api/tokens) and name your token\n\n![](/content/images/2016/10/Screen-Shot-2016-10-26-at-7-04-02-PM-1.png)\n\n* Generate ssh keypair. Here's the instruction: [https://www.digitalocean.com/community/tutorials/how-to-use-ssh-keys-with-digitalocean-droplets](https://www.digitalocean.com/community/tutorials/how-to-use-ssh-keys-with-digitalocean-droplets)\nYes, I'm sending the link to this instructions, but if you don't know what is ssh and how to work with it - please stop at this step.\n\n* Add this ssh key to digitalocean, here's the link - [https://cloud.digitalocean.com/settings/security](https://cloud.digitalocean.com/settings/security)\n\n* Create folder somewhere. You will store all configs/scripts there. Name it something like `terraform-ansible-docker`\n\n* Install terraform. Since I'm an OSX user, for me it's easy to install it - `brew install terraform`\n\n* Install ansible - `brew install ansible`\n\n\n### <a href=\"#createtmpfiles\" name=\"createtmpfiles\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Create template file\nNow, as you did all the required steps, you can start configuring your bootstrap script. Enter your folder, and create another folder, called *dev*. Why *dev* ? Actually, you should be ready to have different environments: one for *dev*, another for *staging*, and, finally, *prod*. Furthermore, according to [ansible best practices](http://docs.ansible.com/ansible/playbooks_best_practices.html), my suggestion is one of the options, that ansible recommends. Ok then, enter *dev* folder and create a file called *variables.tf*\n\n```\nvariable \"token\" {\n  description = \"Digital Ocean Access Token\"\n  default = \"your digital ocean token\"\n}\n\nvariable \"ssh_fingerprint\" {\n  description = \"ssh fingerprint\"\n  default=\"Your ssh fingerprint. Get it via command line, or get it here - https://cloud.digitalocean.com/settings/security\"\n}\n\nvariable \"pub_key\" {\n\tdescription = \"ssh public key\"\n\tdefault = \"Your ssh public key. Copy it as it is.\"\n}\n\nvariable \"pvt_key\" {\n  description = \"ssh private key\"\n  default = \"Your private key. Copy it as it is.\"\n}\n\nvariable \"region\" {\n  description = \"Digital Ocean region\"\n  default = \"ams2\"\n}\n\n```\n\nLater on, you can remove default values, and pass everything from command line, but for now, let's keep it simple, to avoid any pitfalls.\n\n\n### <a href=\"#createmainterfile\" name=\"createmainterfile\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Create your main terraform file\n\nNow, here's the final step in configuring terraform - specifying what machines do you want. Lets name it as **main.tf**:\n\n```\nprovider \"digitalocean\" {\n  token = \"${var.token}\"\n}\n\nresource \"digitalocean_droplet\" \"app-rest-api\" {\n  count = 1\n  image = \"ubuntu-14-04-x64\"\n  name = \"app-rest-api\"\n  region = \"${var.region}\"\n  size = \"1gb\"\n  ssh_keys = [\"${var.ssh_fingerprint}\"]\n}\n\n```\n\nYou should have following folder structure:\n\n```\n.\n dev\n     main.tf\n     variables.tf\n\n1 directory, 2 files\n```\n\nThen, enter dev folder, and run\n\n```\nterraform plan\n```\n\nThis command will plan how many instances should be created, and will output it. I will show some tricks later about your instances modification. You should get similar output:\n\n```\nRefreshing Terraform state in-memory prior to plan...\nThe refreshed state will be used to calculate this plan, but\nwill not be persisted to local or remote state storage.\n\n\nThe Terraform execution plan has been generated and is shown below.\nResources are shown in alphabetical order for quick scanning. Green resources\nwill be created (or destroyed and then created if an existing resource\nexists), yellow resources are being changed in-place, and red resources\nwill be destroyed. Cyan entries are data sources to be read.\n\nNote: You didn't specify an \"-out\" parameter to save this plan, so when\n\"apply\" is called, Terraform can't guarantee this is what will execute.\n\n+ digitalocean_droplet.app-rest-api\n    image:                \"ubuntu-14-04-x64\"\n    ipv4_address:         \"<computed>\"\n    ipv4_address_private: \"<computed>\"\n    ipv6_address:         \"<computed>\"\n    ipv6_address_private: \"<computed>\"\n    locked:               \"<computed>\"\n    name:                 \"app-rest-api\"\n    region:               \"ams2\"\n    size:                 \"1gb\"\n    ssh_keys.#:           \"1\"\n    ssh_keys.0:           \"${ssh_fingerprint}\"\n    status:               \"<computed>\"\n\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n```\n\nIf you're okay with this configuration, you can start creating instances.\n\n```\nterraform apply\n```\n\n```\ndigitalocean_droplet.app-rest-api: Creating...\n  image:                \"\" => \"ubuntu-14-04-x64\"\n  ipv4_address:         \"\" => \"<computed>\"\n  ipv4_address_private: \"\" => \"<computed>\"\n  ipv6_address:         \"\" => \"<computed>\"\n  ipv6_address_private: \"\" => \"<computed>\"\n  locked:               \"\" => \"<computed>\"\n  name:                 \"\" => \"app-rest-api\"\n  region:               \"\" => \"ams2\"\n  size:                 \"\" => \"1gb\"\n  ssh_keys.#:           \"\" => \"1\"\n  ssh_keys.0:           \"\" => \"${ssh_fingerprint}\"\n  status:               \"\" => \"<computed>\"\ndigitalocean_droplet.app-rest-api: Still creating... (10s elapsed)\ndigitalocean_droplet.app-rest-api: Still creating... (20s elapsed)\ndigitalocean_droplet.app-rest-api: Still creating... (30s elapsed)\ndigitalocean_droplet.app-rest-api: Creation complete\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n\nThe state of your infrastructure has been saved to the path\nbelow. This state is required to modify and destroy your\ninfrastructure, so keep it safe. To inspect the complete state\nuse the `terraform show` command.\n\nState path: terraform.tfstate\n```\n\nNow it's ready! You can check your digitalocean instances page and make sure there's some instances with **app-rest-api** name.\n\nYou should notice another file in your dev folder, **terraform.tfstate**:\n```\n.\n main.tf\n terraform.tfstate\n variables.tf\n\n0 directories, 3 files\n```\n\nThis file contains all the necessary information about your instances. Feel free to `cat terraform.tfstate` it, and see what it contains.\n\nNow, change your instance from 1gb to 2gb:\n```\n...\nsize = \"2gb\"\n...\n```\n\nAnd run `terraform plan` again:\n\n```\n...\n\n~ digitalocean_droplet.app-rest-api\n    size: \"1gb\" => \"2gb\"\n\n\nPlan: 0 to add, 1 to change, 0 to destroy.\n```\n\nYou see it ? We increased RAM of our machine, and terraform found out, that we need to change this machine.\n\nRun `terraform apply` again and make sure your digitalocean droplet is resized.\n\nThis is the first chapter, which explains how to work with terraform. In the [next part](https://ivanursul.com/better-application-deployment-with-digitalocean-terraform-ansible-and-docker-dns-records/) I will show how to set dns records for your instances\n\n### <a href=\"#links\" name=\"links\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Links\n\n* [Terraform documentation](https://ivanursul.com/better-application-deployment-with-digitalocean-terraform-ansible-and-docker-dns-records/)\n\n* [terraform.io](https://www.terraform.io/)\n\nPS - this blog post is now a part of [Cloud Application Deployment WIKI](http://electric-cloud.com/wiki/display/releasemanagement/Cloud+Application+Deployment)","mobiledoc":null,"html":"<h3 id=\"ahrefargumentationnameargumentationiclassfafalinkanchorariahiddentrueiaargumentation\"><a href=\"#argumentation\" name=\"argumentation\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Argumentation</h3>\n\n<p>It's not a rare examples, when people need to deploy their applications somewhere in the cloud, without having mature infrastructure, and just for testing purpose. Let's say, you're writing a backend part for mobile application, and your android developer needs to have backend right and now. Your actions ? Of course, create EC2/Droplet instance, <strong>scp</strong> all binaries, connect via <strong>ssh</strong>, run them. You can even write some bash script, which will do all the work. And what should you do, if some day you will decide to go live ? The same thing, you will order some instances in your favorite cloud, configure load balancer, add 1-5 instances and start running your application. But how to scale ? What, if you will decide to add couple servers, and balance them ? By the time you will need to do that, you will be the author of many scripts, which potentially have bugs. You should have something better in your arsenal. I spend couple of weeks to find an instrument, which will fit my needs, and want to share it here, in my blog. It's not claims to be the best solution, I'm sure, there's plenty of them, but by following series I'll explain what I've achieved.</p>\n\n<h3 id=\"ahrefwhomnamewhomiclassfafalinkanchorariahiddentrueiawhomthisarticleisfor\"><a href=\"#whom\" name=\"whom\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Whom this article is for ?</h3>\n\n<p>This is for software engineers, who wants to improve their devops processes. I don't expect, that true devops will find something useful here, because they should already have a good bundle of instruments.</p>\n\n<h3 id=\"ahrefgenearlideanamegeneralideaiclassfafalinkanchorariahiddentrueiageneralidea\"><a href=\"#genearlidea\" name=\"generalidea\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> General idea</h3>\n\n<p>You will use three projects:</p>\n\n<ul>\n<li><p><a href=\"https://www.terraform.io/\">Terraform</a> - Cloud Creation. Since we want to reduce our mouse clicks, we're going to create instances automatically. </p></li>\n<li><p><a href=\"https://www.ansible.com/\">Ansible</a> - Instance configuration. The only thing we will need is ssh key.</p></li>\n<li><p><a href=\"https://www.docker.com/\">Docker</a> - App packaging and running. There's lots of different languages and frameworks around. Why don't we make an abstraction and intoroduce docker in our infrastructure ? It's not that hard.</p></li>\n</ul>\n\n<h3 id=\"ahrefterraformnameterrafomiclassfafalinkanchorariahiddentrueiaterraform\"><a href=\"#terraform\" name=\"terrafom\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Terraform</h3>\n\n<p>We will use Terraform to create instances in the cloud. I will use <a href=\"https://digitalocean.com\">digitalocean.com</a>, it's less mature cloud provider, than <a href=\"https://www.amazon.com\">amazon.com</a>, but I love it, and want to deploy my apps there. Feel free to try EC2 for yourself.</p>\n\n<h3 id=\"ahrefpreparationnamepreparationsiclassfafalinkanchorariahiddentrueiapreparations\"><a href=\"#preparation\" name=\"preparations\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Preparations</h3>\n\n<p>Here're the steps you need to do before creating cloud instances:</p>\n\n<ul>\n<li>Create digitalocean access token. Go to <a href=\"https://cloud.digitalocean.com/settings/api/tokens\">https://cloud.digitalocean.com/settings/api/tokens</a> and name your token</li>\n</ul>\n\n<p><img src=\"/content/images/2016/10/Screen-Shot-2016-10-26-at-7-04-02-PM-1.png\" alt=\"\" /></p>\n\n<ul>\n<li><p>Generate ssh keypair. Here's the instruction: <a href=\"https://www.digitalocean.com/community/tutorials/how-to-use-ssh-keys-with-digitalocean-droplets\">https://www.digitalocean.com/community/tutorials/how-to-use-ssh-keys-with-digitalocean-droplets</a>\nYes, I'm sending the link to this instructions, but if you don't know what is ssh and how to work with it - please stop at this step.</p></li>\n<li><p>Add this ssh key to digitalocean, here's the link - <a href=\"https://cloud.digitalocean.com/settings/security\">https://cloud.digitalocean.com/settings/security</a></p></li>\n<li><p>Create folder somewhere. You will store all configs/scripts there. Name it something like <code>terraform-ansible-docker</code></p></li>\n<li><p>Install terraform. Since I'm an OSX user, for me it's easy to install it - <code>brew install terraform</code></p></li>\n<li><p>Install ansible - <code>brew install ansible</code></p></li>\n</ul>\n\n<h3 id=\"ahrefcreatetmpfilesnamecreatetmpfilesiclassfafalinkanchorariahiddentrueiacreatetemplatefile\"><a href=\"#createtmpfiles\" name=\"createtmpfiles\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Create template file</h3>\n\n<p>Now, as you did all the required steps, you can start configuring your bootstrap script. Enter your folder, and create another folder, called <em>dev</em>. Why <em>dev</em> ? Actually, you should be ready to have different environments: one for <em>dev</em>, another for <em>staging</em>, and, finally, <em>prod</em>. Furthermore, according to <a href=\"http://docs.ansible.com/ansible/playbooks_best_practices.html\">ansible best practices</a>, my suggestion is one of the options, that ansible recommends. Ok then, enter <em>dev</em> folder and create a file called <em>variables.tf</em></p>\n\n<pre><code>variable \"token\" {  \n  description = \"Digital Ocean Access Token\"\n  default = \"your digital ocean token\"\n}\n\nvariable \"ssh_fingerprint\" {  \n  description = \"ssh fingerprint\"\n  default=\"Your ssh fingerprint. Get it via command line, or get it here - https://cloud.digitalocean.com/settings/security\"\n}\n\nvariable \"pub_key\" {  \n    description = \"ssh public key\"\n    default = \"Your ssh public key. Copy it as it is.\"\n}\n\nvariable \"pvt_key\" {  \n  description = \"ssh private key\"\n  default = \"Your private key. Copy it as it is.\"\n}\n\nvariable \"region\" {  \n  description = \"Digital Ocean region\"\n  default = \"ams2\"\n}\n</code></pre>\n\n<p>Later on, you can remove default values, and pass everything from command line, but for now, let's keep it simple, to avoid any pitfalls.</p>\n\n<h3 id=\"ahrefcreatemainterfilenamecreatemainterfileiclassfafalinkanchorariahiddentrueiacreateyourmainterraformfile\"><a href=\"#createmainterfile\" name=\"createmainterfile\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Create your main terraform file</h3>\n\n<p>Now, here's the final step in configuring terraform - specifying what machines do you want. Lets name it as <strong>main.tf</strong>:</p>\n\n<pre><code>provider \"digitalocean\" {  \n  token = \"${var.token}\"\n}\n\nresource \"digitalocean_droplet\" \"app-rest-api\" {  \n  count = 1\n  image = \"ubuntu-14-04-x64\"\n  name = \"app-rest-api\"\n  region = \"${var.region}\"\n  size = \"1gb\"\n  ssh_keys = [\"${var.ssh_fingerprint}\"]\n}\n</code></pre>\n\n<p>You should have following folder structure:</p>\n\n<pre><code>.\n dev\n     main.tf\n     variables.tf\n\n1 directory, 2 files  \n</code></pre>\n\n<p>Then, enter dev folder, and run</p>\n\n<pre><code>terraform plan  \n</code></pre>\n\n<p>This command will plan how many instances should be created, and will output it. I will show some tricks later about your instances modification. You should get similar output:</p>\n\n<pre><code>Refreshing Terraform state in-memory prior to plan...  \nThe refreshed state will be used to calculate this plan, but  \nwill not be persisted to local or remote state storage.\n\n\nThe Terraform execution plan has been generated and is shown below.  \nResources are shown in alphabetical order for quick scanning. Green resources  \nwill be created (or destroyed and then created if an existing resource  \nexists), yellow resources are being changed in-place, and red resources  \nwill be destroyed. Cyan entries are data sources to be read.\n\nNote: You didn't specify an \"-out\" parameter to save this plan, so when  \n\"apply\" is called, Terraform can't guarantee this is what will execute.\n\n+ digitalocean_droplet.app-rest-api\n    image:                \"ubuntu-14-04-x64\"\n    ipv4_address:         \"&lt;computed&gt;\"\n    ipv4_address_private: \"&lt;computed&gt;\"\n    ipv6_address:         \"&lt;computed&gt;\"\n    ipv6_address_private: \"&lt;computed&gt;\"\n    locked:               \"&lt;computed&gt;\"\n    name:                 \"app-rest-api\"\n    region:               \"ams2\"\n    size:                 \"1gb\"\n    ssh_keys.#:           \"1\"\n    ssh_keys.0:           \"${ssh_fingerprint}\"\n    status:               \"&lt;computed&gt;\"\n\n\nPlan: 1 to add, 0 to change, 0 to destroy.  \n</code></pre>\n\n<p>If you're okay with this configuration, you can start creating instances.</p>\n\n<pre><code>terraform apply  \n</code></pre>\n\n<pre><code>digitalocean_droplet.app-rest-api: Creating...  \n  image:                \"\" =&gt; \"ubuntu-14-04-x64\"\n  ipv4_address:         \"\" =&gt; \"&lt;computed&gt;\"\n  ipv4_address_private: \"\" =&gt; \"&lt;computed&gt;\"\n  ipv6_address:         \"\" =&gt; \"&lt;computed&gt;\"\n  ipv6_address_private: \"\" =&gt; \"&lt;computed&gt;\"\n  locked:               \"\" =&gt; \"&lt;computed&gt;\"\n  name:                 \"\" =&gt; \"app-rest-api\"\n  region:               \"\" =&gt; \"ams2\"\n  size:                 \"\" =&gt; \"1gb\"\n  ssh_keys.#:           \"\" =&gt; \"1\"\n  ssh_keys.0:           \"\" =&gt; \"${ssh_fingerprint}\"\n  status:               \"\" =&gt; \"&lt;computed&gt;\"\ndigitalocean_droplet.app-rest-api: Still creating... (10s elapsed)  \ndigitalocean_droplet.app-rest-api: Still creating... (20s elapsed)  \ndigitalocean_droplet.app-rest-api: Still creating... (30s elapsed)  \ndigitalocean_droplet.app-rest-api: Creation complete\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n\nThe state of your infrastructure has been saved to the path  \nbelow. This state is required to modify and destroy your  \ninfrastructure, so keep it safe. To inspect the complete state  \nuse the `terraform show` command.\n\nState path: terraform.tfstate  \n</code></pre>\n\n<p>Now it's ready! You can check your digitalocean instances page and make sure there's some instances with <strong>app-rest-api</strong> name.</p>\n\n<p>You should notice another file in your dev folder, <strong>terraform.tfstate</strong>:  </p>\n\n<pre><code>.\n main.tf\n terraform.tfstate\n variables.tf\n\n0 directories, 3 files  \n</code></pre>\n\n<p>This file contains all the necessary information about your instances. Feel free to <code>cat terraform.tfstate</code> it, and see what it contains.</p>\n\n<p>Now, change your instance from 1gb to 2gb:  </p>\n\n<pre><code>...\nsize = \"2gb\"  \n...\n</code></pre>\n\n<p>And run <code>terraform plan</code> again:</p>\n\n<pre><code>...\n\n~ digitalocean_droplet.app-rest-api\n    size: \"1gb\" =&gt; \"2gb\"\n\n\nPlan: 0 to add, 1 to change, 0 to destroy.  \n</code></pre>\n\n<p>You see it ? We increased RAM of our machine, and terraform found out, that we need to change this machine.</p>\n\n<p>Run <code>terraform apply</code> again and make sure your digitalocean droplet is resized.</p>\n\n<p>This is the first chapter, which explains how to work with terraform. In the <a href=\"https://ivanursul.com/better-application-deployment-with-digitalocean-terraform-ansible-and-docker-dns-records/\">next part</a> I will show how to set dns records for your instances</p>\n\n<h3 id=\"ahreflinksnamelinksiclassfafalinkanchorariahiddentrueialinks\"><a href=\"#links\" name=\"links\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Links</h3>\n\n<ul>\n<li><p><a href=\"https://ivanursul.com/better-application-deployment-with-digitalocean-terraform-ansible-and-docker-dns-records/\">Terraform documentation</a></p></li>\n<li><p><a href=\"https://www.terraform.io/\">terraform.io</a></p></li>\n</ul>\n\n<p>PS - this blog post is now a part of <a href=\"http://electric-cloud.com/wiki/display/releasemanagement/Cloud+Application+Deployment\">Cloud Application Deployment WIKI</a></p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-10-26T15:39:46.000Z","created_by":1,"updated_at":"2016-12-15T21:34:17.000Z","updated_by":1,"published_at":"2016-10-27T12:03:50.000Z","published_by":1},{"id":52,"uuid":"d2245b99-f299-4eaf-ab9e-bf9d286ccd44","title":"Better application deployment with DigitalOcean, Terraform, Ansible and Docker. DNS Records","slug":"better-application-deployment-with-digitalocean-terraform-ansible-and-docker-dns-records","markdown":"As we are creating our instances on the fly, using Terraform declarative scripts, we can also need to create dns domains and records. Let's say, you have two environments: **dev** and **prod**, and you you want to have your backend application stored under **dev.api.myapp.com** and **prod.api.myapp.com**. Luckily, terraform allows to do such things \n\n### Required steps\n\n* Digitalocean account\n* Ready and bought domain somewhere\n* Already defined terraform instances. See article from [previous](https://ivanursul.com/better-application-deployment-with-terraform-ansible-and-docker-part-1/) part.\n\n### <a href=\"#digitaloceandomain\" name=\"digitaloceandomain\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Digital Ocean Domain\n\nAdd additional declaration to your terraform file:\n\n\n```\nresource \"digitalocean_domain\" \"default\" {\n   name = \"dev.api.myapp.com\"\n   ip_address = \"${digitalocean_droplet.app-rest-api.ipv4_address}\"\n}\n```\n\n### <a href=\"#digitaloceanrecord\" name=\"digitaloceanrecord\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a>  DigitalOcean Record\n\nIf you want your app to be accessible through **dev.api.myapp.com**, without **www**m you should add this lines:\n\n```\nresource \"digitalocean_record\" \"CNAME-www\" {\n  domain = \"${digitalocean_domain.default.name}\"\n  type = \"CNAME\"\n  name = \"www\"\n  value = \"@\"\n}\n```\n\n### <a href=\"#finalcheck\" name=\"finalcheck\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Final Check\n\nYou should see your domain name in the list of networking domains here - [https://cloud.digitalocean.com/networking/domains](https://cloud.digitalocean.com/networking/domains)\n\nOn the [next part](https://ivanursul.com/better-application-deployment-with-digitalocean-terraform-ansible-and-docker-connecting-terraform-with-ansible) we will see how to start working with cloud instances using ansible, and how to connect ansible with terraform.\n\n\n\n","mobiledoc":null,"html":"<p>As we are creating our instances on the fly, using Terraform declarative scripts, we can also need to create dns domains and records. Let's say, you have two environments: <strong>dev</strong> and <strong>prod</strong>, and you you want to have your backend application stored under <strong>dev.api.myapp.com</strong> and <strong>prod.api.myapp.com</strong>. Luckily, terraform allows to do such things </p>\n\n<h3 id=\"requiredsteps\">Required steps</h3>\n\n<ul>\n<li>Digitalocean account</li>\n<li>Ready and bought domain somewhere</li>\n<li>Already defined terraform instances. See article from <a href=\"https://ivanursul.com/better-application-deployment-with-terraform-ansible-and-docker-part-1/\">previous</a> part.</li>\n</ul>\n\n<h3 id=\"ahrefdigitaloceandomainnamedigitaloceandomainiclassfafalinkanchorariahiddentrueiadigitaloceandomain\"><a href=\"#digitaloceandomain\" name=\"digitaloceandomain\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Digital Ocean Domain</h3>\n\n<p>Add additional declaration to your terraform file:</p>\n\n<pre><code>resource \"digitalocean_domain\" \"default\" {  \n   name = \"dev.api.myapp.com\"\n   ip_address = \"${digitalocean_droplet.app-rest-api.ipv4_address}\"\n}\n</code></pre>\n\n<h3 id=\"ahrefdigitaloceanrecordnamedigitaloceanrecordiclassfafalinkanchorariahiddentrueiadigitaloceanrecord\"><a href=\"#digitaloceanrecord\" name=\"digitaloceanrecord\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a>  DigitalOcean Record</h3>\n\n<p>If you want your app to be accessible through <strong>dev.api.myapp.com</strong>, without <strong>www</strong>m you should add this lines:</p>\n\n<pre><code>resource \"digitalocean_record\" \"CNAME-www\" {  \n  domain = \"${digitalocean_domain.default.name}\"\n  type = \"CNAME\"\n  name = \"www\"\n  value = \"@\"\n}\n</code></pre>\n\n<h3 id=\"ahreffinalchecknamefinalcheckiclassfafalinkanchorariahiddentrueiafinalcheck\"><a href=\"#finalcheck\" name=\"finalcheck\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Final Check</h3>\n\n<p>You should see your domain name in the list of networking domains here - <a href=\"https://cloud.digitalocean.com/networking/domains\">https://cloud.digitalocean.com/networking/domains</a></p>\n\n<p>On the <a href=\"https://ivanursul.com/better-application-deployment-with-digitalocean-terraform-ansible-and-docker-connecting-terraform-with-ansible\">next part</a> we will see how to start working with cloud instances using ansible, and how to connect ansible with terraform.</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-10-27T12:02:58.000Z","created_by":1,"updated_at":"2016-12-01T09:47:10.000Z","updated_by":1,"published_at":"2016-10-27T18:57:59.000Z","published_by":1},{"id":53,"uuid":"d0a626ca-163e-4d2b-a5bf-b92e6d767028","title":"Better application deployment with DigitalOcean, Terraform,  Ansible and Docker. Connecting Terraform with Ansible.","slug":"better-application-deployment-with-digitalocean-terraform-ansible-and-docker-connecting-terraform-with-ansible","markdown":"Just creating instances in the cloud is an intermediate result. Yes, you know don't need to create them manually, but it's not a target. We need to configure them somehow, deploy logic, restart, etc. That's why this article is about describing how to work with Terraform instances - using [Ansible](https://www.ansible.com/) tool.\n\n### <a href=\"#whatdoyouneed\" name=\"whatdoyouneed\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> What do you need to do before starting \n\n* Create terraform instance - [here's](https://ivanursul.com/better-application-deployment-with-terraform-ansible-and-docker-part-1/) how\n* Install ansible: `brew install ansible`\n\n### <a href=\"#howansibleknows\" name=\"howansibleknows\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> How Ansible knows about Terraform \n\nLet's take a close look how Ansible will recognize terraform instances. Normally, this is being done by specifying instances in [inventory file](http://docs.ansible.com/ansible/intro_inventory.html) - the idea is to have a file, which contains all the IP addresses, organized by the group. Let's say, you have one load balancer and three instances of an application, then you need to have following inventory file:\n```\n[lb]\nlb.example.com\n\n[app]\none.example.com\ntwo.example.com\nthree.example.com\n```\n\nBecause we are creating instances on the fly, we don't have a predefined set of IP addresses, which we're going to use. That's why we should use [dynamic inventory](http://docs.ansible.com/ansible/intro_dynamic_inventory.html). As we're using Terraform, we need some tool, which knows how to read instances from **terraform.tfstate** file and represent them to ansible. Personally, I found it useful to use [terraform-inventory](https://github.com/adammck/terraform-inventory) script, which I found on GitHub.\n\nTo start working, you need to install it:\n\n```\nbrew install terraform-inventory\n```\n\nNow let's review the folder, where we are now:\n```\n.\n main.tf\n variables.tf\n\nOne directory, two files\n```\nYou should have already read and tried previous parts, and they are about working with Terraform:\n\n* [Better application deployment with DigitalOcean, Terraform, Ansible, and Docker. Creating primary terraform instances](https://ivanursul.com/better-application-deployment-with-terraform-ansible-and-docker-part-1/)\n* [https://ivanursul.com/better-application-deployment-with-digitalocean-terraform-ansible-and-docker-dns-records/](https://ivanursul.com/better-application-deployment-with-digitalocean-terraform-ansible-and-docker-dns-records/)\n\nAs we are at the root of our project, we can execute the following script to get some info about our instances, but this time, this info comes from Ansible:\n\n```\nTF_STATE=terraform.tfstate ansible app-rest-api -m setup --inventory-file=/usr/local/bin/terraform-inventory         \n```\n\nI didn't find a particular document describing this command, and this is just something that works for me.  This is just an informative command, and it does nothing. To do something, you need to write a [playbook](http://docs.ansible.com/ansible/playbooks.html).\n\nWe're going to create a simple web application, which we will deploy into our **app-rest-api** server.\n\n### <a href=\"#writingwebapp\" name=\"writingwebapp\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Writing web app \nI've wrote a [simple Spring Boot application](https://github.com/ivanursul/terraform-ansible-spring-boot-demo/tree/a5915f4a94f2f61f9f3c083745188a64a19a4ba5), which will work just fine for our example. It will run on `8080` port, and will have a single endpoint - `http://{ip}:8080/`\n\nAs you see, we will use Docker for containerisation. I've already made a public Docker hub repository. To run it, you first need to pull it out from the repository.\n\n```\ndocker pull ivanursul/terraform-ansible-spring-boot-demo\n```\nThis command should download an image from Docker Hub. It'll take some time to do it, and after then run following command:\n\n```\ndocker run -d \\\n-h terraform-ansible-spring-boot-demo \\\n-p 8080:8080 \\\n--restart always \\\nivanursul/terraform-ansible-spring-boot-demo\n```\n\nYou will get a unique container id, which you can use to get logs, stop container, etc:\n\n```\ndocker logs -f 12ef980b6cf1cd7962011fd0e7fd873fabc615a9cf7b874c47eedcca0b641d70 \n```\n\nMake sure container is started by finding following line\n```\n2016-11-18 11:01:06.115  INFO 5 --- [           main] o.i.terraform.ansible.DemoApplication    : Started DemoApplication in 5.49 seconds (JVM running for 6.148)\n```\n\nNext, you need to get the docker machine IP, so run this command: \n```\ndocker-machine ls\n```\n\nThis should list you all your local docker machines:\n\n```\nNAME              ACTIVE   DRIVER         STATE     URL                         SWARM   DOCKER    ERRORS\ndefault           *        virtualbox     Running   tcp://192.168.99.100:2376           v1.12.2   \n```\n\nThen just curl it\n\n```\ncurl http://192.168.99.100:8080/\nHello big world!%  \n```\n\nDon't forget to stop container:\n\n```\ndocker stop 12ef980b6cf1cd7962011fd0e7fd873fabc615a9cf7b874c47eedcca0b641d70\n```\n\nOk, so that's all the manual steps you need to do to understand how it should work: you do changes, push it to Docker hub, then pull docker image, and run it. Luckily, I did this job for you, and you only need to launch terraform and Ansible scripts. The name of Docker image is [terraform-ansible-spring-boot-demo](https://hub.docker.com/r/ivanursul/terraform-ansible-spring-boot-demo/)\n\n### <a href=\"#writinplaybook\" name=\"writingplaybook\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Writing ansible playbook\n\nThere are just two steps you need to do:\n\n* Install Docker to your machines. We'll use a [ready ansible playbook](https://github.com/angstwad/docker.ubuntu) for that. Is there a way to install it automatically? \n* Pull image from Docker Hub public repository\n* Run it using Docker Ansible command:\n \nFirst, install docker.ubuntu role\n```\nansible-galaxy install angstwad.docker_ubuntu\n```\n\nThen, create `playbook-install.yml` file with the following configuration:\n```\n---\n- name: install docker\n  hosts: app-rest-api\n  become: yes\n  become_method: sudo\n  roles:\n    - { role: angstwad.docker_ubuntu, pip_version_docker_py: 1.10.6 }\n\n``` \n\nThen, create `playbook-deploy.yml`:\n```\n---\n- hosts: app-rest-api\n  become: yes\n  become_method: sudo\n  tasks:\n    - name: Starting app-rest-api docker image\n      docker:\n        name: app-rest-api\n        image: ivanursul/terraform-ansible-spring-boot-demo\n        state: reloaded\n        pull: always\n        ports:\n          - \"8080:8080\"\n        expose:\n          - 8080\n\n```\n\n\nAnd finally, `playbook-bootstrap.yml`:\n\n```\n---\n- include: playbook-install.yml\n- include: playbook-deploy.yml\n\n```\n\nAll terraform configs are in previous chapters, or you can see them on github page - [terraform-ansible-spring-boot-demo](https://github.com/ivanursul/terraform-ansible-spring-boot-demo/tree/a5915f4a94f2f61f9f3c083745188a64a19a4ba5/infrastructure), just see [main.tf](https://github.com/ivanursul/terraform-ansible-spring-boot-demo/blob/a5915f4a94f2f61f9f3c083745188a64a19a4ba5/infrastructure/main.tf) and [variables.tf](https://github.com/ivanursul/terraform-ansible-spring-boot-demo/blob/a5915f4a94f2f61f9f3c083745188a64a19a4ba5/infrastructure/variables.tf)\n\n### <a href=\"#smoketest\" name=\"smoketest\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Smoke test\n\nYou should have your `${DIGITALOCEAN_ACCESS_TOKEN}` and `${SSH_FINGERPRINT}` variables set. Also, your ssh key should be located under `$HOME/.ssh/` folder and should have `id_rsa_do_token` name.\n\nFirst, let's create instances with terraform:\n\n```\nterraform apply \\  \n-var \"token=${DIGITALOCEAN_ACCESS_TOKEN}\" \\\n-var \"pub_key=$HOME/.ssh/id_rsa_do_token.pub\" \\\n-var \"pvt_key=$HOME/.ssh/id_rsa_do_token\" \\\n-var \"ssh_fingerprint=${SSH_FINGERPRINT}\"\n```\n\nThen, let's execute ansible `playbook-bootstrap.yml` script:\n\n```\nTF_STATE=terraform.tfstate ansible-playbook --inventory-file=/usr/local/bin/terraform-inventory playbook-bootstrap.yml\n```\n\nThen it should be possible to access your service via external ip. Check it out.\n\n### <a href=\"#results\" name=\"results\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Results\nThe connection of Terraform and Ansible tools gave us a good way of creating instances. Also, Docker brings a significant improvement on deploying your application. You should prepare five things to start: cloud access token(Amazon, Azure, DigitalOcean), ssh key, `*.tf` for Terraform, `playbook-*.yml` and `Dockerfile`. Personally, I find this set of tools to be a very powerful and useful approach to deploying and maintaining your applications. \n\n### <a href=\"#links\" name=\"links\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Links\n\n* [Better application deployment with DigitalOcean, Terraform, Ansible and Docker. Creating basic terraform instances. Part1](https://ivanursul.com/better-application-deployment-with-terraform-ansible-and-docker-part-1/)\n* [Better application deployment with DigitalOcean, Terraform, Ansible and Docker. DNS Records. Part2](https://ivanursul.com/better-application-deployment-with-digitalocean-terraform-ansible-and-docker-dns-records/)\n* [ terraform-ansible-spring-boot-demo github repo](https://github.com/ivanursul/terraform-ansible-spring-boot-demo/tree/a5915f4a94f2f61f9f3c083745188a64a19a4ba5)\n* [ansible.com](https://www.ansible.com/)\n* [terraform-inventory](https://github.com/adammck/terraform-inventory)","mobiledoc":null,"html":"<p>Just creating instances in the cloud is an intermediate result. Yes, you know don't need to create them manually, but it's not a target. We need to configure them somehow, deploy logic, restart, etc. That's why this article is about describing how to work with Terraform instances - using <a href=\"https://www.ansible.com/\">Ansible</a> tool.</p>\n\n<h3 id=\"ahrefwhatdoyouneednamewhatdoyouneediclassfafalinkanchorariahiddentrueiawhatdoyouneedtodobeforestarting\"><a href=\"#whatdoyouneed\" name=\"whatdoyouneed\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> What do you need to do before starting</h3>\n\n<ul>\n<li>Create terraform instance - <a href=\"https://ivanursul.com/better-application-deployment-with-terraform-ansible-and-docker-part-1/\">here's</a> how</li>\n<li>Install ansible: <code>brew install ansible</code></li>\n</ul>\n\n<h3 id=\"ahrefhowansibleknowsnamehowansibleknowsiclassfafalinkanchorariahiddentrueiahowansibleknowsaboutterraform\"><a href=\"#howansibleknows\" name=\"howansibleknows\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> How Ansible knows about Terraform</h3>\n\n<p>Let's take a close look how Ansible will recognize terraform instances. Normally, this is being done by specifying instances in <a href=\"http://docs.ansible.com/ansible/intro_inventory.html\">inventory file</a> - the idea is to have a file, which contains all the IP addresses, organized by the group. Let's say, you have one load balancer and three instances of an application, then you need to have following inventory file:  </p>\n\n<pre><code>[lb]\nlb.example.com\n\n[app]\none.example.com  \ntwo.example.com  \nthree.example.com  \n</code></pre>\n\n<p>Because we are creating instances on the fly, we don't have a predefined set of IP addresses, which we're going to use. That's why we should use <a href=\"http://docs.ansible.com/ansible/intro_dynamic_inventory.html\">dynamic inventory</a>. As we're using Terraform, we need some tool, which knows how to read instances from <strong>terraform.tfstate</strong> file and represent them to ansible. Personally, I found it useful to use <a href=\"https://github.com/adammck/terraform-inventory\">terraform-inventory</a> script, which I found on GitHub.</p>\n\n<p>To start working, you need to install it:</p>\n\n<pre><code>brew install terraform-inventory  \n</code></pre>\n\n<p>Now let's review the folder, where we are now:  </p>\n\n<pre><code>.\n main.tf\n variables.tf\n\nOne directory, two files  \n</code></pre>\n\n<p>You should have already read and tried previous parts, and they are about working with Terraform:</p>\n\n<ul>\n<li><a href=\"https://ivanursul.com/better-application-deployment-with-terraform-ansible-and-docker-part-1/\">Better application deployment with DigitalOcean, Terraform, Ansible, and Docker. Creating primary terraform instances</a></li>\n<li><a href=\"https://ivanursul.com/better-application-deployment-with-digitalocean-terraform-ansible-and-docker-dns-records/\">https://ivanursul.com/better-application-deployment-with-digitalocean-terraform-ansible-and-docker-dns-records/</a></li>\n</ul>\n\n<p>As we are at the root of our project, we can execute the following script to get some info about our instances, but this time, this info comes from Ansible:</p>\n\n<pre><code>TF_STATE=terraform.tfstate ansible app-rest-api -m setup --inventory-file=/usr/local/bin/terraform-inventory  \n</code></pre>\n\n<p>I didn't find a particular document describing this command, and this is just something that works for me.  This is just an informative command, and it does nothing. To do something, you need to write a <a href=\"http://docs.ansible.com/ansible/playbooks.html\">playbook</a>.</p>\n\n<p>We're going to create a simple web application, which we will deploy into our <strong>app-rest-api</strong> server.</p>\n\n<h3 id=\"ahrefwritingwebappnamewritingwebappiclassfafalinkanchorariahiddentrueiawritingwebapp\"><a href=\"#writingwebapp\" name=\"writingwebapp\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Writing web app</h3>\n\n<p>I've wrote a <a href=\"https://github.com/ivanursul/terraform-ansible-spring-boot-demo/tree/a5915f4a94f2f61f9f3c083745188a64a19a4ba5\">simple Spring Boot application</a>, which will work just fine for our example. It will run on <code>8080</code> port, and will have a single endpoint - <code>http://{ip}:8080/</code></p>\n\n<p>As you see, we will use Docker for containerisation. I've already made a public Docker hub repository. To run it, you first need to pull it out from the repository.</p>\n\n<pre><code>docker pull ivanursul/terraform-ansible-spring-boot-demo  \n</code></pre>\n\n<p>This command should download an image from Docker Hub. It'll take some time to do it, and after then run following command:</p>\n\n<pre><code>docker run -d \\  \n-h terraform-ansible-spring-boot-demo \\\n-p 8080:8080 \\\n--restart always \\\nivanursul/terraform-ansible-spring-boot-demo  \n</code></pre>\n\n<p>You will get a unique container id, which you can use to get logs, stop container, etc:</p>\n\n<pre><code>docker logs -f 12ef980b6cf1cd7962011fd0e7fd873fabc615a9cf7b874c47eedcca0b641d70  \n</code></pre>\n\n<p>Make sure container is started by finding following line  </p>\n\n<pre><code>2016-11-18 11:01:06.115  INFO 5 --- [           main] o.i.terraform.ansible.DemoApplication    : Started DemoApplication in 5.49 seconds (JVM running for 6.148)  \n</code></pre>\n\n<p>Next, you need to get the docker machine IP, so run this command:  </p>\n\n<pre><code>docker-machine ls  \n</code></pre>\n\n<p>This should list you all your local docker machines:</p>\n\n<pre><code>NAME              ACTIVE   DRIVER         STATE     URL                         SWARM   DOCKER    ERRORS  \ndefault           *        virtualbox     Running   tcp://192.168.99.100:2376           v1.12.2  \n</code></pre>\n\n<p>Then just curl it</p>\n\n<pre><code>curl http://192.168.99.100:8080/  \nHello big world!%  \n</code></pre>\n\n<p>Don't forget to stop container:</p>\n\n<pre><code>docker stop 12ef980b6cf1cd7962011fd0e7fd873fabc615a9cf7b874c47eedcca0b641d70  \n</code></pre>\n\n<p>Ok, so that's all the manual steps you need to do to understand how it should work: you do changes, push it to Docker hub, then pull docker image, and run it. Luckily, I did this job for you, and you only need to launch terraform and Ansible scripts. The name of Docker image is <a href=\"https://hub.docker.com/r/ivanursul/terraform-ansible-spring-boot-demo/\">terraform-ansible-spring-boot-demo</a></p>\n\n<h3 id=\"ahrefwritinplaybooknamewritingplaybookiclassfafalinkanchorariahiddentrueiawritingansibleplaybook\"><a href=\"#writinplaybook\" name=\"writingplaybook\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Writing ansible playbook</h3>\n\n<p>There are just two steps you need to do:</p>\n\n<ul>\n<li>Install Docker to your machines. We'll use a <a href=\"https://github.com/angstwad/docker.ubuntu\">ready ansible playbook</a> for that. Is there a way to install it automatically? </li>\n<li>Pull image from Docker Hub public repository</li>\n<li>Run it using Docker Ansible command:</li>\n</ul>\n\n<p>First, install docker.ubuntu role  </p>\n\n<pre><code>ansible-galaxy install angstwad.docker_ubuntu  \n</code></pre>\n\n<p>Then, create <code>playbook-install.yml</code> file with the following configuration:  </p>\n\n<pre><code>---\n- name: install docker\n  hosts: app-rest-api\n  become: yes\n  become_method: sudo\n  roles:\n    - { role: angstwad.docker_ubuntu, pip_version_docker_py: 1.10.6 }\n</code></pre>\n\n<p>Then, create <code>playbook-deploy.yml</code>:  </p>\n\n<pre><code>---\n- hosts: app-rest-api\n  become: yes\n  become_method: sudo\n  tasks:\n    - name: Starting app-rest-api docker image\n      docker:\n        name: app-rest-api\n        image: ivanursul/terraform-ansible-spring-boot-demo\n        state: reloaded\n        pull: always\n        ports:\n          - \"8080:8080\"\n        expose:\n          - 8080\n</code></pre>\n\n<p>And finally, <code>playbook-bootstrap.yml</code>:</p>\n\n<pre><code>---\n- include: playbook-install.yml\n- include: playbook-deploy.yml\n</code></pre>\n\n<p>All terraform configs are in previous chapters, or you can see them on github page - <a href=\"https://github.com/ivanursul/terraform-ansible-spring-boot-demo/tree/a5915f4a94f2f61f9f3c083745188a64a19a4ba5/infrastructure\">terraform-ansible-spring-boot-demo</a>, just see <a href=\"https://github.com/ivanursul/terraform-ansible-spring-boot-demo/blob/a5915f4a94f2f61f9f3c083745188a64a19a4ba5/infrastructure/main.tf\">main.tf</a> and <a href=\"https://github.com/ivanursul/terraform-ansible-spring-boot-demo/blob/a5915f4a94f2f61f9f3c083745188a64a19a4ba5/infrastructure/variables.tf\">variables.tf</a></p>\n\n<h3 id=\"ahrefsmoketestnamesmoketesticlassfafalinkanchorariahiddentrueiasmoketest\"><a href=\"#smoketest\" name=\"smoketest\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Smoke test</h3>\n\n<p>You should have your <code>${DIGITALOCEAN_ACCESS_TOKEN}</code> and <code>${SSH_FINGERPRINT}</code> variables set. Also, your ssh key should be located under <code>$HOME/.ssh/</code> folder and should have <code>id_rsa_do_token</code> name.</p>\n\n<p>First, let's create instances with terraform:</p>\n\n<pre><code>terraform apply \\  \n-var \"token=${DIGITALOCEAN_ACCESS_TOKEN}\" \\\n-var \"pub_key=$HOME/.ssh/id_rsa_do_token.pub\" \\\n-var \"pvt_key=$HOME/.ssh/id_rsa_do_token\" \\\n-var \"ssh_fingerprint=${SSH_FINGERPRINT}\"\n</code></pre>\n\n<p>Then, let's execute ansible <code>playbook-bootstrap.yml</code> script:</p>\n\n<pre><code>TF_STATE=terraform.tfstate ansible-playbook --inventory-file=/usr/local/bin/terraform-inventory playbook-bootstrap.yml  \n</code></pre>\n\n<p>Then it should be possible to access your service via external ip. Check it out.</p>\n\n<h3 id=\"ahrefresultsnameresultsiclassfafalinkanchorariahiddentrueiaresults\"><a href=\"#results\" name=\"results\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Results</h3>\n\n<p>The connection of Terraform and Ansible tools gave us a good way of creating instances. Also, Docker brings a significant improvement on deploying your application. You should prepare five things to start: cloud access token(Amazon, Azure, DigitalOcean), ssh key, <code>*.tf</code> for Terraform, <code>playbook-*.yml</code> and <code>Dockerfile</code>. Personally, I find this set of tools to be a very powerful and useful approach to deploying and maintaining your applications. </p>\n\n<h3 id=\"ahreflinksnamelinksiclassfafalinkanchorariahiddentrueialinks\"><a href=\"#links\" name=\"links\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Links</h3>\n\n<ul>\n<li><a href=\"https://ivanursul.com/better-application-deployment-with-terraform-ansible-and-docker-part-1/\">Better application deployment with DigitalOcean, Terraform, Ansible and Docker. Creating basic terraform instances. Part1</a></li>\n<li><a href=\"https://ivanursul.com/better-application-deployment-with-digitalocean-terraform-ansible-and-docker-dns-records/\">Better application deployment with DigitalOcean, Terraform, Ansible and Docker. DNS Records. Part2</a></li>\n<li><a href=\"https://github.com/ivanursul/terraform-ansible-spring-boot-demo/tree/a5915f4a94f2f61f9f3c083745188a64a19a4ba5\"> terraform-ansible-spring-boot-demo github repo</a></li>\n<li><a href=\"https://www.ansible.com/\">ansible.com</a></li>\n<li><a href=\"https://github.com/adammck/terraform-inventory\">terraform-inventory</a></li>\n</ul>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-10-28T13:01:32.000Z","created_by":1,"updated_at":"2016-12-08T10:12:53.000Z","updated_by":1,"published_at":"2016-11-28T16:10:34.000Z","published_by":1},{"id":54,"uuid":"8a4fb0aa-dbb1-48f5-ba31-d71322e64531","title":"watch - useful unix command I've never heard of","slug":"watch-useful-unix-command-ive-never-heard-of","markdown":"I use ImageMagick for resizing images in one of the projects, and I needed a command to monitor folder changes in a temporary folder. So I found [watch](https://linux.die.net/man/1/watch):\n\n```\nwatch -d -n 0.3 'ls -l | grep ivanursul'\n```\n\nThis command check every **0.3** seconds for current folder, and find all files, created by user **ivanursul**\n\n\n![](/content/images/2016/12/out.gif)\n\n### <a href=\"#links\" name=\"links\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Links\n\n* [Linux Watch](https://linux.die.net/man/1/watch)","mobiledoc":null,"html":"<p>I use ImageMagick for resizing images in one of the projects, and I needed a command to monitor folder changes in a temporary folder. So I found <a href=\"https://linux.die.net/man/1/watch\">watch</a>:</p>\n\n<pre><code>watch -d -n 0.3 'ls -l | grep ivanursul'  \n</code></pre>\n\n<p>This command check every <strong>0.3</strong> seconds for current folder, and find all files, created by user <strong>ivanursul</strong></p>\n\n<p><img src=\"/content/images/2016/12/out.gif\" alt=\"\" /></p>\n\n<h3 id=\"ahreflinksnamelinksiclassfafalinkanchorariahiddentrueialinks\"><a href=\"#links\" name=\"links\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Links</h3>\n\n<ul>\n<li><a href=\"https://linux.die.net/man/1/watch\">Linux Watch</a></li>\n</ul>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-12-01T10:55:04.000Z","created_by":1,"updated_at":"2016-12-08T09:40:07.000Z","updated_by":1,"published_at":"2016-12-01T11:11:37.000Z","published_by":1},{"id":55,"uuid":"f7ab9f80-31a6-4ef3-b86a-9e33b89afcb5","title":"Better application deployment. Monitoring your application with Graphite/Grafana","slug":"better-application-deployment-monitoring-your-application-with-graphite-grafana","markdown":"### <a href=\"#intro\" name=\"intro\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Intro\n\nIn the [previous part](https://ivanursul.com/better-application-deployment-with-digitalocean-terraform-ansible-and-docker-connecting-terraform-with-ansible/) I explained how to use Terraform, Ansible, Docker and Spring Boot to deploy applications in the cloud. Today I'd like to introduce something, which will work as a monitoring tool, inside our infrastructure. If you follow my blog posts, you should remember a post about [Spring and Dropwizard Module](https://ivanursul.com/monitoring-your-spring-application-using-dropwizard-metrics-module/) - there I explained how you could get a meaningful metrics from your app. \n\nBut wait, why you should even do monitoring and can you skip this part? Well, when I first came to the project, when a wide variety of metrics was present in each of the microservices in the ecosystem, I had a feeling that this is something which I won't use in the future. I was right, and I didn't use them...until my first incident, on which I had to understand what's going on. I start looking for some explanations, and found that our service is sending many 500 statuses. Then I found out, that one dependant service, which we use to get some part of response, is broken, and problem is not on our side.\n\nFrom that period I introduced a couple of custom dashboards, and during incidents/crashes, I can answer most of the questions about what's going on by just opening my monitoring dashboards. Sometimes, I need to prove my assumption by reading logs, and this is a different story, and I'll cover it later. f you're reading this blog post and don't have a monitoring system on your project, now you have enough arguments on having some. There're many options on the market, and you're free to choose. \n\nFor this blog post I decided to use [Graphite](https://graphiteapp.org/)/[Grafana](http://grafana.org/) couple, which will be running on [DigitalOcean](https://www.digitalocean.com/) cloud, created by [Terraform](https://www.terraform.io/), and configured by [Ansible](https://www.ansible.com/).\n\n![](/content/images/2016/12/out1.gif?style=centerme)\n\n### <a href=\"#exposehttp\" name=\"exposehttp\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Turning on metrics in Spring\n\nI encourage you to read [Monitoring your Spring application using Dropwizard metrics module](https://ivanursul.com/monitoring-your-spring-application-using-dropwizard-metrics-module) article, there's a deep explanation on how to, in this article I'll cover the basics, without step-by-step explanation.\n\nCreate **MonitoringConfiguration** class:\n\n```\npackage org.ivanursul.terraform.ansible;\n\nimport com.codahale.metrics.JmxReporter;\nimport com.codahale.metrics.MetricFilter;\nimport com.codahale.metrics.MetricRegistry;\nimport com.codahale.metrics.graphite.Graphite;\nimport com.codahale.metrics.graphite.GraphiteReporter;\nimport com.codahale.metrics.graphite.GraphiteReporter.Builder;\nimport com.codahale.metrics.health.HealthCheckRegistry;\nimport com.codahale.metrics.jvm.GarbageCollectorMetricSet;\nimport com.codahale.metrics.jvm.MemoryUsageGaugeSet;\nimport com.codahale.metrics.jvm.ThreadStatesGaugeSet;\nimport com.codahale.metrics.servlets.AdminServlet;\nimport com.ryantenney.metrics.spring.config.annotation.EnableMetrics;\nimport com.ryantenney.metrics.spring.config.annotation.MetricsConfigurerAdapter;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.beans.factory.annotation.Value;\nimport org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;\nimport org.springframework.boot.context.embedded.ServletRegistrationBean;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\nimport javax.annotation.PostConstruct;\nimport java.net.InetSocketAddress;\nimport java.util.concurrent.TimeUnit;\n\n@Configuration\n@EnableMetrics\n@ConditionalOnProperty(name = \"metrics.enabled\", havingValue = \"true\")\npublic class MonitoringConfiguration extends MetricsConfigurerAdapter {\n\n    @Value(\"${graphite.host}\")\n    private String graphiteHost;\n\n    @Value(\"${graphite.port}\")\n    private int graphitePort;\n\n    @Value(\"${graphite.amount.of.time.between.polls}\")\n    private long graphiteAmountOfTimeBetweenPolls;\n\n    @Autowired\n    private MetricRegistry metricRegistry;\n\n    @Autowired\n    private HealthCheckRegistry healthCheckRegistry;\n\n    @Autowired\n    private Graphite graphite;\n\n    @PostConstruct\n    public void init() {\n        configureReporters(metricRegistry);\n    }\n\n    @Bean\n    public MetricsServletContextListener metricsServletContextListener(MetricRegistry metricRegistry, HealthCheckRegistry healthCheckRegistry) {\n        return new MetricsServletContextListener(metricRegistry, healthCheckRegistry);\n    }\n\n    @Bean\n    public ServletRegistrationBean servletRegistrationBean(){\n        return new ServletRegistrationBean(new AdminServlet(),\"/dropwizard/*\");\n    }\n\n    @Bean\n    public Graphite graphite() {\n        return new Graphite(\n                new InetSocketAddress(graphiteHost, graphitePort)\n        );\n    }\n\n    @Bean\n    @ConditionalOnProperty(name = { \"graphite.enabled\", \"metrics.enabled\"}, havingValue = \"true\")\n    public GraphiteReporter graphiteReporter(Graphite graphite) {\n        GraphiteReporter graphiteReporter = getGraphiteReporterBuilder(metricRegistry).build(graphite);\n        registerReporter(graphiteReporter);\n        graphiteReporter.start(graphiteAmountOfTimeBetweenPolls, TimeUnit.MILLISECONDS);\n\n        return graphiteReporter;\n    }\n\n    @Override\n    public void configureReporters(MetricRegistry metricRegistry) {\n        registerReporter(JmxReporter.forRegistry(metricRegistry).build()).start();\n    }\n\n    private Builder getGraphiteReporterBuilder(MetricRegistry metricRegistry) {\n        metricRegistry.register(\"gc\", new GarbageCollectorMetricSet());\n        metricRegistry.register(\"memory\", new MemoryUsageGaugeSet());\n        metricRegistry.register(\"threads\", new ThreadStatesGaugeSet());\n        return GraphiteReporter.forRegistry(metricRegistry)\n                .convertRatesTo(TimeUnit.SECONDS)\n                .convertDurationsTo(TimeUnit.MILLISECONDS)\n                .filter(MetricFilter.ALL);\n    }\n\n}\n```\n\nAnd this one, **MetricsServletContextListener**:\n\n```\npackage org.ivanursul.terraform.ansible;\n\nimport com.codahale.metrics.MetricRegistry;\nimport com.codahale.metrics.health.HealthCheckRegistry;\nimport com.codahale.metrics.servlets.HealthCheckServlet;\nimport com.codahale.metrics.servlets.MetricsServlet;\n\nimport javax.servlet.ServletContextEvent;\nimport javax.servlet.ServletContextListener;\n\npublic class MetricsServletContextListener implements ServletContextListener {\n\n    private MetricRegistry metricRegistry;\n    private HealthCheckRegistry healthCheckRegistry = new HealthCheckRegistry();\n\n    public MetricsServletContextListener(MetricRegistry metricRegistry, HealthCheckRegistry healthCheckRegistry) {\n        this.metricRegistry = metricRegistry;\n        this.healthCheckRegistry = healthCheckRegistry;\n    }\n\n    @Override\n    public void contextInitialized(ServletContextEvent servletContextEvent) {\n        servletContextEvent.getServletContext().setAttribute(HealthCheckServlet.HEALTH_CHECK_REGISTRY,healthCheckRegistry);\n        servletContextEvent.getServletContext().setAttribute(MetricsServlet.METRICS_REGISTRY, metricRegistry);\n    }\n\n    @Override\n    public void contextDestroyed(ServletContextEvent sce) {\n\n    }\n}\n```\n\nYou also need to update your **application.properties** file:\n\n```\nmetrics.enabled=true\ngraphite.enabled=false\ngraphite.host=localhost\ngraphite.port=2003\ngraphite.amount.of.time.between.polls=20000\n```\n\nHere we set **graphite.enabled** to false, since we don't want to corrupt things on localhost. When we set graphite/grafana connection, we will be able to specify what host do we have on the cloud, and then we will enable graphite.\n\nPS - You can, of course, try for yourself, and enable it on localhost, but before you need to install graphite. [Here's](https://gist.github.com/relaxdiego/7539911) a brief Github Gist on how to install it.\n\nJust setting proper metrics and exposing them to HTTP endpoint is not enough. In our case, we need to send this metric data to Graphite. That's why we need to create an instance of a class called [**GraphiteReporter**](http://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/graphite/GraphiteReporter.html) - everything you need to set is graphite host, port and polling pause.\n \n\n### <a href=\"#monitoringinstance\" name=\"monitoringinstance\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Additional cloud instance for Grafana\n\nThings are getting interesting, aren't they? We did all Java work, and now we need to send our metrics to Graphite and show them on Grafana. I decided to create a separate instance because in case of failures and system shutdowns I'll be able to open the dashboard and see, what's happening. And besides, the more general rule of thumb is that we should use separate machines for each unit in our architecture. Separation of concerns. Everything should be on its place.\n\n![](/content/images/2016/12/ScholarlySolidGermanshepherd.gif?style=centerme)\n\nSo, let's create terraform resource - an instruction for additional cloud instance. Please revise [previous](https://ivanursul.com/better-application-deployment-with-digitalocean-terraform-ansible-and-docker-connecting-terraform-with-ansible/) article for understanding things.\n\n```\n...\nresource \"digitalocean_droplet\" \"app-monitoring\" {\n  image = \"ubuntu-14-04-x64\"\n  name = \"app-monitoring\"\n  region = \"${var.region}\"\n  size = \"512mb\"\n  ssh_keys = [\"${var.ssh_fingerprint}\"]\n}\n```\nFull terraform file can be found on [github-ADDD-LINk].(...)\nCreating instance...\n\n```\nterraform apply \\  \n-var \"token=${DIGITALOCEAN_ACCESS_TOKEN}\" \\\n-var \"pub_key=$HOME/.ssh/id_rsa_do_token.pub\" \\\n-var \"pvt_key=$HOME/.ssh/id_rsa_do_token\" \\\n-var \"ssh_fingerprint=${SSH_FINGERPRINT}\"\n```\n\nOk, we've checked out the cloud, we see a new instance, there's nothing new comparing to previous parts, so we can start configuring it.\n\n### <a href=\"#playbook\" name=\"playbook\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Playbook for setting Graphite/Grafana\n\nFinally, we can start doing something entirely new - installing Graphite and Grafana on our instance.\n\nWe should install Graphite first, and then Grafana. Graphite should be configured together with Carbon. Carbon is one of three components within Graphite project.\n\n![](/content/images/2016/12/overview.png)\n\nThe idea is that we collect metrics from Java application, and then send them to carbon, which processes these metrics. Then there is a graphite-web app, which can render and show metrics, but user interface, which is present there, could be better. Additionally, there are no features, which helps us to understand graphs. That's why we use Grafana - for its excellent user interface and bunch of functions.\n\n![](/content/images/2016/12/diagram-1.svg)\n\nYou see a diagram with two databases. However, there'll be a single database, with tables from both Graphite and Grafana. If you need two databases, feel free to tune ansible script and create an additional database.\n\nAll scripts located on GitHub project, and you can observe them [here](https://github.com/ivanursul/terraform-ansible-spring-boot-demo/tree/2bc1b501fe1603d3ff9f3f157810fc28e0506882)\n\nSo, what's ansible doing on **app-monitoring** instance?\n\n* Installs all Graphite, including Carbon. Carbon runs on **2003** port and Graphite is on **8080**\n* Installs postgres database\n* Installs Grafana, on **8567** port. Credentials **admin:SecureAdminPass**\n* Deploys Spring Boot application with graphite host [param](https://github.com/ivanursul/terraform-ansible-spring-boot-demo/blob/2bc1b501fe1603d3ff9f3f157810fc28e0506882/infrastructure/playbook-deploy.yml#L19)\n\nSteps I did should be enough for Graphite/Grafana, we've configured Spring Boot o send metrics to Carbon, and added data source for Grafana, so we have monitoring dashboards, which works out of the box.\n\nRun following script \n```\nTF_STATE=terraform.tfstate ansible-playbook --inventory-file=/usr/local/bin/terraform-inventory playbook-bootstrap.yml\n``` \n\nPosting a little video which shows how to deploy and start everything from this article.\n\n<div style=\"text-align: center;\"><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/zHxHgZE1svU\" frameborder=\"0\" allowfullscreen></iframe></div>\n\n### <a href=\"#links\" name=\"links\"><i class=\"centerme fa fa-link anchor\" aria-hidden=\"true\"></i></a> Links\n\n* [Better application deployment with DigitalOcean, Terraform, Ansible, and Docker. Creating basic terraform instances](https://ivanursul.com/better-application-deployment-with-terraform-ansible-and-docker-part-1/)\n* [Better application deployment with DigitalOcean, Terraform, Ansible and Docker. DNS Records](https://ivanursul.com/better-application-deployment-with-digitalocean-terraform-ansible-and-docker-dns-records/)\n* [Better application deployment with DigitalOcean, Terraform, Ansible and Docker. Connecting Terraform with Ansible.](https://ivanursul.com/better-application-deployment-with-digitalocean-terraform-ansible-and-docker-connecting-terraform-with-ansible/)\n* [Monitoring your Spring application using Dropwizard metrics module](https://ivanursul.com/monitoring-your-spring-application-using-dropwizard-metrics-module/)\n* [Carbon project](https://github.com/graphite-project/carbon)","mobiledoc":null,"html":"<h3 id=\"ahrefintronameintroiclassfafalinkanchorariahiddentrueiaintro\"><a href=\"#intro\" name=\"intro\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Intro</h3>\n\n<p>In the <a href=\"https://ivanursul.com/better-application-deployment-with-digitalocean-terraform-ansible-and-docker-connecting-terraform-with-ansible/\">previous part</a> I explained how to use Terraform, Ansible, Docker and Spring Boot to deploy applications in the cloud. Today I'd like to introduce something, which will work as a monitoring tool, inside our infrastructure. If you follow my blog posts, you should remember a post about <a href=\"https://ivanursul.com/monitoring-your-spring-application-using-dropwizard-metrics-module/\">Spring and Dropwizard Module</a> - there I explained how you could get a meaningful metrics from your app. </p>\n\n<p>But wait, why you should even do monitoring and can you skip this part? Well, when I first came to the project, when a wide variety of metrics was present in each of the microservices in the ecosystem, I had a feeling that this is something which I won't use in the future. I was right, and I didn't use them...until my first incident, on which I had to understand what's going on. I start looking for some explanations, and found that our service is sending many 500 statuses. Then I found out, that one dependant service, which we use to get some part of response, is broken, and problem is not on our side.</p>\n\n<p>From that period I introduced a couple of custom dashboards, and during incidents/crashes, I can answer most of the questions about what's going on by just opening my monitoring dashboards. Sometimes, I need to prove my assumption by reading logs, and this is a different story, and I'll cover it later. f you're reading this blog post and don't have a monitoring system on your project, now you have enough arguments on having some. There're many options on the market, and you're free to choose. </p>\n\n<p>For this blog post I decided to use <a href=\"https://graphiteapp.org/\">Graphite</a>/<a href=\"http://grafana.org/\">Grafana</a> couple, which will be running on <a href=\"https://www.digitalocean.com/\">DigitalOcean</a> cloud, created by <a href=\"https://www.terraform.io/\">Terraform</a>, and configured by <a href=\"https://www.ansible.com/\">Ansible</a>.</p>\n\n<p><img src=\"/content/images/2016/12/out1.gif?style=centerme\" alt=\"\" /></p>\n\n<h3 id=\"ahrefexposehttpnameexposehttpiclassfafalinkanchorariahiddentrueiaturningonmetricsinspring\"><a href=\"#exposehttp\" name=\"exposehttp\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Turning on metrics in Spring</h3>\n\n<p>I encourage you to read <a href=\"https://ivanursul.com/monitoring-your-spring-application-using-dropwizard-metrics-module\">Monitoring your Spring application using Dropwizard metrics module</a> article, there's a deep explanation on how to, in this article I'll cover the basics, without step-by-step explanation.</p>\n\n<p>Create <strong>MonitoringConfiguration</strong> class:</p>\n\n<pre><code>package org.ivanursul.terraform.ansible;\n\nimport com.codahale.metrics.JmxReporter;  \nimport com.codahale.metrics.MetricFilter;  \nimport com.codahale.metrics.MetricRegistry;  \nimport com.codahale.metrics.graphite.Graphite;  \nimport com.codahale.metrics.graphite.GraphiteReporter;  \nimport com.codahale.metrics.graphite.GraphiteReporter.Builder;  \nimport com.codahale.metrics.health.HealthCheckRegistry;  \nimport com.codahale.metrics.jvm.GarbageCollectorMetricSet;  \nimport com.codahale.metrics.jvm.MemoryUsageGaugeSet;  \nimport com.codahale.metrics.jvm.ThreadStatesGaugeSet;  \nimport com.codahale.metrics.servlets.AdminServlet;  \nimport com.ryantenney.metrics.spring.config.annotation.EnableMetrics;  \nimport com.ryantenney.metrics.spring.config.annotation.MetricsConfigurerAdapter;  \nimport org.springframework.beans.factory.annotation.Autowired;  \nimport org.springframework.beans.factory.annotation.Value;  \nimport org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;  \nimport org.springframework.boot.context.embedded.ServletRegistrationBean;  \nimport org.springframework.context.annotation.Bean;  \nimport org.springframework.context.annotation.Configuration;\n\nimport javax.annotation.PostConstruct;  \nimport java.net.InetSocketAddress;  \nimport java.util.concurrent.TimeUnit;\n\n@Configuration\n@EnableMetrics\n@ConditionalOnProperty(name = \"metrics.enabled\", havingValue = \"true\")\npublic class MonitoringConfiguration extends MetricsConfigurerAdapter {\n\n    @Value(\"${graphite.host}\")\n    private String graphiteHost;\n\n    @Value(\"${graphite.port}\")\n    private int graphitePort;\n\n    @Value(\"${graphite.amount.of.time.between.polls}\")\n    private long graphiteAmountOfTimeBetweenPolls;\n\n    @Autowired\n    private MetricRegistry metricRegistry;\n\n    @Autowired\n    private HealthCheckRegistry healthCheckRegistry;\n\n    @Autowired\n    private Graphite graphite;\n\n    @PostConstruct\n    public void init() {\n        configureReporters(metricRegistry);\n    }\n\n    @Bean\n    public MetricsServletContextListener metricsServletContextListener(MetricRegistry metricRegistry, HealthCheckRegistry healthCheckRegistry) {\n        return new MetricsServletContextListener(metricRegistry, healthCheckRegistry);\n    }\n\n    @Bean\n    public ServletRegistrationBean servletRegistrationBean(){\n        return new ServletRegistrationBean(new AdminServlet(),\"/dropwizard/*\");\n    }\n\n    @Bean\n    public Graphite graphite() {\n        return new Graphite(\n                new InetSocketAddress(graphiteHost, graphitePort)\n        );\n    }\n\n    @Bean\n    @ConditionalOnProperty(name = { \"graphite.enabled\", \"metrics.enabled\"}, havingValue = \"true\")\n    public GraphiteReporter graphiteReporter(Graphite graphite) {\n        GraphiteReporter graphiteReporter = getGraphiteReporterBuilder(metricRegistry).build(graphite);\n        registerReporter(graphiteReporter);\n        graphiteReporter.start(graphiteAmountOfTimeBetweenPolls, TimeUnit.MILLISECONDS);\n\n        return graphiteReporter;\n    }\n\n    @Override\n    public void configureReporters(MetricRegistry metricRegistry) {\n        registerReporter(JmxReporter.forRegistry(metricRegistry).build()).start();\n    }\n\n    private Builder getGraphiteReporterBuilder(MetricRegistry metricRegistry) {\n        metricRegistry.register(\"gc\", new GarbageCollectorMetricSet());\n        metricRegistry.register(\"memory\", new MemoryUsageGaugeSet());\n        metricRegistry.register(\"threads\", new ThreadStatesGaugeSet());\n        return GraphiteReporter.forRegistry(metricRegistry)\n                .convertRatesTo(TimeUnit.SECONDS)\n                .convertDurationsTo(TimeUnit.MILLISECONDS)\n                .filter(MetricFilter.ALL);\n    }\n\n}\n</code></pre>\n\n<p>And this one, <strong>MetricsServletContextListener</strong>:</p>\n\n<pre><code>package org.ivanursul.terraform.ansible;\n\nimport com.codahale.metrics.MetricRegistry;  \nimport com.codahale.metrics.health.HealthCheckRegistry;  \nimport com.codahale.metrics.servlets.HealthCheckServlet;  \nimport com.codahale.metrics.servlets.MetricsServlet;\n\nimport javax.servlet.ServletContextEvent;  \nimport javax.servlet.ServletContextListener;\n\npublic class MetricsServletContextListener implements ServletContextListener {\n\n    private MetricRegistry metricRegistry;\n    private HealthCheckRegistry healthCheckRegistry = new HealthCheckRegistry();\n\n    public MetricsServletContextListener(MetricRegistry metricRegistry, HealthCheckRegistry healthCheckRegistry) {\n        this.metricRegistry = metricRegistry;\n        this.healthCheckRegistry = healthCheckRegistry;\n    }\n\n    @Override\n    public void contextInitialized(ServletContextEvent servletContextEvent) {\n        servletContextEvent.getServletContext().setAttribute(HealthCheckServlet.HEALTH_CHECK_REGISTRY,healthCheckRegistry);\n        servletContextEvent.getServletContext().setAttribute(MetricsServlet.METRICS_REGISTRY, metricRegistry);\n    }\n\n    @Override\n    public void contextDestroyed(ServletContextEvent sce) {\n\n    }\n}\n</code></pre>\n\n<p>You also need to update your <strong>application.properties</strong> file:</p>\n\n<pre><code>metrics.enabled=true  \ngraphite.enabled=false  \ngraphite.host=localhost  \ngraphite.port=2003  \ngraphite.amount.of.time.between.polls=20000  \n</code></pre>\n\n<p>Here we set <strong>graphite.enabled</strong> to false, since we don't want to corrupt things on localhost. When we set graphite/grafana connection, we will be able to specify what host do we have on the cloud, and then we will enable graphite.</p>\n\n<p>PS - You can, of course, try for yourself, and enable it on localhost, but before you need to install graphite. <a href=\"https://gist.github.com/relaxdiego/7539911\">Here's</a> a brief Github Gist on how to install it.</p>\n\n<p>Just setting proper metrics and exposing them to HTTP endpoint is not enough. In our case, we need to send this metric data to Graphite. That's why we need to create an instance of a class called <a href=\"http://metrics.dropwizard.io/3.1.0/apidocs/com/codahale/metrics/graphite/GraphiteReporter.html\"><strong>GraphiteReporter</strong></a> - everything you need to set is graphite host, port and polling pause.</p>\n\n<h3 id=\"ahrefmonitoringinstancenamemonitoringinstanceiclassfafalinkanchorariahiddentrueiaadditionalcloudinstanceforgrafana\"><a href=\"#monitoringinstance\" name=\"monitoringinstance\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Additional cloud instance for Grafana</h3>\n\n<p>Things are getting interesting, aren't they? We did all Java work, and now we need to send our metrics to Graphite and show them on Grafana. I decided to create a separate instance because in case of failures and system shutdowns I'll be able to open the dashboard and see, what's happening. And besides, the more general rule of thumb is that we should use separate machines for each unit in our architecture. Separation of concerns. Everything should be on its place.</p>\n\n<p><img src=\"/content/images/2016/12/ScholarlySolidGermanshepherd.gif?style=centerme\" alt=\"\" /></p>\n\n<p>So, let's create terraform resource - an instruction for additional cloud instance. Please revise <a href=\"https://ivanursul.com/better-application-deployment-with-digitalocean-terraform-ansible-and-docker-connecting-terraform-with-ansible/\">previous</a> article for understanding things.</p>\n\n<pre><code>...\nresource \"digitalocean_droplet\" \"app-monitoring\" {  \n  image = \"ubuntu-14-04-x64\"\n  name = \"app-monitoring\"\n  region = \"${var.region}\"\n  size = \"512mb\"\n  ssh_keys = [\"${var.ssh_fingerprint}\"]\n}\n</code></pre>\n\n<p>Full terraform file can be found on [github-ADDD-LINk].(...) <br />\nCreating instance...</p>\n\n<pre><code>terraform apply \\  \n-var \"token=${DIGITALOCEAN_ACCESS_TOKEN}\" \\\n-var \"pub_key=$HOME/.ssh/id_rsa_do_token.pub\" \\\n-var \"pvt_key=$HOME/.ssh/id_rsa_do_token\" \\\n-var \"ssh_fingerprint=${SSH_FINGERPRINT}\"\n</code></pre>\n\n<p>Ok, we've checked out the cloud, we see a new instance, there's nothing new comparing to previous parts, so we can start configuring it.</p>\n\n<h3 id=\"ahrefplaybooknameplaybookiclassfafalinkanchorariahiddentrueiaplaybookforsettinggraphitegrafana\"><a href=\"#playbook\" name=\"playbook\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Playbook for setting Graphite/Grafana</h3>\n\n<p>Finally, we can start doing something entirely new - installing Graphite and Grafana on our instance.</p>\n\n<p>We should install Graphite first, and then Grafana. Graphite should be configured together with Carbon. Carbon is one of three components within Graphite project.</p>\n\n<p><img src=\"/content/images/2016/12/overview.png\" alt=\"\" /></p>\n\n<p>The idea is that we collect metrics from Java application, and then send them to carbon, which processes these metrics. Then there is a graphite-web app, which can render and show metrics, but user interface, which is present there, could be better. Additionally, there are no features, which helps us to understand graphs. That's why we use Grafana - for its excellent user interface and bunch of functions.</p>\n\n<p><img src=\"/content/images/2016/12/diagram-1.svg\" alt=\"\" /></p>\n\n<p>You see a diagram with two databases. However, there'll be a single database, with tables from both Graphite and Grafana. If you need two databases, feel free to tune ansible script and create an additional database.</p>\n\n<p>All scripts located on GitHub project, and you can observe them <a href=\"https://github.com/ivanursul/terraform-ansible-spring-boot-demo/tree/2bc1b501fe1603d3ff9f3f157810fc28e0506882\">here</a></p>\n\n<p>So, what's ansible doing on <strong>app-monitoring</strong> instance?</p>\n\n<ul>\n<li>Installs all Graphite, including Carbon. Carbon runs on <strong>2003</strong> port and Graphite is on <strong>8080</strong></li>\n<li>Installs postgres database</li>\n<li>Installs Grafana, on <strong>8567</strong> port. Credentials <strong>admin:SecureAdminPass</strong></li>\n<li>Deploys Spring Boot application with graphite host <a href=\"https://github.com/ivanursul/terraform-ansible-spring-boot-demo/blob/2bc1b501fe1603d3ff9f3f157810fc28e0506882/infrastructure/playbook-deploy.yml#L19\">param</a></li>\n</ul>\n\n<p>Steps I did should be enough for Graphite/Grafana, we've configured Spring Boot o send metrics to Carbon, and added data source for Grafana, so we have monitoring dashboards, which works out of the box.</p>\n\n<p>Run following script  </p>\n\n<pre><code>TF_STATE=terraform.tfstate ansible-playbook --inventory-file=/usr/local/bin/terraform-inventory playbook-bootstrap.yml  \n</code></pre>\n\n<p>Posting a little video which shows how to deploy and start everything from this article.</p>\n\n<div style=\"text-align: center;\"><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/zHxHgZE1svU\" frameborder=\"0\" allowfullscreen></iframe></div>\n\n<h3 id=\"ahreflinksnamelinksiclasscentermefafalinkanchorariahiddentrueialinks\"><a href=\"#links\" name=\"links\"><i class=\"centerme fa fa-link anchor\" aria-hidden=\"true\"></i></a> Links</h3>\n\n<ul>\n<li><a href=\"https://ivanursul.com/better-application-deployment-with-terraform-ansible-and-docker-part-1/\">Better application deployment with DigitalOcean, Terraform, Ansible, and Docker. Creating basic terraform instances</a></li>\n<li><a href=\"https://ivanursul.com/better-application-deployment-with-digitalocean-terraform-ansible-and-docker-dns-records/\">Better application deployment with DigitalOcean, Terraform, Ansible and Docker. DNS Records</a></li>\n<li><a href=\"https://ivanursul.com/better-application-deployment-with-digitalocean-terraform-ansible-and-docker-connecting-terraform-with-ansible/\">Better application deployment with DigitalOcean, Terraform, Ansible and Docker. Connecting Terraform with Ansible.</a></li>\n<li><a href=\"https://ivanursul.com/monitoring-your-spring-application-using-dropwizard-metrics-module/\">Monitoring your Spring application using Dropwizard metrics module</a></li>\n<li><a href=\"https://github.com/graphite-project/carbon\">Carbon project</a></li>\n</ul>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-12-02T10:06:01.000Z","created_by":1,"updated_at":"2016-12-07T15:52:12.000Z","updated_by":1,"published_at":"2016-12-05T19:59:36.000Z","published_by":1},{"id":56,"uuid":"c432ee66-2720-4f75-a709-3e7cacfa67c2","title":"Cooperation","slug":"cooperation","markdown":"If you've reached this page, most probably, you need some help: \n\n* Interesting, dedicated projects.\n* Proof of concepts for your ideas. \n* Reviewal for your frameworks and tools. \n\n![](/content/images/2016/12/Life-of-a-DBA-GIFs-typing-fast.gif?style=centerme)\n\nEmail me at [me@ivanursul.com](me@ivanursul.com) to discuss the details.","mobiledoc":null,"html":"<p>If you've reached this page, most probably, you need some help: </p>\n\n<ul>\n<li>Interesting, dedicated projects.</li>\n<li>Proof of concepts for your ideas. </li>\n<li>Reviewal for your frameworks and tools. </li>\n</ul>\n\n<p><img src=\"/content/images/2016/12/Life-of-a-DBA-GIFs-typing-fast.gif?style=centerme\" alt=\"\" /></p>\n\n<p>Email me at <a href=\"me@ivanursul.com\">me@ivanursul.com</a> to discuss the details.</p>","image":null,"featured":0,"page":1,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-12-03T10:31:55.000Z","created_by":1,"updated_at":"2016-12-08T09:39:36.000Z","updated_by":1,"published_at":"2016-12-03T10:32:00.000Z","published_by":1},{"id":57,"uuid":"ffa5ddfb-92b9-419a-ad9e-0639bd61995d","title":"How to limit number of requests","slug":"how-to-limit-number-of-requests","markdown":"Did you ever need to limit requests coming to your endpoints? Say, your maximum design capacity is near 10k requests/second, and you don't want to probe your service on higher rates. \n\nI choose to use [RateLimiter](https://github.com/google/guava/blob/master/guava/src/com/google/common/util/concurrent/RateLimiter.java) - small class from [Guava](https://github.com/google/guava).\n\nThe init process aren't so huge, just couple of lines:\n```\nRateLimiter rateLimiter = RateLimiter.create(10); // 10 requests/second\n```\n\nThen, just place `rateLimiter` in place, where you need to have limited requests:\n\n```\nboolean isAcquired = rateLimiter.tryAcquire();\n\nif (!isAcquired) {\n    throw new NotSoFastBuddyException(...);\n}\n```\n\n### <a href=\"#links\" name=\"links\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Links\n\n* [Guava](https://github.com/google/guava)","mobiledoc":null,"html":"<p>Did you ever need to limit requests coming to your endpoints? Say, your maximum design capacity is near 10k requests/second, and you don't want to probe your service on higher rates. </p>\n\n<p>I choose to use <a href=\"https://github.com/google/guava/blob/master/guava/src/com/google/common/util/concurrent/RateLimiter.java\">RateLimiter</a> - small class from <a href=\"https://github.com/google/guava\">Guava</a>.</p>\n\n<p>The init process aren't so huge, just couple of lines:  </p>\n\n<pre><code>RateLimiter rateLimiter = RateLimiter.create(10); // 10 requests/second  \n</code></pre>\n\n<p>Then, just place <code>rateLimiter</code> in place, where you need to have limited requests:</p>\n\n<pre><code>boolean isAcquired = rateLimiter.tryAcquire();\n\nif (!isAcquired) {  \n    throw new NotSoFastBuddyException(...);\n}\n</code></pre>\n\n<h3 id=\"ahreflinksnamelinksiclassfafalinkanchorariahiddentrueialinks\"><a href=\"#links\" name=\"links\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Links</h3>\n\n<ul>\n<li><a href=\"https://github.com/google/guava\">Guava</a></li>\n</ul>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-12-06T18:18:35.000Z","created_by":1,"updated_at":"2016-12-07T15:45:41.000Z","updated_by":1,"published_at":"2016-12-06T18:37:45.000Z","published_by":1},{"id":58,"uuid":"84add054-0641-4563-ae43-8b6e2f8b3e2d","title":"11 short stories about creating good pull requests","slug":"11-short-stories-about-creating-good-pull-requests","markdown":"Once upon a time there was a good developer. He produced a good code, had good relations with his teammates and never break the master branch. This guy followed 11 rules and lived a long - long life. So I'm posting them here, in case someone will have a good reason to improve his way of working on pull requests.\n\n### <a href=\"#makesmall\" name=\"makesmall\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Make them small\n\nIf you plan to change your source code, don't make significant changes with dozens of classes involved. Make a granular, little changes containing < 10 files. It's important because your teammates will do a review, and in a case of big pull requests it'll take time to understand what you wrote, why you wrote it, and find possible bugs. By making smaller pull requests, you let your teammates review your code more precisely and find possible mistakes there.\n\n### <a href=\"#initialpoint\" name=\"initialpoint\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Give an initial point\n\nIf you failed with small pull requests or not, always help your reviewers with a starting point. Where should they start? What unit of logic should they review first? It won't be a problem for you to give this information and your teammates will review your PR's faster.\n\n### <a href=\"#coupleofcomments\" name=\"coupleofcomments\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Write couple of comments \n\nIf needed, support your pull requests with a couple of comments. It may be a bad idea to put them in your code(it should be self-descriptive), but it's ok to explain some part of logic straightly inside your PR.\n\n### <a href=\"#doonething\" name=\"doonething\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Do only one thing\n\nIf you want to fix some bug, then don't start refactoring. Why on earth you need to do it within one pull request? Create two separate PR's, and this will remove all questions.\n\n### <a href=\"#sync\" name=\"sync\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Synchronize formatting configuration with your team\n\nMost probably, all your teammates use the same IDE, so spend some time on generating default configuration for it. Time spent on this will reduce the amount of code you will send to PR's since all imports/formatting will always be the same.\n\n### <a href=\"#bethefirstreviewer\" name=\"bethefirstreviewer\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Be the first reviewer\n\nYou don't need to wait for someone to review your code - start reviewing your request as soon as you create it. Most probably, you will find some little problems right after.\n\n### <a href=\"#dontbeinarush\" name=\"dontbeinarush\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Don't be in a rush\n\nThere's nothing worse than creating pull requests, and then another right after, which contradicts with each other because you made some mistake. Don't be in a rush, test your pull request well and create just one PR.\n\n### <a href=\"#description\" name=\"description\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Good description\n\nDo you use issue tracking system? Come on, give some title upon pull request. Write a couple of sentences for description section. Reviewer should already know what's it about before starting to review it.\n\n### <a href=\"#proofs\" name=\"proofs\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Prove your code is working\n\nDo you use tests in your project? Cover your code with tests and write a bunch of them. You don't have tests, but you're working on some issue? Give a screenshots/logs/other evidence which signalizes that you fix it.\n\n### <a href=\"#useprbuilder\" name=\"useprbuilder\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Build your code\n\nUse Pull Request builders. Before starting to read your code, the reviewer should already know it's working. What's the sense of reviewing it, if it doesn't work?\n\n### <a href=\"#dontmerge\" name=\"dontmerge\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Don't merge your PR if master is broken\n\nIt should be a rule of thumb - you don't merge your pull request if a master gets down. If you do this - you will get a confusion.\n\nRemember your tests aren't a silver bullet. Your tests can suck, and your proofs from PR can be incorrect. That's why, as soon as you notice problems with master - don't touch anything!\n\n\n\n\n\n","mobiledoc":null,"html":"<p>Once upon a time there was a good developer. He produced a good code, had good relations with his teammates and never break the master branch. This guy followed 11 rules and lived a long - long life. So I'm posting them here, in case someone will have a good reason to improve his way of working on pull requests.</p>\n\n<h3 id=\"ahrefmakesmallnamemakesmalliclassfafalinkanchorariahiddentrueiamakethemsmall\"><a href=\"#makesmall\" name=\"makesmall\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Make them small</h3>\n\n<p>If you plan to change your source code, don't make significant changes with dozens of classes involved. Make a granular, little changes containing &lt; 10 files. It's important because your teammates will do a review, and in a case of big pull requests it'll take time to understand what you wrote, why you wrote it, and find possible bugs. By making smaller pull requests, you let your teammates review your code more precisely and find possible mistakes there.</p>\n\n<h3 id=\"ahrefinitialpointnameinitialpointiclassfafalinkanchorariahiddentrueiagiveaninitialpoint\"><a href=\"#initialpoint\" name=\"initialpoint\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Give an initial point</h3>\n\n<p>If you failed with small pull requests or not, always help your reviewers with a starting point. Where should they start? What unit of logic should they review first? It won't be a problem for you to give this information and your teammates will review your PR's faster.</p>\n\n<h3 id=\"ahrefcoupleofcommentsnamecoupleofcommentsiclassfafalinkanchorariahiddentrueiawritecoupleofcomments\"><a href=\"#coupleofcomments\" name=\"coupleofcomments\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Write couple of comments</h3>\n\n<p>If needed, support your pull requests with a couple of comments. It may be a bad idea to put them in your code(it should be self-descriptive), but it's ok to explain some part of logic straightly inside your PR.</p>\n\n<h3 id=\"ahrefdoonethingnamedoonethingiclassfafalinkanchorariahiddentrueiadoonlyonething\"><a href=\"#doonething\" name=\"doonething\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Do only one thing</h3>\n\n<p>If you want to fix some bug, then don't start refactoring. Why on earth you need to do it within one pull request? Create two separate PR's, and this will remove all questions.</p>\n\n<h3 id=\"ahrefsyncnamesynciclassfafalinkanchorariahiddentrueiasynchronizeformattingconfigurationwithyourteam\"><a href=\"#sync\" name=\"sync\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Synchronize formatting configuration with your team</h3>\n\n<p>Most probably, all your teammates use the same IDE, so spend some time on generating default configuration for it. Time spent on this will reduce the amount of code you will send to PR's since all imports/formatting will always be the same.</p>\n\n<h3 id=\"ahrefbethefirstreviewernamebethefirstreviewericlassfafalinkanchorariahiddentrueiabethefirstreviewer\"><a href=\"#bethefirstreviewer\" name=\"bethefirstreviewer\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Be the first reviewer</h3>\n\n<p>You don't need to wait for someone to review your code - start reviewing your request as soon as you create it. Most probably, you will find some little problems right after.</p>\n\n<h3 id=\"ahrefdontbeinarushnamedontbeinarushiclassfafalinkanchorariahiddentrueiadontbeinarush\"><a href=\"#dontbeinarush\" name=\"dontbeinarush\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Don't be in a rush</h3>\n\n<p>There's nothing worse than creating pull requests, and then another right after, which contradicts with each other because you made some mistake. Don't be in a rush, test your pull request well and create just one PR.</p>\n\n<h3 id=\"ahrefdescriptionnamedescriptioniclassfafalinkanchorariahiddentrueiagooddescription\"><a href=\"#description\" name=\"description\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Good description</h3>\n\n<p>Do you use issue tracking system? Come on, give some title upon pull request. Write a couple of sentences for description section. Reviewer should already know what's it about before starting to review it.</p>\n\n<h3 id=\"ahrefproofsnameproofsiclassfafalinkanchorariahiddentrueiaproveyourcodeisworking\"><a href=\"#proofs\" name=\"proofs\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Prove your code is working</h3>\n\n<p>Do you use tests in your project? Cover your code with tests and write a bunch of them. You don't have tests, but you're working on some issue? Give a screenshots/logs/other evidence which signalizes that you fix it.</p>\n\n<h3 id=\"ahrefuseprbuildernameuseprbuildericlassfafalinkanchorariahiddentrueiabuildyourcode\"><a href=\"#useprbuilder\" name=\"useprbuilder\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Build your code</h3>\n\n<p>Use Pull Request builders. Before starting to read your code, the reviewer should already know it's working. What's the sense of reviewing it, if it doesn't work?</p>\n\n<h3 id=\"ahrefdontmergenamedontmergeiclassfafalinkanchorariahiddentrueiadontmergeyourprifmasterisbroken\"><a href=\"#dontmerge\" name=\"dontmerge\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Don't merge your PR if master is broken</h3>\n\n<p>It should be a rule of thumb - you don't merge your pull request if a master gets down. If you do this - you will get a confusion.</p>\n\n<p>Remember your tests aren't a silver bullet. Your tests can suck, and your proofs from PR can be incorrect. That's why, as soon as you notice problems with master - don't touch anything!</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":"11 short stories about creating good pull requests","meta_description":"11 tips on how to make a good pull requests. Best practices for pull requests","author_id":1,"created_at":"2016-12-06T18:52:23.000Z","created_by":1,"updated_at":"2016-12-07T22:34:57.000Z","updated_by":1,"published_at":"2016-12-07T15:19:44.000Z","published_by":1},{"id":59,"uuid":"20adf01c-4539-441f-a671-0f0f19e4e083","title":"Lessons leant from tuning inciddent alerting system","slug":"lessons-leant-from-tuning-inciddent-alerting-system","markdown":"### <a href=\"#dontusehistorical\" name=\"dontusehistorical\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Don't use historical metrics for your alerts!\n\nMean vs median example from imageProcessingListener\n\n### <a href=\"#dontusehistorical\" name=\"dontusehistorical\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> General exceptions can do a bad favour for you\n\n### Not all 5xx exceptions should escalate and incident","mobiledoc":null,"html":"<h3 id=\"ahrefdontusehistoricalnamedontusehistoricaliclassfafalinkanchorariahiddentrueiadontusehistoricalmetricsforyouralerts\"><a href=\"#dontusehistorical\" name=\"dontusehistorical\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Don't use historical metrics for your alerts!</h3>\n\n<p>Mean vs median example from imageProcessingListener</p>\n\n<h3 id=\"ahrefdontusehistoricalnamedontusehistoricaliclassfafalinkanchorariahiddentrueiageneralexceptionscandoabadfavourforyou\"><a href=\"#dontusehistorical\" name=\"dontusehistorical\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> General exceptions can do a bad favour for you</h3>\n\n<h3 id=\"notall5xxexceptionsshouldescalateandincident\">Not all 5xx exceptions should escalate and incident</h3>","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-12-06T21:20:18.000Z","created_by":1,"updated_at":"2017-02-23T12:00:50.000Z","updated_by":1,"published_at":null,"published_by":null},{"id":60,"uuid":"f5393ea5-b756-4d06-b189-6850cc18cb6a","title":"Trying Amazon API Gateway, Lambda and DynamoDb","slug":"hosting-your-blog-on-github-using-amazon-infrastructure-part-one-serverless-backend-2","markdown":"At the beginning of June 2016, I visited a [JavaDay](http://javaday.org.ua/) conference in Lviv, where I listen to a talk about serverless architecture. Today I'd like to try serverless.\n\n[According to Martin Fowler](http://martinfowler.com/articles/serverless.html), serverless is an architecture, which uses third party services or a custom code on ephemeral containers, best known as Amazon Lambda. So, I understand it as an idea to run your backend code on demand: there won't be any real servers like EC2, Amazon infrastructure will start a container, which will run your Lambda function. Consider reading [this](http://docs.aws.amazon.com/lambda/latest/dg/lambda-introduction.html) article for explanations. I'd like to try this infrastructure.\n\nI decided to use Amazon API Gateway and Amazon Lambda for analyzing GitHub repositories: after each push to the repository I will examine all **.md** files and store them to Amazon Dynamo Database. More generally, I'll create a backend part for a self-written blogging platform. Who knows, maybe I will write a frontend part soon.\n\n### <a href=\"#generalplan\" name=\"generalplan\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> General plan\n\nThe approach isn't clear enough, so let's clarify what do we need:\n\n* We want to save articles on **GitHub**\n* We need to have an API for our blog\n* Since we need to have a good latency for API, reading articles from GitHub is not an option for us. So, we need some storage. For this post, I choose **Amazon DynamoDB**.\n* We need to have a **backend** part. I'll be using **Amazon Lambda.**\n\nHow it's going to work:\n\n* GitHub will send a WebHook to Amazon Lambda through API Gateway. WebHook will contain information about all new, updated or removed articles, so will handle each type of event accordingly.\n* **Amazon Lambda** will receive an event, and process all affected items. All the elements will be inserted, updated or removed from Amazon DynamoDB. \n* There'll be a second **Lambda** function, which will give paginated information about articles.\n\nFollowing diagram explains how blog posts will be handled by our system.\n\n![](/content/images/2016/12/serverless-4.svg)\n\n### <a href=\"#iam\" name=\"iam\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Create IAM role\n\nBefore you start, you need to create some role for lambda. I created a **lambda-demo** role with following policies:\n\n* AWSLambdaFullAccess\n* AmazonDynamoDBFullAccess\n\nI believe, there should be more restricted policies, but for demo purpose, I decided not think about that.\n\nHere's how it should look\n\n![](/content/images/2016/12/Screen-Shot-2016-12-11-at-12-05-03-PM-1.png?style=centerme)\n\nThis two policies will us a green light on running Amazon Lambda and working with DynamoDB.\n\n### <a href=\"#dynamo\" name=\"dynamo\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Create Amazon DynamoDB table\n\nGo to Amazon DynamoDB dashboard page and create a table with **posts** table name and **title** key. We don't need to create more columns, this is just an example.\n\n### <a href=\"#lambda1\" name=\"lambda1\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Creating Lambda functions\n\nAccording to [documentation](http://docs.aws.amazon.com/lambda/latest/dg/lambda-java-how-to-create-deployment-package.html), to run lambda, you need to upload a **.zip** file with Java classes. You'll need to provide a full reference path, together with **packages** prefix.\n\nI created a separate repository for you so don't need to copy every piece of code from blog - [github.com/ivanursul/amazon-lambda](https://github.com/ivanursul/amazon-lambda)\n\nThere are only two lambda functions:\n\n* [**ProcessRequestHandler**](https://github.com/ivanursul/amazon-lambda/blob/master/src/main/java/org/ivanursul/blog/ProcessRequestHandler.java), which process GitHub WebHook and saves/updates/removes articles from **DynamoDB**\n* [**ArticlesRequestHandler**](https://github.com/ivanursul/amazon-lambda/blob/master/src/main/java/org/ivanursul/blog/ArticlesRequestHandler.java), which exposes information about articles.\n\nClone this project to your local machine, build it with \n```\ngradle clean build\n``` \n\ngo to `build/distributions/` and make sure archive `amazon-lambda.zip` is present there.\n\nNext, go to `Lambda Management Console` and perform following steps:\n\n* Press `Create a Lambda function.`\n* Choose `Blank function.`\n* Next\n* Fill in following data\n * Name -> **ProcessRequestHandler**\n * Runtime -> **Java 8**\n * Code entry type -> Upload a zip file\n * Handler -> **org.ivanursul.blog.ProcessRequestHandler**\n * Role -> Choose an existing role\n * Existing role -> **lambda-demo**\n\n\nRepeat the same steps with the second function, **ArticlesRequestHandler**.\n\n### <a href=\"#apigateway\" name=\"apigateway\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Creating API Gateway\n\nHaving just a Lambda function is not enough for using it with GitHub. We need to expose it to HTTP protocol. If we are using Amazon products here, then it's obvious to try [Amazon API Gateway](https://aws.amazon.com/api-gateway)\n\nWe need to expose two lambda functions - ProcessRequestHandler and ArticlesRequestHandler.\n\nYou need to perform several steps for connecting lambda with api gateway:\n\n* Enter [API Gateway Console](https://aws.amazon.com/api-gateway)\n* Press **Create API**\n* Choose **Import from Swagger**\n* Paste content from **amazon-lambda-api-gateway.yaml**\n* Additionally, you need to configure Lambda integration\n\n\nHere's how ArticlesRequestHandler API Gateway should look like:\n\n![](/content/images/2016/12/Screen-Shot-2016-12-13-at-3-31-47-PM.png?style=centeme)\n\nNext, add Body Mappings Template: **application/json** with empty string content.\n\nDo the same with ProcessRequestHandler, but add another **application/json** mappings template:\n```\n{\n    \"body\" : $input.json('$')\n}\n```\n\nSave, and Deploy your API.\n\n\n### <a href=\"#webhook\" name=\"webhook\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Configuring WebHook\n\nAfter you deploy API Gateway, you need to specify a link for ProcessRequestHandler gateway for your WebHook.\n\nGo to your blogging repository on the Settings page, then on WebHook section and put the link.\n\nIn my case, it looks like this\n![](/content/images/2016/12/Screen-Shot-2016-12-13-at-6-07-45-PM-1.png?style=centerme)\n\nLink example: **https://tja9ll5tv3.execute-api.us-west-1.amazonaws.com/prod/ProcessRequestHandler**\nYou should have something similar.\n\n\nIf you fail with text instuction, you can watch video:\n\n<div style=\"text-align: center;\"><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/4k26xh3TqSQ\" frameborder=\"0\" allowfullscreen></iframe></div>\n\n### <a href=\"#conclusions name=\"conclusions\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Conclusions\n\nWhat I've learned from Amazon Lambda?\n\n* There's an instrument, which allows you to write and deploy some logic within minutes. You just need to write a single function, pack it inside a zip file, and upload to Amazon. Great, doesn't it?\n* If you have small traffic, it will cost you few cents per month. \n* It takes a time to deploy your single line function to Amazon container and to run it, so don't put large libraries inside.\n* If you start to use some of Amazon products, it's extremely hard to use just one tool. Without fail, you will need to use other products, so be ready to learn them. I found it harder to use MongoDB, than DynamoDB, in this case. The problem is that we are accustomed to using servers in our backend applications and it's okay to start an application within 3-5 seconds. Here, inside Lambda, we need to calculate every millisecond.\n* Don't hesitate on removing old Lambda's, which you don't use.\n\nPS - This blog post is now a part of [Deployment Using Containers WIKI](http://electric-cloud.com/wiki/display/releasemanagement/Deployment+Using+Containers#DeploymentUsingContainers-HowTo)\n\n### <a href=\"#links\" name=\"links\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Links\n\n* [Amazon Lambda Backend Repo](https://github.com/ivanursul/amazon-lambda)\n* [Lambda Functions](http://docs.aws.amazon.com/lambda/latest/dg/lambda-introduction-function.html)\n* [Deployment Using Containers](http://electric-cloud.com/wiki/display/releasemanagement/Deployment+Using+Containers#DeploymentUsingContainers-HowTo)","mobiledoc":null,"html":"<p>At the beginning of June 2016, I visited a <a href=\"http://javaday.org.ua/\">JavaDay</a> conference in Lviv, where I listen to a talk about serverless architecture. Today I'd like to try serverless.</p>\n\n<p><a href=\"http://martinfowler.com/articles/serverless.html\">According to Martin Fowler</a>, serverless is an architecture, which uses third party services or a custom code on ephemeral containers, best known as Amazon Lambda. So, I understand it as an idea to run your backend code on demand: there won't be any real servers like EC2, Amazon infrastructure will start a container, which will run your Lambda function. Consider reading <a href=\"http://docs.aws.amazon.com/lambda/latest/dg/lambda-introduction.html\">this</a> article for explanations. I'd like to try this infrastructure.</p>\n\n<p>I decided to use Amazon API Gateway and Amazon Lambda for analyzing GitHub repositories: after each push to the repository I will examine all <strong>.md</strong> files and store them to Amazon Dynamo Database. More generally, I'll create a backend part for a self-written blogging platform. Who knows, maybe I will write a frontend part soon.</p>\n\n<h3 id=\"ahrefgeneralplannamegeneralplaniclassfafalinkanchorariahiddentrueiageneralplan\"><a href=\"#generalplan\" name=\"generalplan\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> General plan</h3>\n\n<p>The approach isn't clear enough, so let's clarify what do we need:</p>\n\n<ul>\n<li>We want to save articles on <strong>GitHub</strong></li>\n<li>We need to have an API for our blog</li>\n<li>Since we need to have a good latency for API, reading articles from GitHub is not an option for us. So, we need some storage. For this post, I choose <strong>Amazon DynamoDB</strong>.</li>\n<li>We need to have a <strong>backend</strong> part. I'll be using <strong>Amazon Lambda.</strong></li>\n</ul>\n\n<p>How it's going to work:</p>\n\n<ul>\n<li>GitHub will send a WebHook to Amazon Lambda through API Gateway. WebHook will contain information about all new, updated or removed articles, so will handle each type of event accordingly.</li>\n<li><strong>Amazon Lambda</strong> will receive an event, and process all affected items. All the elements will be inserted, updated or removed from Amazon DynamoDB. </li>\n<li>There'll be a second <strong>Lambda</strong> function, which will give paginated information about articles.</li>\n</ul>\n\n<p>Following diagram explains how blog posts will be handled by our system.</p>\n\n<p><img src=\"/content/images/2016/12/serverless-4.svg\" alt=\"\" /></p>\n\n<h3 id=\"ahrefiamnameiamiclassfafalinkanchorariahiddentrueiacreateiamrole\"><a href=\"#iam\" name=\"iam\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Create IAM role</h3>\n\n<p>Before you start, you need to create some role for lambda. I created a <strong>lambda-demo</strong> role with following policies:</p>\n\n<ul>\n<li>AWSLambdaFullAccess</li>\n<li>AmazonDynamoDBFullAccess</li>\n</ul>\n\n<p>I believe, there should be more restricted policies, but for demo purpose, I decided not think about that.</p>\n\n<p>Here's how it should look</p>\n\n<p><img src=\"/content/images/2016/12/Screen-Shot-2016-12-11-at-12-05-03-PM-1.png?style=centerme\" alt=\"\" /></p>\n\n<p>This two policies will us a green light on running Amazon Lambda and working with DynamoDB.</p>\n\n<h3 id=\"ahrefdynamonamedynamoiclassfafalinkanchorariahiddentrueiacreateamazondynamodbtable\"><a href=\"#dynamo\" name=\"dynamo\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Create Amazon DynamoDB table</h3>\n\n<p>Go to Amazon DynamoDB dashboard page and create a table with <strong>posts</strong> table name and <strong>title</strong> key. We don't need to create more columns, this is just an example.</p>\n\n<h3 id=\"ahreflambda1namelambda1iclassfafalinkanchorariahiddentrueiacreatinglambdafunctions\"><a href=\"#lambda1\" name=\"lambda1\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Creating Lambda functions</h3>\n\n<p>According to <a href=\"http://docs.aws.amazon.com/lambda/latest/dg/lambda-java-how-to-create-deployment-package.html\">documentation</a>, to run lambda, you need to upload a <strong>.zip</strong> file with Java classes. You'll need to provide a full reference path, together with <strong>packages</strong> prefix.</p>\n\n<p>I created a separate repository for you so don't need to copy every piece of code from blog - <a href=\"https://github.com/ivanursul/amazon-lambda\">github.com/ivanursul/amazon-lambda</a></p>\n\n<p>There are only two lambda functions:</p>\n\n<ul>\n<li><a href=\"https://github.com/ivanursul/amazon-lambda/blob/master/src/main/java/org/ivanursul/blog/ProcessRequestHandler.java\"><strong>ProcessRequestHandler</strong></a>, which process GitHub WebHook and saves/updates/removes articles from <strong>DynamoDB</strong></li>\n<li><a href=\"https://github.com/ivanursul/amazon-lambda/blob/master/src/main/java/org/ivanursul/blog/ArticlesRequestHandler.java\"><strong>ArticlesRequestHandler</strong></a>, which exposes information about articles.</li>\n</ul>\n\n<p>Clone this project to your local machine, build it with  </p>\n\n<pre><code>gradle clean build  \n</code></pre>\n\n<p>go to <code>build/distributions/</code> and make sure archive <code>amazon-lambda.zip</code> is present there.</p>\n\n<p>Next, go to <code>Lambda Management Console</code> and perform following steps:</p>\n\n<ul>\n<li>Press <code>Create a Lambda function.</code></li>\n<li>Choose <code>Blank function.</code></li>\n<li>Next</li>\n<li>Fill in following data\n<ul><li>Name -> <strong>ProcessRequestHandler</strong></li>\n<li>Runtime -> <strong>Java 8</strong></li>\n<li>Code entry type -> Upload a zip file</li>\n<li>Handler -> <strong>org.ivanursul.blog.ProcessRequestHandler</strong></li>\n<li>Role -> Choose an existing role</li>\n<li>Existing role -> <strong>lambda-demo</strong></li></ul></li>\n</ul>\n\n<p>Repeat the same steps with the second function, <strong>ArticlesRequestHandler</strong>.</p>\n\n<h3 id=\"ahrefapigatewaynameapigatewayiclassfafalinkanchorariahiddentrueiacreatingapigateway\"><a href=\"#apigateway\" name=\"apigateway\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Creating API Gateway</h3>\n\n<p>Having just a Lambda function is not enough for using it with GitHub. We need to expose it to HTTP protocol. If we are using Amazon products here, then it's obvious to try <a href=\"https://aws.amazon.com/api-gateway\">Amazon API Gateway</a></p>\n\n<p>We need to expose two lambda functions - ProcessRequestHandler and ArticlesRequestHandler.</p>\n\n<p>You need to perform several steps for connecting lambda with api gateway:</p>\n\n<ul>\n<li>Enter <a href=\"https://aws.amazon.com/api-gateway\">API Gateway Console</a></li>\n<li>Press <strong>Create API</strong></li>\n<li>Choose <strong>Import from Swagger</strong></li>\n<li>Paste content from <strong>amazon-lambda-api-gateway.yaml</strong></li>\n<li>Additionally, you need to configure Lambda integration</li>\n</ul>\n\n<p>Here's how ArticlesRequestHandler API Gateway should look like:</p>\n\n<p><img src=\"/content/images/2016/12/Screen-Shot-2016-12-13-at-3-31-47-PM.png?style=centeme\" alt=\"\" /></p>\n\n<p>Next, add Body Mappings Template: <strong>application/json</strong> with empty string content.</p>\n\n<p>Do the same with ProcessRequestHandler, but add another <strong>application/json</strong> mappings template:  </p>\n\n<pre><code>{\n    \"body\" : $input.json('$')\n}\n</code></pre>\n\n<p>Save, and Deploy your API.</p>\n\n<h3 id=\"ahrefwebhooknamewebhookiclassfafalinkanchorariahiddentrueiaconfiguringwebhook\"><a href=\"#webhook\" name=\"webhook\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Configuring WebHook</h3>\n\n<p>After you deploy API Gateway, you need to specify a link for ProcessRequestHandler gateway for your WebHook.</p>\n\n<p>Go to your blogging repository on the Settings page, then on WebHook section and put the link.</p>\n\n<p>In my case, it looks like this <br />\n<img src=\"/content/images/2016/12/Screen-Shot-2016-12-13-at-6-07-45-PM-1.png?style=centerme\" alt=\"\" /></p>\n\n<p>Link example: <strong><a href=\"https://tja9ll5tv3.execute-api.us-west-1.amazonaws.com/prod/ProcessRequestHandler\">https://tja9ll5tv3.execute-api.us-west-1.amazonaws.com/prod/ProcessRequestHandler</a></strong> <br />\nYou should have something similar.</p>\n\n<p>If you fail with text instuction, you can watch video:</p>\n\n<div style=\"text-align: center;\"><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/4k26xh3TqSQ\" frameborder=\"0\" allowfullscreen></iframe></div>\n\n<h3 id=\"ahrefconclusionsnameconclusionsiclassfafalinkanchorariahiddentrueiaconclusions\"><a href=\"#conclusions name=\"conclusions\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Conclusions</h3>\n\n<p>What I've learned from Amazon Lambda?</p>\n\n<ul>\n<li>There's an instrument, which allows you to write and deploy some logic within minutes. You just need to write a single function, pack it inside a zip file, and upload to Amazon. Great, doesn't it?</li>\n<li>If you have small traffic, it will cost you few cents per month. </li>\n<li>It takes a time to deploy your single line function to Amazon container and to run it, so don't put large libraries inside.</li>\n<li>If you start to use some of Amazon products, it's extremely hard to use just one tool. Without fail, you will need to use other products, so be ready to learn them. I found it harder to use MongoDB, than DynamoDB, in this case. The problem is that we are accustomed to using servers in our backend applications and it's okay to start an application within 3-5 seconds. Here, inside Lambda, we need to calculate every millisecond.</li>\n<li>Don't hesitate on removing old Lambda's, which you don't use.</li>\n</ul>\n\n<p>PS - This blog post is now a part of <a href=\"http://electric-cloud.com/wiki/display/releasemanagement/Deployment+Using+Containers#DeploymentUsingContainers-HowTo\">Deployment Using Containers WIKI</a></p>\n\n<h3 id=\"ahreflinksnamelinksiclassfafalinkanchorariahiddentrueialinks\"><a href=\"#links\" name=\"links\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Links</h3>\n\n<ul>\n<li><a href=\"https://github.com/ivanursul/amazon-lambda\">Amazon Lambda Backend Repo</a></li>\n<li><a href=\"http://docs.aws.amazon.com/lambda/latest/dg/lambda-introduction-function.html\">Lambda Functions</a></li>\n<li><a href=\"http://electric-cloud.com/wiki/display/releasemanagement/Deployment+Using+Containers#DeploymentUsingContainers-HowTo\">Deployment Using Containers</a></li>\n</ul>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":"Trying Amazon API Gateway, Lambda and DynamoDb","meta_description":"This blog post shows how I used Amazon Lambda together with API Gateway and DynamoDB. ","author_id":1,"created_at":"2016-12-09T07:46:59.000Z","created_by":1,"updated_at":"2016-12-19T09:38:35.000Z","updated_by":1,"published_at":"2016-12-13T18:20:55.000Z","published_by":1},{"id":62,"uuid":"d6cfa932-07b7-4908-aeda-d5f83ffedd43","title":"How to trace your logs using SLF4J MDC","slug":"slf4j-mdc","markdown":"How do you use your logs for searching problem requests? \nFor instance, you got a problem response, with all headers, response body, and you need to find appropriate logs.  How would you do that?\n\nPersonally, I found it useful to write some words about MDC - Mapped Diagnostic Context. Shortly, it is a concept of mapping request specific information. \n\n### <a href=\"#usage\" name=\"usage\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Usage\n\nWe will configure MDC in Spring Boot application. We will use SLF4J on top of Logback implementation. Using it all together, we will create a unique requestId for each request in our system.\n\n### <a href=\"#components\" name=\"components\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Components\n\nWe will use 4 components here: Spring Boot, Slf4j, Logback and Java\n\n##### **Spring Boot**\n\nSpring Boot will be used for managing dependency injection and registering pure Java Filter.\n\n##### **Slf4j**\n\nSimple Logging Facade is used for following abstraction principles. Additionally, [MDC](http://www.slf4j.org/api/org/slf4j/MDC.html) class is located inside slf4j dependency. Similar classes are inside log4j and logback dependencies.\n\n##### **Logback** \n\nLogback is used as one of logging providers\n\n###### **Pure Java** \n\nJava is used for writing simple Java Filter.\n\n### <a href=\"#affectedfiles\" name=\"affectedfiles\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Affected files\n\n#####  **MDCFilter**\n\n```\npackage org.startup.queue.filter;\n\nimport org.slf4j.MDC;\nimport org.springframework.stereotype.Component;\n\nimport javax.servlet.*;\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.http.HttpServletResponse;\nimport java.io.IOException;\nimport java.util.UUID;\n\n@Component\npublic class MDCFilter implements Filter {\n\n    @Override\n    public void init(FilterConfig filterConfig) throws ServletException { }\n\n    @Override\n    public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException {\n        HttpServletRequest httpRequest = (HttpServletRequest) request;\n        HttpServletResponse httpResponse = (HttpServletResponse) response;\n\n        String requestId = httpRequest.getHeader(\"requestId\");\n\n        if (requestId == null) {\n            requestId = UUID.randomUUID().toString();\n        }\n\n        MDC.put(\"requestId\", requestId);\n        httpResponse.setHeader(\"requestId\", requestId);\n\n        try {\n            chain.doFilter(request, response);\n        } finally {\n            MDC.remove(\"requestId\");\n        }\n    }\n\n    @Override\n    public void destroy() {\n\n    }\n}\n```\n\nHow MDC handle multiple requests ?\nIt works per thread using ThreadLocal:\n```\npublic class BasicMDCAdapter implements MDCAdapter {\n\n    private InheritableThreadLocal<Map<String, String>> inheritableThreadLocal = new InheritableThreadLocal<Map<String, String>>();\n...\n}\n```\n\n##### **logback.xml**\n\n```java\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<configuration>\n\n    <appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\">\n        <layout class=\"ch.qos.logback.classic.PatternLayout\">\n            <Pattern>\n                %d{yyyy-MM-dd HH:mm:ss} %X{requestId} [%thread] %-5level %logger{36} - %msg%n\n            </Pattern>\n        </layout>\n    </appender>\n\n    <root level=\"error\">\n        <appender-ref ref=\"STDOUT\" />\n    </root>\n\n</configuration>\n```\n\n\n### <a href=\"#result\" name=\"result\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Result\n\nThe result of this little trick is to have **requestId** in your headers:\n\n```\nHTTP/1.1 200 OK\n...\nrequestId: 11ae099f-3d8f-4574-9c40-1c38613f6605\n...\n```\n\nYou can now use it as an identifier for searching your logs.\n\n\n### <a href=\"#links\" name=\"links\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Links\n\n* [Mapped Diagnostic Context](http://logback.qos.ch/manual/mdc.html)","mobiledoc":null,"html":"<p>How do you use your logs for searching problem requests? <br />\nFor instance, you got a problem response, with all headers, response body, and you need to find appropriate logs.  How would you do that?</p>\n\n<p>Personally, I found it useful to write some words about MDC - Mapped Diagnostic Context. Shortly, it is a concept of mapping request specific information. </p>\n\n<h3 id=\"ahrefusagenameusageiclassfafalinkanchorariahiddentrueiausage\"><a href=\"#usage\" name=\"usage\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Usage</h3>\n\n<p>We will configure MDC in Spring Boot application. We will use SLF4J on top of Logback implementation. Using it all together, we will create a unique requestId for each request in our system.</p>\n\n<h3 id=\"ahrefcomponentsnamecomponentsiclassfafalinkanchorariahiddentrueiacomponents\"><a href=\"#components\" name=\"components\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Components</h3>\n\n<p>We will use 4 components here: Spring Boot, Slf4j, Logback and Java</p>\n\n<h5 id=\"springboot\"><strong>Spring Boot</strong></h5>\n\n<p>Spring Boot will be used for managing dependency injection and registering pure Java Filter.</p>\n\n<h5 id=\"slf4j\"><strong>Slf4j</strong></h5>\n\n<p>Simple Logging Facade is used for following abstraction principles. Additionally, <a href=\"http://www.slf4j.org/api/org/slf4j/MDC.html\">MDC</a> class is located inside slf4j dependency. Similar classes are inside log4j and logback dependencies.</p>\n\n<h5 id=\"logback\"><strong>Logback</strong></h5>\n\n<p>Logback is used as one of logging providers</p>\n\n<h6 id=\"purejava\"><strong>Pure Java</strong></h6>\n\n<p>Java is used for writing simple Java Filter.</p>\n\n<h3 id=\"ahrefaffectedfilesnameaffectedfilesiclassfafalinkanchorariahiddentrueiaaffectedfiles\"><a href=\"#affectedfiles\" name=\"affectedfiles\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Affected files</h3>\n\n<h5 id=\"mdcfilter\"><strong>MDCFilter</strong></h5>\n\n<pre><code>package org.startup.queue.filter;\n\nimport org.slf4j.MDC;  \nimport org.springframework.stereotype.Component;\n\nimport javax.servlet.*;  \nimport javax.servlet.http.HttpServletRequest;  \nimport javax.servlet.http.HttpServletResponse;  \nimport java.io.IOException;  \nimport java.util.UUID;\n\n@Component\npublic class MDCFilter implements Filter {\n\n    @Override\n    public void init(FilterConfig filterConfig) throws ServletException { }\n\n    @Override\n    public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException {\n        HttpServletRequest httpRequest = (HttpServletRequest) request;\n        HttpServletResponse httpResponse = (HttpServletResponse) response;\n\n        String requestId = httpRequest.getHeader(\"requestId\");\n\n        if (requestId == null) {\n            requestId = UUID.randomUUID().toString();\n        }\n\n        MDC.put(\"requestId\", requestId);\n        httpResponse.setHeader(\"requestId\", requestId);\n\n        try {\n            chain.doFilter(request, response);\n        } finally {\n            MDC.remove(\"requestId\");\n        }\n    }\n\n    @Override\n    public void destroy() {\n\n    }\n}\n</code></pre>\n\n<p>How MDC handle multiple requests ? <br />\nIt works per thread using ThreadLocal:  </p>\n\n<pre><code>public class BasicMDCAdapter implements MDCAdapter {\n\n    private InheritableThreadLocal&lt;Map&lt;String, String&gt;&gt; inheritableThreadLocal = new InheritableThreadLocal&lt;Map&lt;String, String&gt;&gt;();\n...\n}\n</code></pre>\n\n<h5 id=\"logbackxml\"><strong>logback.xml</strong></h5>\n\n<pre><code class=\"language-java\">&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;  \n&lt;configuration&gt;\n\n    &lt;appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt;\n        &lt;layout class=\"ch.qos.logback.classic.PatternLayout\"&gt;\n            &lt;Pattern&gt;\n                %d{yyyy-MM-dd HH:mm:ss} %X{requestId} [%thread] %-5level %logger{36} - %msg%n\n            &lt;/Pattern&gt;\n        &lt;/layout&gt;\n    &lt;/appender&gt;\n\n    &lt;root level=\"error\"&gt;\n        &lt;appender-ref ref=\"STDOUT\" /&gt;\n    &lt;/root&gt;\n\n&lt;/configuration&gt;  \n</code></pre>\n\n<h3 id=\"ahrefresultnameresulticlassfafalinkanchorariahiddentrueiaresult\"><a href=\"#result\" name=\"result\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Result</h3>\n\n<p>The result of this little trick is to have <strong>requestId</strong> in your headers:</p>\n\n<pre><code>HTTP/1.1 200 OK  \n...\nrequestId: 11ae099f-3d8f-4574-9c40-1c38613f6605  \n...\n</code></pre>\n\n<p>You can now use it as an identifier for searching your logs.</p>\n\n<h3 id=\"ahreflinksnamelinksiclassfafalinkanchorariahiddentrueialinks\"><a href=\"#links\" name=\"links\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Links</h3>\n\n<ul>\n<li><a href=\"http://logback.qos.ch/manual/mdc.html\">Mapped Diagnostic Context</a></li>\n</ul>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":"How to trace your logs using SLF4J MDC","meta_description":"This blog post explains a short tip which you can use to better search your logs using request identifier.","author_id":1,"created_at":"2016-12-15T18:52:18.000Z","created_by":1,"updated_at":"2016-12-17T08:59:25.000Z","updated_by":1,"published_at":"2016-12-17T08:42:14.000Z","published_by":1},{"id":64,"uuid":"55ee1391-5b65-4bff-b744-84e7fbb57d21","title":"How to map Spring Boot properties to Java class","slug":"how-to-map-spring-boot-properties-to-java-class","markdown":"Have you ever had a need to use some values from **application.properties** or **application.yml? How did you take them out?\n\nPersonally, I always used [@Value](http://docs.spring.io/spring/docs/current/javadoc-api/org/springframework/beans/factory/annotation/Value.html) annotation:\n\n```\n    @Value(\"${graphite.host}\")\n    private String graphiteHost;\n```\n\nIt wasn't the best way to work with my properties. However, I didn't know a better approach.\n\nThen, I found [@ConfigurationProperties](http://docs.spring.io/spring-boot/docs/1.1.7.RELEASE/api/org/springframework/boot/context/properties/ConfigurationProperties.html) - annotation from Boot package, which has everything you need to map your properties. \n\n### <a href=\"#given\" name=\"given\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Given\n\nLet's say, your **application.yml** looks like following:\n\n```\ngraphite:\n  enabled: true\n  host: localhost\n  port: 2003\n  amountOfTimeBetweenPolls: 20000\n```\n\n### <a href=\"#when\" name=\"when\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> When\n\n\nYou need to create a bunch of classes, which you will be autowiring in all parts of your code.\n\nI'm using [Project Lombok](https://projectlombok.org/) for skipping java formalities, if you are not, then create getters and setters for your classes. \n\n```\npackage org.rngr.properties;\n\nimport lombok.Data;\nimport org.springframework.boot.context.properties.ConfigurationProperties;\n\nimport javax.validation.constraints.NotNull;\n\n@ConfigurationProperties(ignoreUnknownFields = true)\n@Data\npublic class ApplicationProperties {\n\n    @NotNull\n    private GraphiteProperties graphite;\n\n}\n```\n\nPay attention to `@ConfigurationProperties` annotation, it's playing a key role here.\n\nDon't forget about subclass:\n\n```\npackage org.rngr.properties;\n\nimport lombok.Data;\n\n@Data\npublic class GraphiteProperties {\n\n    private boolean enabled;\n    private String host;\n    private int port;\n    private int amountOfTimeBetweenPolls;\n\n}\n```\n\nIn the end, you need to enable configuration properties:\n\n```\npackage org.rngr;\n\nimport lombok.extern.slf4j.Slf4j;\nimport org.rngr.config.*;\nimport org.rngr.properties.ApplicationProperties;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.boot.context.properties.EnableConfigurationProperties;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.context.annotation.Import;\n\n@SpringBootApplication\n@Import({\n\t\tMonitoringConfiguration.class\n})\n@Configuration\n@EnableConfigurationProperties(ApplicationProperties.class)\npublic class WebappApplication {\n\n\tpublic static void main(String[] args) {\n\t\tSpringApplication.run(WebappApplication.class, args);\n\t}\n}\n```\n\n### <a href=\"#then\" name=\"then\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Then\n\nThen, you are free to Autowire **ApplicationProperties** instance wherever you want:\n\n```\n    ...\n    @Autowired\n    private ApplicationProperties properties;\n    ...\n```\n\n### <a href=\"#conclusions\" name=\"conclusions\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Conclusions\n\nI guess, this approach was [borrowed](https://github.com/dropwizard/dropwizard/blob/master/dropwizard-core/src/main/java/io/dropwizard/Configuration.java) from Dropwizard framework. One good thing about introducing configuration properties class is that you will get power to validate them.\n\nI remember a few years ago there was a broad discussion about pros and cons of switching to Java configuration. For me, Configuration properties and their transformation to ConfigurationProperties class is something similar.\n\n### <a href=\"#links\" name=\"links\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Links\n\n* [Empowering your apps with Spring Boot's property support](https://spring.io/blog/2013/10/30/empowering-your-apps-with-spring-boot-s-property-support)","mobiledoc":null,"html":"<p>Have you ever had a need to use some values from <strong>application.properties</strong> or **application.yml? How did you take them out?</p>\n\n<p>Personally, I always used <a href=\"http://docs.spring.io/spring/docs/current/javadoc-api/org/springframework/beans/factory/annotation/Value.html\">@Value</a> annotation:</p>\n\n<pre><code>    @Value(\"${graphite.host}\")\n    private String graphiteHost;\n</code></pre>\n\n<p>It wasn't the best way to work with my properties. However, I didn't know a better approach.</p>\n\n<p>Then, I found <a href=\"http://docs.spring.io/spring-boot/docs/1.1.7.RELEASE/api/org/springframework/boot/context/properties/ConfigurationProperties.html\">@ConfigurationProperties</a> - annotation from Boot package, which has everything you need to map your properties. </p>\n\n<h3 id=\"ahrefgivennamegiveniclassfafalinkanchorariahiddentrueiagiven\"><a href=\"#given\" name=\"given\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Given</h3>\n\n<p>Let's say, your <strong>application.yml</strong> looks like following:</p>\n\n<pre><code>graphite:  \n  enabled: true\n  host: localhost\n  port: 2003\n  amountOfTimeBetweenPolls: 20000\n</code></pre>\n\n<h3 id=\"ahrefwhennamewheniclassfafalinkanchorariahiddentrueiawhen\"><a href=\"#when\" name=\"when\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> When</h3>\n\n<p>You need to create a bunch of classes, which you will be autowiring in all parts of your code.</p>\n\n<p>I'm using <a href=\"https://projectlombok.org/\">Project Lombok</a> for skipping java formalities, if you are not, then create getters and setters for your classes. </p>\n\n<pre><code>package org.rngr.properties;\n\nimport lombok.Data;  \nimport org.springframework.boot.context.properties.ConfigurationProperties;\n\nimport javax.validation.constraints.NotNull;\n\n@ConfigurationProperties(ignoreUnknownFields = true)\n@Data\npublic class ApplicationProperties {\n\n    @NotNull\n    private GraphiteProperties graphite;\n\n}\n</code></pre>\n\n<p>Pay attention to <code>@ConfigurationProperties</code> annotation, it's playing a key role here.</p>\n\n<p>Don't forget about subclass:</p>\n\n<pre><code>package org.rngr.properties;\n\nimport lombok.Data;\n\n@Data\npublic class GraphiteProperties {\n\n    private boolean enabled;\n    private String host;\n    private int port;\n    private int amountOfTimeBetweenPolls;\n\n}\n</code></pre>\n\n<p>In the end, you need to enable configuration properties:</p>\n\n<pre><code>package org.rngr;\n\nimport lombok.extern.slf4j.Slf4j;  \nimport org.rngr.config.*;  \nimport org.rngr.properties.ApplicationProperties;  \nimport org.springframework.beans.factory.annotation.Autowired;  \nimport org.springframework.boot.SpringApplication;  \nimport org.springframework.boot.autoconfigure.SpringBootApplication;  \nimport org.springframework.boot.context.properties.EnableConfigurationProperties;  \nimport org.springframework.context.annotation.Configuration;  \nimport org.springframework.context.annotation.Import;\n\n@SpringBootApplication\n@Import({\n        MonitoringConfiguration.class\n})\n@Configuration\n@EnableConfigurationProperties(ApplicationProperties.class)\npublic class WebappApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(WebappApplication.class, args);\n    }\n}\n</code></pre>\n\n<h3 id=\"ahrefthennametheniclassfafalinkanchorariahiddentrueiathen\"><a href=\"#then\" name=\"then\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Then</h3>\n\n<p>Then, you are free to Autowire <strong>ApplicationProperties</strong> instance wherever you want:</p>\n\n<pre><code>    ...\n    @Autowired\n    private ApplicationProperties properties;\n    ...\n</code></pre>\n\n<h3 id=\"ahrefconclusionsnameconclusionsiclassfafalinkanchorariahiddentrueiaconclusions\"><a href=\"#conclusions\" name=\"conclusions\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Conclusions</h3>\n\n<p>I guess, this approach was <a href=\"https://github.com/dropwizard/dropwizard/blob/master/dropwizard-core/src/main/java/io/dropwizard/Configuration.java\">borrowed</a> from Dropwizard framework. One good thing about introducing configuration properties class is that you will get power to validate them.</p>\n\n<p>I remember a few years ago there was a broad discussion about pros and cons of switching to Java configuration. For me, Configuration properties and their transformation to ConfigurationProperties class is something similar.</p>\n\n<h3 id=\"ahreflinksnamelinksiclassfafalinkanchorariahiddentrueialinks\"><a href=\"#links\" name=\"links\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Links</h3>\n\n<ul>\n<li><a href=\"https://spring.io/blog/2013/10/30/empowering-your-apps-with-spring-boot-s-property-support\">Empowering your apps with Spring Boot's property support</a></li>\n</ul>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2017-01-02T18:16:03.000Z","created_by":1,"updated_at":"2017-01-02T22:24:23.000Z","updated_by":1,"published_at":"2017-01-02T19:08:01.000Z","published_by":1},{"id":66,"uuid":"c72c02a7-2c39-4da9-98da-9ff838fc7356","title":"Database Migration tools for Mongo DB","slug":"database-migration-tools-for-mongo-db","markdown":"![](/content/images/2017/02/MongoDB-Logo-5c3a7405a85675366beb3a5ec4c032348c390b3f142f5e6dddf1d78e2df5cb5c.png)\n\nWhen it comes to the problem of migrating database structure, some of you may think about relational databases: there is a strict schema, and to remove something(field, table, index, etc.), you need to take action: execute an SQL statement. But when you work on schema-less databases, it may look like you don't need those migrations. But to be honest, are schema-less database [are schemaless](https://jaxenter.com/mongodb-schemaless-database-112878.html)? In fact, you have more freedom in column and document-based database, but sooner or later you will have to modify some of the results of your work: remove the index, transform column format, etc. That's why with the help of this article I would like to review the available tools for Mongo migrations.\n\n### <a href=\"#mongobee\" name=\"mongobee\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Mongobee\n\nIf you use Spring in your project, then [MongoBee](https://github.com/mongobee/mongobee) should be the most suitable tool for you. The idea is that you write Java methods(changesets), which describe what changes need to be done. The method annotated by @ChangeSet is taken and applied to the database. Mongobee stores changesets history in dbchangelog collection. If you are a Spring guy, and like Java config among others, then you should choose this tool.\n\nYou have two options how to run Mongobee - inside Spring container at the beginning of startup or as an independent process, running separately from Spring. \n\nThe first option is good for the local environment when you have a local database and want to keep it up-to-date with shared one:\n\n```\n@Bean\npublic Mongobee mongobee(){\n  Mongobee runner = new Mongobee(\"mongodb://YOUR_DB_HOST:27017/DB_NAME\");\n  runner.setDbName(\"yourDbName\");  // host must be set if not set in URI\n  runner.setChangeLogsScanPackage(\n       \"com.example.yourapp.changelogs\"); // package to scan for changesets\n  runner.setEnabled(true);         // optional: default is true\n\n  return runner;\n}\n```\n\nThe second option is suitable for shared environments - you don't run migrations on startup, but you run them manually before deploying a new version, which requires those changes.In this case, you can run migrations in some **main** method inside a separate module:\n```\n  Mongobee runner = new Mongobee(\"mongodb://YOUR_DB_HOST:27017/DB_NAME\");\n  runner.setDbName(\"yourDbName\");  // host must be set if not set in URI\n  runner.setChangeLogsScanPackage(\n       \"com.example.yourapp.changelogs\"); // package to scan for changesets\n  runner.setEnabled(true);         // optional: default is true\n  runner.execute();  // start migration methods\n```\n\nThen you can start writing \n```\n@ChangeSet(order = \"001\", id = \"someChangeWithoutArgs\", author = \"testAuthor\")\npublic void someChange1() {\n   // method without arguments can do some non-db changes\n}\n\n@ChangeSet(order = \"002\", id = \"someChangeWithMongoDatabase\", author = \"testAuthor\")\npublic void someChange2(MongoDatabase db) {\n  // type: com.mongodb.client.MongoDatabase : original MongoDB driver v. 3.x, operations allowed by driver are possible\n  // example: \n  MongoCollection<Document> mycollection = db.getCollection(\"mycollection\");\n  Document doc = new Document(\"testName\", \"example\").append(\"test\", \"1\");\n  mycollection.insertOne(doc);\n}\n\n@ChangeSet(order = \"003\", id = \"someChangeWithDb\", author = \"testAuthor\")\npublic void someChange3(DB db) {\n  // This is deprecated in mongo-java-driver 3.x, use MongoDatabase instead\n  // type: com.mongodb.DB : original MongoDB driver v. 2.x, operations allowed by driver are possible\n  // example: \n  DBCollection mycollection = db.getCollection(\"mycollection\");\n  BasicDBObject doc = new BasicDBObject().append(\"test\", \"1\");\n  mycollection .insert(doc);\n}\n\n@ChangeSet(order = \"004\", id = \"someChangeWithJongo\", author = \"testAuthor\")\npublic void someChange4(Jongo jongo) {\n  // type: org.jongo.Jongo : Jongo driver can be used, used for simpler notation\n  // example:\n  MongoCollection mycollection = jongo.getCollection(\"mycollection\");\n  mycollection.insert(\"{test : 1}\");\n}\n\n@ChangeSet(order = \"005\", id = \"someChangeWithSpringDataTemplate\", author = \"testAuthor\")\npublic void someChange5(MongoTemplate mongoTemplate) {\n  // type: org.springframework.data.mongodb.core.MongoTemplate\n  // Spring Data integration allows using MongoTemplate in the ChangeSet\n  // example:\n  mongoTemplate.save(myEntity);\n}\n```\n\n**Mongobee** has a good [Wiki](https://github.com/mongobee/mongobee/wiki/How-to-use-mongobee) page, if you choose this tool, you need to read it carefully.\n\n\n### <a href=\"#mongeez\" name=\"mongeez\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Mongeez\n\n[Mongeez](https://github.com/mongeez/mongeez) is something which you also can run with Spring, or without it, the only difference with Mongobee is that this tool forces you to write XML instructions, like this one:\n\n```java\n<mongoChangeLog>\n    <changeSet changeId=\"ChangeSet-1\" author=\"mlysaght\">\n        <script>\n            db.organization.insert({\n              \"Name\" : \"10Gen\", \"Location\" : \"NYC\", DateFounded : {\"Year\":2008, \"Month\":01, \"day\":01}});\n            db.organization.insert({\n              \"Name\" : \"SecondMarket\", \"Location\" : \"NYC\", DateFounded : {\"Year\":2004, \"Month\":5, \"day\":4}});\n        </script>\n    </changeSet>\n    <changeSet changeId=\"ChangeSet-2\" author=\"mlysaght\">\n        <script>\n            db.user.insert({ \"Name\" : \"Michael Lysaght\"});\n        </script>\n        <script>\n            db.user.insert({ \"Name\" : \"Oleksii Iepishkin\"});\n        </script>\n    </changeSet>\n</mongoChangeLog>\n```\n\nThis tool has an old XML-based config, the last commit to GitHub was [made on Dec 25, 2015](https://github.com/mongeez/mongeez/commit/7bceecaf99b0520d87d5baa690b0e498164a756c) and it looks like this is a bit older instrument than Mongobee.\n\nThese guys also have a [Wiki](https://github.com/mongeez/mongeez/wiki/How-to-use-mongeez) page describing how to set up their tool, so don't miss a chance to read it.\n\n### <a href=\"#java-mongo-migrations\" name=\"java-mongo-migrations\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> java-mongo-migrations\n[java-mongo-migrations](https://github.com/ozwolf-software/java-mongo-migrations) is a pretty simple library and it is written on top of [Jongo](http://jongo.org/). The main idea - you write migration classes, and run them somewhere:\n\n```\npublic class MyApplication {\n    public void start(){\n        List<MongoCommand> commands = new ArrayList<>();\n        commands.add(new FirstMigration());\n        commands.add(new SecondMigration());\n\n        try {\n            MongoMigrations migrations = new MongoMigrations(\"mongo://localhost:27017/my_application_schema\");\n            migrations.setSchemaVersionCollection(\"_my_custom_schema_version\");\n            migrations.migrate(commands);\n        } catch (MongoMigrationsFailureException e) {\n            LOGGER.error(\"Failed to migrate database\", e);\n        }\n    }\n}\n```\n\nI don't like to specify commands manually, and it'd be nice to specify package only. However, I like, that this tool is framework-agnostic.\n\n### <a href=\"#conclusion\" name=\"conclusion\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Conclusion\n\n\nMy choice - Mongobee:\n\n* It can be framework agnostic\n* I like their changeset format\n* It's clear how the tool works, and Wiki has all needed information.\n\nPS - if you have an idea to test Mongo in spring container, you should read my previous article about Spring DATA MongoDb - [Spring DATA Mongo. Testing your mongo using in-memory db](https://ivanursul.com/spring-data-mongo-testing-using-in-memory-db/)\n\n### <a href=\"#links\" name=\"links\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Links\n\n* [Is MongoDB really a schemaless database?](https://jaxenter.com/mongodb-schemaless-database-112878.html)","mobiledoc":null,"html":"<p><img src=\"/content/images/2017/02/MongoDB-Logo-5c3a7405a85675366beb3a5ec4c032348c390b3f142f5e6dddf1d78e2df5cb5c.png\" alt=\"\" /></p>\n\n<p>When it comes to the problem of migrating database structure, some of you may think about relational databases: there is a strict schema, and to remove something(field, table, index, etc.), you need to take action: execute an SQL statement. But when you work on schema-less databases, it may look like you don't need those migrations. But to be honest, are schema-less database <a href=\"https://jaxenter.com/mongodb-schemaless-database-112878.html\">are schemaless</a>? In fact, you have more freedom in column and document-based database, but sooner or later you will have to modify some of the results of your work: remove the index, transform column format, etc. That's why with the help of this article I would like to review the available tools for Mongo migrations.</p>\n\n<h3 id=\"ahrefmongobeenamemongobeeiclassfafalinkanchorariahiddentrueiamongobee\"><a href=\"#mongobee\" name=\"mongobee\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Mongobee</h3>\n\n<p>If you use Spring in your project, then <a href=\"https://github.com/mongobee/mongobee\">MongoBee</a> should be the most suitable tool for you. The idea is that you write Java methods(changesets), which describe what changes need to be done. The method annotated by @ChangeSet is taken and applied to the database. Mongobee stores changesets history in dbchangelog collection. If you are a Spring guy, and like Java config among others, then you should choose this tool.</p>\n\n<p>You have two options how to run Mongobee - inside Spring container at the beginning of startup or as an independent process, running separately from Spring. </p>\n\n<p>The first option is good for the local environment when you have a local database and want to keep it up-to-date with shared one:</p>\n\n<pre><code>@Bean\npublic Mongobee mongobee(){  \n  Mongobee runner = new Mongobee(\"mongodb://YOUR_DB_HOST:27017/DB_NAME\");\n  runner.setDbName(\"yourDbName\");  // host must be set if not set in URI\n  runner.setChangeLogsScanPackage(\n       \"com.example.yourapp.changelogs\"); // package to scan for changesets\n  runner.setEnabled(true);         // optional: default is true\n\n  return runner;\n}\n</code></pre>\n\n<p>The second option is suitable for shared environments - you don't run migrations on startup, but you run them manually before deploying a new version, which requires those changes.In this case, you can run migrations in some <strong>main</strong> method inside a separate module:  </p>\n\n<pre><code>  Mongobee runner = new Mongobee(\"mongodb://YOUR_DB_HOST:27017/DB_NAME\");\n  runner.setDbName(\"yourDbName\");  // host must be set if not set in URI\n  runner.setChangeLogsScanPackage(\n       \"com.example.yourapp.changelogs\"); // package to scan for changesets\n  runner.setEnabled(true);         // optional: default is true\n  runner.execute();  // start migration methods\n</code></pre>\n\n<p>Then you can start writing  </p>\n\n<pre><code>@ChangeSet(order = \"001\", id = \"someChangeWithoutArgs\", author = \"testAuthor\")\npublic void someChange1() {  \n   // method without arguments can do some non-db changes\n}\n\n@ChangeSet(order = \"002\", id = \"someChangeWithMongoDatabase\", author = \"testAuthor\")\npublic void someChange2(MongoDatabase db) {  \n  // type: com.mongodb.client.MongoDatabase : original MongoDB driver v. 3.x, operations allowed by driver are possible\n  // example: \n  MongoCollection&lt;Document&gt; mycollection = db.getCollection(\"mycollection\");\n  Document doc = new Document(\"testName\", \"example\").append(\"test\", \"1\");\n  mycollection.insertOne(doc);\n}\n\n@ChangeSet(order = \"003\", id = \"someChangeWithDb\", author = \"testAuthor\")\npublic void someChange3(DB db) {  \n  // This is deprecated in mongo-java-driver 3.x, use MongoDatabase instead\n  // type: com.mongodb.DB : original MongoDB driver v. 2.x, operations allowed by driver are possible\n  // example: \n  DBCollection mycollection = db.getCollection(\"mycollection\");\n  BasicDBObject doc = new BasicDBObject().append(\"test\", \"1\");\n  mycollection .insert(doc);\n}\n\n@ChangeSet(order = \"004\", id = \"someChangeWithJongo\", author = \"testAuthor\")\npublic void someChange4(Jongo jongo) {  \n  // type: org.jongo.Jongo : Jongo driver can be used, used for simpler notation\n  // example:\n  MongoCollection mycollection = jongo.getCollection(\"mycollection\");\n  mycollection.insert(\"{test : 1}\");\n}\n\n@ChangeSet(order = \"005\", id = \"someChangeWithSpringDataTemplate\", author = \"testAuthor\")\npublic void someChange5(MongoTemplate mongoTemplate) {  \n  // type: org.springframework.data.mongodb.core.MongoTemplate\n  // Spring Data integration allows using MongoTemplate in the ChangeSet\n  // example:\n  mongoTemplate.save(myEntity);\n}\n</code></pre>\n\n<p><strong>Mongobee</strong> has a good <a href=\"https://github.com/mongobee/mongobee/wiki/How-to-use-mongobee\">Wiki</a> page, if you choose this tool, you need to read it carefully.</p>\n\n<h3 id=\"ahrefmongeeznamemongeeziclassfafalinkanchorariahiddentrueiamongeez\"><a href=\"#mongeez\" name=\"mongeez\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Mongeez</h3>\n\n<p><a href=\"https://github.com/mongeez/mongeez\">Mongeez</a> is something which you also can run with Spring, or without it, the only difference with Mongobee is that this tool forces you to write XML instructions, like this one:</p>\n\n<pre><code class=\"language-java\">&lt;mongoChangeLog&gt;  \n    &lt;changeSet changeId=\"ChangeSet-1\" author=\"mlysaght\"&gt;\n        &lt;script&gt;\n            db.organization.insert({\n              \"Name\" : \"10Gen\", \"Location\" : \"NYC\", DateFounded : {\"Year\":2008, \"Month\":01, \"day\":01}});\n            db.organization.insert({\n              \"Name\" : \"SecondMarket\", \"Location\" : \"NYC\", DateFounded : {\"Year\":2004, \"Month\":5, \"day\":4}});\n        &lt;/script&gt;\n    &lt;/changeSet&gt;\n    &lt;changeSet changeId=\"ChangeSet-2\" author=\"mlysaght\"&gt;\n        &lt;script&gt;\n            db.user.insert({ \"Name\" : \"Michael Lysaght\"});\n        &lt;/script&gt;\n        &lt;script&gt;\n            db.user.insert({ \"Name\" : \"Oleksii Iepishkin\"});\n        &lt;/script&gt;\n    &lt;/changeSet&gt;\n&lt;/mongoChangeLog&gt;  \n</code></pre>\n\n<p>This tool has an old XML-based config, the last commit to GitHub was <a href=\"https://github.com/mongeez/mongeez/commit/7bceecaf99b0520d87d5baa690b0e498164a756c\">made on Dec 25, 2015</a> and it looks like this is a bit older instrument than Mongobee.</p>\n\n<p>These guys also have a <a href=\"https://github.com/mongeez/mongeez/wiki/How-to-use-mongeez\">Wiki</a> page describing how to set up their tool, so don't miss a chance to read it.</p>\n\n<h3 id=\"ahrefjavamongomigrationsnamejavamongomigrationsiclassfafalinkanchorariahiddentrueiajavamongomigrations\"><a href=\"#java-mongo-migrations\" name=\"java-mongo-migrations\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> java-mongo-migrations</h3>\n\n<p><a href=\"https://github.com/ozwolf-software/java-mongo-migrations\">java-mongo-migrations</a> is a pretty simple library and it is written on top of <a href=\"http://jongo.org/\">Jongo</a>. The main idea - you write migration classes, and run them somewhere:</p>\n\n<pre><code>public class MyApplication {  \n    public void start(){\n        List&lt;MongoCommand&gt; commands = new ArrayList&lt;&gt;();\n        commands.add(new FirstMigration());\n        commands.add(new SecondMigration());\n\n        try {\n            MongoMigrations migrations = new MongoMigrations(\"mongo://localhost:27017/my_application_schema\");\n            migrations.setSchemaVersionCollection(\"_my_custom_schema_version\");\n            migrations.migrate(commands);\n        } catch (MongoMigrationsFailureException e) {\n            LOGGER.error(\"Failed to migrate database\", e);\n        }\n    }\n}\n</code></pre>\n\n<p>I don't like to specify commands manually, and it'd be nice to specify package only. However, I like, that this tool is framework-agnostic.</p>\n\n<h3 id=\"ahrefconclusionnameconclusioniclassfafalinkanchorariahiddentrueiaconclusion\"><a href=\"#conclusion\" name=\"conclusion\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Conclusion</h3>\n\n<p>My choice - Mongobee:</p>\n\n<ul>\n<li>It can be framework agnostic</li>\n<li>I like their changeset format</li>\n<li>It's clear how the tool works, and Wiki has all needed information.</li>\n</ul>\n\n<p>PS - if you have an idea to test Mongo in spring container, you should read my previous article about Spring DATA MongoDb - <a href=\"https://ivanursul.com/spring-data-mongo-testing-using-in-memory-db/\">Spring DATA Mongo. Testing your mongo using in-memory db</a></p>\n\n<h3 id=\"ahreflinksnamelinksiclassfafalinkanchorariahiddentrueialinks\"><a href=\"#links\" name=\"links\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Links</h3>\n\n<ul>\n<li><a href=\"https://jaxenter.com/mongodb-schemaless-database-112878.html\">Is MongoDB really a schemaless database?</a></li>\n</ul>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2017-02-06T09:36:26.000Z","created_by":1,"updated_at":"2017-02-06T13:01:16.000Z","updated_by":1,"published_at":"2017-02-06T12:47:57.000Z","published_by":1},{"id":67,"uuid":"2357fb2a-9887-4cab-8d4f-afa5812f3f98","title":"Using Redis with Spring Cache","slug":"using-redis-with-spring-cache-2","markdown":"https://docs.spring.io/spring/docs/current/spring-framework-reference/html/cache.html\n\nhttps://www.mkyong.com/spring/spring-caching-and-ehcache-example/\n\nhttp://www.javaworld.com/article/3062899/big-data/lightning-fast-nosql-with-spring-data-redis.html\n\nhttps://dzone.com/articles/enabling-caching-in-mongodb-database-with-redis-us\n\nhttps://www.javacodegeeks.com/2013/02/caching-with-spring-data-redis.html\n\nhttp://caseyscarborough.com/blog/2014/12/18/caching-data-in-spring-using-redis/","mobiledoc":null,"html":"<p><a href=\"https://docs.spring.io/spring/docs/current/spring-framework-reference/html/cache.html\">https://docs.spring.io/spring/docs/current/spring-framework-reference/html/cache.html</a></p>\n\n<p><a href=\"https://www.mkyong.com/spring/spring-caching-and-ehcache-example/\">https://www.mkyong.com/spring/spring-caching-and-ehcache-example/</a></p>\n\n<p><a href=\"http://www.javaworld.com/article/3062899/big-data/lightning-fast-nosql-with-spring-data-redis.html\">http://www.javaworld.com/article/3062899/big-data/lightning-fast-nosql-with-spring-data-redis.html</a></p>\n\n<p><a href=\"https://dzone.com/articles/enabling-caching-in-mongodb-database-with-redis-us\">https://dzone.com/articles/enabling-caching-in-mongodb-database-with-redis-us</a></p>\n\n<p><a href=\"https://www.javacodegeeks.com/2013/02/caching-with-spring-data-redis.html\">https://www.javacodegeeks.com/2013/02/caching-with-spring-data-redis.html</a></p>\n\n<p><a href=\"http://caseyscarborough.com/blog/2014/12/18/caching-data-in-spring-using-redis/\">http://caseyscarborough.com/blog/2014/12/18/caching-data-in-spring-using-redis/</a></p>","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2017-02-23T08:16:10.000Z","created_by":1,"updated_at":"2017-03-24T11:14:49.000Z","updated_by":1,"published_at":null,"published_by":null},{"id":68,"uuid":"3db1c34f-5543-4448-941a-9b990ede5491","title":"Java, interesting topic","slug":"java-interesting-topic","markdown":"Lock Striping\nFalse sharing\nHappens before\n","mobiledoc":null,"html":"<p>Lock Striping <br />\nFalse sharing <br />\nHappens before</p>","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2017-02-23T11:56:32.000Z","created_by":1,"updated_at":"2017-02-23T11:58:22.000Z","updated_by":1,"published_at":null,"published_by":null},{"id":69,"uuid":"63612c4f-026d-449c-ba28-f5ff56be34a5","title":"LMAX Disruptor","slug":"lmax-disruptor","markdown":"http://lmax-exchange.github.io/disruptor/\n\nhttps://github.com/LMAX-Exchange/disruptor\nhttps://github.com/LMAX-Exchange/disruptor/wiki/Introduction\nhttps://github.com/LMAX-Exchange/disruptor/wiki/Getting-Started","mobiledoc":null,"html":"<p><a href=\"http://lmax-exchange.github.io/disruptor/\">http://lmax-exchange.github.io/disruptor/</a></p>\n\n<p><a href=\"https://github.com/LMAX-Exchange/disruptor\">https://github.com/LMAX-Exchange/disruptor</a> <br />\n<a href=\"https://github.com/LMAX-Exchange/disruptor/wiki/Introduction\">https://github.com/LMAX-Exchange/disruptor/wiki/Introduction</a> <br />\n<a href=\"https://github.com/LMAX-Exchange/disruptor/wiki/Getting-Started\">https://github.com/LMAX-Exchange/disruptor/wiki/Getting-Started</a></p>","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2017-02-23T11:58:36.000Z","created_by":1,"updated_at":"2017-02-23T12:06:03.000Z","updated_by":1,"published_at":null,"published_by":null},{"id":70,"uuid":"4a5701a1-6ea7-47a5-9f2e-e3cfeb9e64fb","title":"Concurrency structures written using core java","slug":"concurrency-structures","markdown":"### <a href=\"#motivation\" name=\"motivation\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Motivation\nThe reason for writing this article is a lack of my understanding of Java concurrency. That's why I decided to create a post with some structures, which I wrote by myself. For sure, you may agree or disagree with me, I encourage to post your thoughts in comments so we can discuss them there.\n\n### <a href=\"#list\" name=\"list\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> List of structures\n\nImplemented: \n\n* [Publish-Subscribe](#pubsub)\n* [Queue](#queue)\n\nWill implement them later:\n\n* Fixed Thread Pool\n* Cached Thread Pool\n* CountDownLatch\n* CyclicBarrier\n* Phaser\n* Semaphore\n* Exchanger\n* Rate Limiter\n* Lock, ReentrantLock, ReentrantReadWriteLock\n* ConcurrentHashMap\n* AtomicInteger(For fun)\n\n### <a href=\"#pubsub\" name=\"pubsub\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Publish/Subscribe\n\nWhat is Publish-Subscribe, in a nutshell? Obviously, there someone, who has something share something to others, and want to be sure, that everyone will receive this message.\n\nThe idea of following structure is to have a publish-subscribe mechanism, which will act asynchronously, without usage of high-level classes from `java.util.concurrent` package:\n\n* Consumer - entity, which will consume messages from main thread.\n* PubSubModel - main thread, which will send messages to the consumers. Formally, you can treat it as a consumer.\n\nCode can be found [here](https://gist.github.com/ivanursul/2f64c8d3b81eeff348dbdd85c9823027).\n\nLet's go over this code and try all the details:\n\n* First of all, there will be one main thread, which will read lines from console and act as a producer, so there's no `producer` class, just a main method.\n* Consumer extends **Thread** class\n* Consumer runs in a while loop.\n* Each consumer has it's own queue. Don't ask why, there's no serious reason for that, it was done just for fun. \n* Each queue in consumer waits to be notified.\n* Notification is done in `notify(String message)` method:\n* Some of you may ask why we need to wait for queue to be notified here. I filed a special question on [StackOverflow](http://stackoverflow.com/questions/42417636/what-is-the-relationship-between-thread-sleep-and-happens-before),you are free to read it, the short answer is that if we don't wait for something, then our thread will always be in **RUNNING** mode, which is not good, for whatever reasons. With example below, all consumer threads will run in **WAIT** mode most of the time.\n\n\n### <a href=\"#queue\" name=\"queue\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Queue\n\nQueue is something different from publish-subscribe way of sending messages - there's a shared resource, queue, and each consumer attempts to read from it, and because of concurrency monitor, only one will manage to do that.\n\nCode can be found [here](https://gist.github.com/ivanursul/7a7c1c0b329b70bf61719a6213f7fa37)\n\n* All the consumers and producer have one shared resource - queue\n* When producer sends message, he enters queue monitor, adds element to the queue and **notify**.\n* QueueConsumer is extended by Thread and running an infinite loop.\n* Inside infinite loop, consumer waits for **queue**.\n* As soon as producer will execute `queue.notify()`, only one QueueConsumer will manage to enter synchronized block and read this message.\n* Instead of `queue.notify()`, try `queue.notifyAll()`, the order will be removed, and there no guarantee that some threads will consume it, at all. In case of **notify**, I noticed an order in consumers: `\n message 0 \n Consumer0: Consuming message: message 0\n message 1\n Consumer1: Consuming message: message 1\n message 2\n Consumer2: Consuming message: message 2\n message 3\n Consumer0: Consuming message: message 3`\n\nWhat, if we would like to decide, who is going to consume this message? Let's say, we have consumer groups and want to address some messages to some consumer groups? \n[Here is](https://gist.github.com/ivanursul/ca826fce48baed04ade754dfde102859) a new version of QueueModel.\n\nThe output will look like this:\n\n```\nMessage 1\ngroup2\ngroup2: Consumer4: Consuming message: Message{message='Message 1', consumerGroup='group2'}\nMessage 2\ngroup2\ngroup1: Consumer5: Skipping message, wrong group, message: Message{message='Message 2', consumerGroup='group2'}\ngroup1: Consumer6: Skipping message, wrong group, message: Message{message='Message 2', consumerGroup='group2'}\ngroup1: Consumer7: Skipping message, wrong group, message: Message{message='Message 2', consumerGroup='group2'}\ngroup1: Consumer8: Skipping message, wrong group, message: Message{message='Message 2', consumerGroup='group2'}\ngroup1: Consumer9: Skipping message, wrong group, message: Message{message='Message 2', consumerGroup='group2'}\ngroup1: Consumer0: Skipping message, wrong group, message: Message{message='Message 2', consumerGroup='group2'}\ngroup2: Consumer1: Consuming message: Message{message='Message 2', consumerGroup='group2'}\n\n```\n\nThis approach is doing his work, however, there's a problem: I don't like the idea of receiving a message if you are initially a wrong recipient. It's the same if you will get a call from post service with a package for your neighbor. That's why here is a new version of [QueueModel](https://gist.github.com/ivanursul/9cb82cf4c01fb10a74456258dea9d31f). The difference is straighforward - now each consumer group has it's own queue, so you don't need to mix things up. ","mobiledoc":null,"html":"<h3 id=\"ahrefmotivationnamemotivationiclassfafalinkanchorariahiddentrueiamotivation\"><a href=\"#motivation\" name=\"motivation\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Motivation</h3>\n\n<p>The reason for writing this article is a lack of my understanding of Java concurrency. That's why I decided to create a post with some structures, which I wrote by myself. For sure, you may agree or disagree with me, I encourage to post your thoughts in comments so we can discuss them there.</p>\n\n<h3 id=\"ahreflistnamelisticlassfafalinkanchorariahiddentrueialistofstructures\"><a href=\"#list\" name=\"list\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> List of structures</h3>\n\n<p>Implemented: </p>\n\n<ul>\n<li><a href=\"#pubsub\">Publish-Subscribe</a></li>\n<li><a href=\"#queue\">Queue</a></li>\n</ul>\n\n<p>Will implement them later:</p>\n\n<ul>\n<li>Fixed Thread Pool</li>\n<li>Cached Thread Pool</li>\n<li>CountDownLatch</li>\n<li>CyclicBarrier</li>\n<li>Phaser</li>\n<li>Semaphore</li>\n<li>Exchanger</li>\n<li>Rate Limiter</li>\n<li>Lock, ReentrantLock, ReentrantReadWriteLock</li>\n<li>ConcurrentHashMap</li>\n<li>AtomicInteger(For fun)</li>\n</ul>\n\n<h3 id=\"ahrefpubsubnamepubsubiclassfafalinkanchorariahiddentrueiapublishsubscribe\"><a href=\"#pubsub\" name=\"pubsub\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Publish/Subscribe</h3>\n\n<p>What is Publish-Subscribe, in a nutshell? Obviously, there someone, who has something share something to others, and want to be sure, that everyone will receive this message.</p>\n\n<p>The idea of following structure is to have a publish-subscribe mechanism, which will act asynchronously, without usage of high-level classes from <code>java.util.concurrent</code> package:</p>\n\n<ul>\n<li>Consumer - entity, which will consume messages from main thread.</li>\n<li>PubSubModel - main thread, which will send messages to the consumers. Formally, you can treat it as a consumer.</li>\n</ul>\n\n<p>Code can be found <a href=\"https://gist.github.com/ivanursul/2f64c8d3b81eeff348dbdd85c9823027\">here</a>.</p>\n\n<p>Let's go over this code and try all the details:</p>\n\n<ul>\n<li>First of all, there will be one main thread, which will read lines from console and act as a producer, so there's no <code>producer</code> class, just a main method.</li>\n<li>Consumer extends <strong>Thread</strong> class</li>\n<li>Consumer runs in a while loop.</li>\n<li>Each consumer has it's own queue. Don't ask why, there's no serious reason for that, it was done just for fun. </li>\n<li>Each queue in consumer waits to be notified.</li>\n<li>Notification is done in <code>notify(String message)</code> method:</li>\n<li>Some of you may ask why we need to wait for queue to be notified here. I filed a special question on <a href=\"http://stackoverflow.com/questions/42417636/what-is-the-relationship-between-thread-sleep-and-happens-before\">StackOverflow</a>,you are free to read it, the short answer is that if we don't wait for something, then our thread will always be in <strong>RUNNING</strong> mode, which is not good, for whatever reasons. With example below, all consumer threads will run in <strong>WAIT</strong> mode most of the time.</li>\n</ul>\n\n<h3 id=\"ahrefqueuenamequeueiclassfafalinkanchorariahiddentrueiaqueue\"><a href=\"#queue\" name=\"queue\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Queue</h3>\n\n<p>Queue is something different from publish-subscribe way of sending messages - there's a shared resource, queue, and each consumer attempts to read from it, and because of concurrency monitor, only one will manage to do that.</p>\n\n<p>Code can be found <a href=\"https://gist.github.com/ivanursul/7a7c1c0b329b70bf61719a6213f7fa37\">here</a></p>\n\n<ul>\n<li>All the consumers and producer have one shared resource - queue</li>\n<li>When producer sends message, he enters queue monitor, adds element to the queue and <strong>notify</strong>.</li>\n<li>QueueConsumer is extended by Thread and running an infinite loop.</li>\n<li>Inside infinite loop, consumer waits for <strong>queue</strong>.</li>\n<li>As soon as producer will execute <code>queue.notify()</code>, only one QueueConsumer will manage to enter synchronized block and read this message.</li>\n<li>Instead of <code>queue.notify()</code>, try <code>queue.notifyAll()</code>, the order will be removed, and there no guarantee that some threads will consume it, at all. In case of <strong>notify</strong>, I noticed an order in consumers: <code>\nmessage 0 \nConsumer0: Consuming message: message 0\nmessage 1\nConsumer1: Consuming message: message 1\nmessage 2\nConsumer2: Consuming message: message 2\nmessage 3\nConsumer0: Consuming message: message 3</code></li>\n</ul>\n\n<p>What, if we would like to decide, who is going to consume this message? Let's say, we have consumer groups and want to address some messages to some consumer groups? <br />\n<a href=\"https://gist.github.com/ivanursul/ca826fce48baed04ade754dfde102859\">Here is</a> a new version of QueueModel.</p>\n\n<p>The output will look like this:</p>\n\n<pre><code>Message 1  \ngroup2  \ngroup2: Consumer4: Consuming message: Message{message='Message 1', consumerGroup='group2'}  \nMessage 2  \ngroup2  \ngroup1: Consumer5: Skipping message, wrong group, message: Message{message='Message 2', consumerGroup='group2'}  \ngroup1: Consumer6: Skipping message, wrong group, message: Message{message='Message 2', consumerGroup='group2'}  \ngroup1: Consumer7: Skipping message, wrong group, message: Message{message='Message 2', consumerGroup='group2'}  \ngroup1: Consumer8: Skipping message, wrong group, message: Message{message='Message 2', consumerGroup='group2'}  \ngroup1: Consumer9: Skipping message, wrong group, message: Message{message='Message 2', consumerGroup='group2'}  \ngroup1: Consumer0: Skipping message, wrong group, message: Message{message='Message 2', consumerGroup='group2'}  \ngroup2: Consumer1: Consuming message: Message{message='Message 2', consumerGroup='group2'}\n</code></pre>\n\n<p>This approach is doing his work, however, there's a problem: I don't like the idea of receiving a message if you are initially a wrong recipient. It's the same if you will get a call from post service with a package for your neighbor. That's why here is a new version of <a href=\"https://gist.github.com/ivanursul/9cb82cf4c01fb10a74456258dea9d31f\">QueueModel</a>. The difference is straighforward - now each consumer group has it's own queue, so you don't need to mix things up. </p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2017-02-23T14:13:30.000Z","created_by":1,"updated_at":"2017-02-26T09:50:01.000Z","updated_by":1,"published_at":"2017-02-25T07:19:37.000Z","published_by":1},{"id":73,"uuid":"4c7ca70b-784a-4d75-b8aa-4914d4cd321f","title":"Microservices interaction at scale using Apache Kafka","slug":"microservices-interaction-apache-kafka","markdown":"### <a href=\"#intro\" name=\"intro\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Intro\n\nWhy do people do microservices architecture?  Let's start with a review of a classical, old-school, n-tier application, which is often called a monolith:\n![](/content/images/2017/05/Screen-Shot-2017-05-25-at-12-55-37-PM.png?style=centerme)\nEveryone worked with this types of applications, we were studying them in university, we did lot's of projects with this architecture and at first glance, it's a good architecture. We can immediately start doing systems on this architecture, they are perfectly simple and easy to implement.\nBut as such system becomes bigger, it starts receiving lots of different issues. \n\nThe first issue is about scalability, it doesn't scale well. If some part of such system will need to scale, then the overall system should be scaled. Since the system is divided into modules, it runs as one physical process and there's no option to scale one module. This can lead to expensive large instances, which you will have to maintain.\n\nThe second issue is about engineers. Since it will become enormously big, you will have to hire a really big team for such system.\n\n### <a href=\"#evolutio\" name=\"evolution\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Evolution\n\nHow can you address this issues? One of the ways you can evolve is to make a transition to a microservices architecture\n\n![](/content/images/2017/05/Screen-Shot-2017-05-25-at-1-50-35-PM.png?style=centerme)\n\nThe way it works is that you have a bunch of services with separate **independent** deployment pipelines, separate teams, and separate codebase. This services can be better written, supported and scaled. In theory.\n\nHere's how it looks in practice:\n\n![](https://media.giphy.com/media/xUPGcoQgzgXZp263i8/giphy.gif?style=centerme)\n\nYou will get a lot of different services, running across the network, having different approaches, some of them will occasionally fail, some of them will have lots of bugs. \nThis is the world, where you will live.\n\n### <a href=\"#goals\" name=\"goals\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Goals\n\nWhat are the goals you want to achieve while migrating to microservices architecture? Of course, saying \"I want to migrate to microservices, because I would like to split the monolith application\" doesn't answer any goals, by implementing it you can get even more problems, than you had before, so you have to think carefully about your goals.\n\n#### <a href=\"#scalingintermsofpeople\" name=\"scalingintermsofpeople\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Scaling in terms of people\n\nFirst of all, you would like to be able to scale in terms of people.\n![](/content/images/2017/05/Screen-Shot-2017-05-25-at-4-19-36-PM.png?style=centerme)\n\nBy working in monolith application, you had a lot of engineers inefficiently working on one big application. Now, you can manage them more effectively, because you can assign more engineers on some big services, fewer engineers on smaller services. You can follow so-called [pizza team](http://www.westerndevs.com/microservices-sizing/) pattern or [number of lines](http://www.westerndevs.com/microservices-sizing/) pattern. Personally, I think that the size of the team should depend on a scope of the context this service is responsible.\n\n#### <a href=\"#scalingintermsofdata\" name=\"scalingintermsofdata\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Scaling in terms of data\n\n![](/content/images/2017/05/Screen-Shot-2017-05-25-at-4-35-46-PM.png?style=centerme)\n\nThe second goal is about data. If you are migrating to microservices, then you should have enough data. This age is about different storages, so when migrating, you will get an option to choose the data storage you want. If you have some graph structures in your business domain, you can decide to use some graph storages like Neo4J. If you have search logic, then search indexes are for you. In case you have a non-stable schema, you can always decide to use NoSQL types of storages like MongoDB. \n\n#### <a href=\"#independence\" name=\"independence\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Service independence\n\nAnd of course, you would like to be independent. Who doesn't like to be independent?\n![](/content/images/2017/05/Screen-Shot-2017-05-25-at-4-45-13-PM.png?style=centerme)\n\nThe key idea here is that your service should be as much independent as he can. For sure, there will be lot's of other services, and it would be a bad idea to communicate with all of them. So, this service will have a separate codebase, separate deployment pipeline. You will have separate meetings, separate projects in your issue tracking system, separate Slack channel, whatever.\n\n### <a href=\"#communication\" name=\"communication\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Service communication\n\nBut companies are inevitably a collection of applications/modules/subsystems, which should work together to achieve one global result. In other words, your microservices should communicate. While working inside monolith, you couldn't take it seriously, because you had only one single process with a bunch of modules. Now, as your application evolves, you have to share information between services, since you will have them running across the network.\n\nSo, what are the communication patterns for microservices? I will review three key patterns:\n\n* **Shared database**\n* **Request/Response aka Shared interfaces**\n* **Asynchronous messaging through a message broker**\n\nLet's review all them, step by step.\n\n#### <a href=\"#sharedb\" name=\"sharedb\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Shared database\n\nIt's the first and the worst communication pattern, which I know. The idea is that you have many microservices with a single database for all of them.\n\n\n![](/content/images/2017/05/Screen-Shot-2017-05-26-at-12-44-04-PM-1.png?style=centerme)\n\nIf you have many services, distributed across a network, you will probably want to have them independent. You can't have them independent if they all are writing and reading from a single database? Even if this database is NoSQL, supports AP from [CAP](https://en.wikipedia.org/wiki/CAP_theorem) theorem(Availability and Partition Tolerance), you still need to have a lot of coordination around your database.\n\nThe true way to deal with databases in microservices infrastructure is to have them separate for every particular service. In other words, you have your database, you support it, you scale it according to your service needs.\n\n![](/content/images/2017/05/Screen-Shot-2017-05-26-at-12-53-26-PM.png)\n\n#### <a href=\"#serviceinterfaces\" name=\"serviceinterfaces\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Service interfaces\nWe live in a world of http endpoints and we naively think that communication between microservices can be organized using this abstractions. This pattern deserves to be partially implemented in modern microservices architecture, but it should be carefully reviewed first.\n![](/content/images/2017/05/Screen-Shot-2017-05-27-at-10-36-26-AM.png)\n\nIf you use service interfaces to query other services for some information, which you don't store in your storage, then it's perfectly okay. There are several reasons why you don't have this information in your storage, for example, you are a small service with a small storage and the information that you are requesting is far too big to store in your little storage.\nHowever, if you want to use service interfaces for doing **write** operations, then you can get into lots of issues. I will explain them, step-by-step.\n\n#### <a href=\"#servicesize\" name=\"servicesize\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Service size\n![](/content/images/2017/06/Screen-Shot-2017-06-04-at-2-18-19-PM.png)\nFirst of all, the size of different services is often different. So, if you decide to query some other service, think twice if you won't overload it. If you ever worked with DynamoDB, you know, that it has a notion of capacity provisioning - you chose an optimal number of read and writes. So, once I saw, how one service started to send requests to another service, which was using DynamoDB for storing its internal data. The number of requests per second was quite big and service started to fail.\n\n#### <a href=\"#criticalpath\" name=\"criticalpath\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Critical path\n\n![](/content/images/2017/06/Screen-Shot-2017-06-04-at-2-14-26-PM.png)\n\nWhen we are talking about microservices, we can assume that not every service needs to be a part of this critical path. Some of the services can do the job later. For example, if you are deleting some user from the system, you don't need to update all the information in all services. You can do it later, without any problems.\n\n\nAdditionally, it brings bigger latency, your users will be affected and for sure, they won't be happy with it.\n\n#### <a href=\"#consistency\" name=\"consistency\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Consistency\n\nAnd even if you had to update all the services immediately, I bet, you could do that without bringing inconsistency to your system. \n\n![](/content/images/2017/06/Screen-Shot-2017-06-04-at-2-19-32-PM.png)\n\nThink about a case, when you are calling services one-by-one and one of the services fails. Some services are already executed, and in the middle of the process, one service fails. What would you do then? Well, you could introduce two-phase commit approach, but the problem is that not every database supports two-phase commits, and even if it does - revise CAP theorem, you can choose only two of three properties - Consistency, Availability, and Partition tolerance. Hopefully, you have Partition tolerance out of the box, and you can choose either Consistency or Availability. And it's better to have an available system, rather than consistent. Furthermore, most distributed system nowadays can guarantee eventual consistency.\n\n### <a href=\"#asyncmessaging\" name=\"asyncmessaging\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Asynchronous messaging\n\nService interfaces are sometimes a good approach for getting some real-time data, but the bad thing about them is that they introduce real-time coupling.\nWho worked some time in software engineering, knows, that the best option to avoid real-time coupling is to bring asynchronous approach. What does it mean in the scope of our microservices architecture? \n\n![](/content/images/2017/06/Screen-Shot-2017-06-04-at-2-23-16-PM-1.png)\n\nIt means, that we will have some level of abstraction, some entity, which will do the operation of transporting messages between different services. This level of abstraction is often called a message broker or a message queue. An entity, which receives some messages from one service, stores it somewhere and sends this message to all consumers, that are subscribed to receive such type of messages. Interesting, don't you find? This way of transporting messages and doing synchronization brings more questions, than answers and needs to be carefully implemented. We will talk about this problems a bit later.\n\nWhen we do such architecture, we can then clearly see, that there are two types of data, in our infrastructure - so called, data-on-the-outside and data-on-the-inside. First one is used as a source of truth, as a storage, for giving information to the outside world, for outside requests. Second is used as a data, shared inside infrastructure for all interested services.\n\n![](/content/images/2017/06/Screen-Shot-2017-06-04-at-2-37-05-PM.png)\n\nThis way of doing microservice communication is more natural, rather than doing communication each particular service. \n\n![](/content/images/2017/06/Screen-Shot-2017-06-04-at-2-40-56-PM.png)\n\nMore formally, the architecture is more pluggable, it's a lot easier for the new service to start receiving existing data: you just start listening to specific information in message broker, and that's it. Existing producers of such messages can don't even know about this new service.\n\n### <a href=\"#apachekafka\" name=\"apachekafka\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Apache Kafka\n\n![](/content/images/2017/06/Screen-Shot-2017-06-04-at-2-44-24-PM.png)\n\nApache Kafka is an example of such message broker. Created inside LinkedIn, it later became one of the best solutions in the market. It's not a message queue, but rather a distributed, publish-subscribe messaging system. It is very fast, scalable and durable.\n\nHere, in this article, we'd like to know how Kafka can improve our microservices, right? Let's review a couple of patterns.\n\n### <a href=\"#eventsourcing\" name=\"eventsourcing\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Event Sourcing \n\nIt's the first pattern, which can be used together with Kafka. For those of you who don't know what is Event Sourcing pattern, I recommend reading some articles. The idea is pretty simple - you use Kafka as an event-sourcing storage, for storing all the events. Then, you use Kafka Consumer for reading those changes. In microservices, it means, that you will design your requests according to the fact, that you will store a message in Kafka and process it later. This means you won't be able to give an immediate answer and this forces you to change the way you process your data. \n\n![](/content/images/2017/06/Screen-Shot-2017-06-19-at-5-15-06-PM.png)\n\nHowever, this gives you more options to work with your data. First of all, as you know, Kafka has an option to rewind messages for specific consumers. You just give an instruction, that you need to rewind it 2 hours behind and that's it, your consumer will start processing older messages. That's a good compensation for delayed processing, don't you find? \nI didn't mention debugging. As you store all the requests in Kafka, you will be able to analyze them in your non-production environment, find a bug in your code, fix it and rewind the messages. \n\nAnother big advantage in having event sourced approach in your system is that you will have a better scaling opportunities. You will have three dedicated functions: writing to Kafka, reading from the Kafka(processing data to your separate storage) and reading from separate storage. Why don't you create separate instances for all those three functions? Here's how it should look:\n\n![](/content/images/2017/06/Screen-Shot-2017-06-19-at-5-13-45-PM.png)\n\nBy having this separation your will have the ability to create 1 instance of write service, 2 instances of listener and 3 instances of Read service. If for some reasons, you will need to scale and add more instances to the write services -there won't be any problems.\n\nI wrote a lot of text about event sourcing pattern, but I haven't explained the relation with Kafka. Kafka fits well in event sourced systems, first of all, because of it's design. It has a retention period for messages, which is exactly [checkpoint pattern]() in event sourcing. It also has an option to save the latest version of your message, having the right to say that Kafka can be treated as a source of truth, in some situations. Kafka is also very fast, configurable and easy to use. That's a perfect candidate for event sourcing approach.\n\n### <a href=\"#cdc\" name=\"cdc\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Change Data Capture\n\nWe've discussed event sourcing previously. While event sourcing intends to update it's database after inserting to event store, CDC(Change Data Capture) approach works completely different.\n\nBefore we discuss this approach, let's imagine we have our microservice, it's data storage and some message broker like Kafka. We would like to update our storage during each request and we also need to update Kafka, so other microservices can reach information about our updates. Does anyone see a problem here? \n\n![](/content/images/2017/06/Screen-Shot-2017-06-19-at-5-15-40-PM.png)\n\nThe problem is that we have two source, which needs to be updated. It's almost impossible to save information in data storage and insert a new message in Kafka and have a consistent system. This approach suffers from race conditions and it's very easy to insert a record in the database and have an exception in Kafka side. What should we do instead? \n\nThe alternative way is to continue inserts into our data storage and stop inserting new messages into Kafka. How is Kafka going to be up-to-date? Let's think about what we're inserting in Kafka. The truth is that we are inserting the same message(or almost the same), so the other consumers(microservices) can react on this event. Change Data Capture approach is getting fresh changes from the database and inserts them to Kafka. \n\n![](/content/images/2017/06/Screen-Shot-2017-06-19-at-5-18-25-PM.png)\n\nFor example, in PostgreSQL it uses a feature called [logical decoding](), introduced in 9.4 version, to read [Write-Ahead Log]() of changes. You can find a lot of tools around, which provide CDC functionality. As I am using PostgreSQL, I know that there are such tools as [Bottled Water](), [Debezium]() and a lot of smaller tools.\n\nThe process consists of two parts:\n\n* Collecting all information from the table and putting it to the Kafka side\n* Reading the new data and continuously inserting it to the Kafka.\n\nThe first part is quite big. It can take several hours before it finishes. The second step won't stop and will continue putting data to the Kafka, so you can read it and react.\n\nOne of the biggest advantages, that I see is that we can adapt existing databases for streaming it's data changes. For example you have a PostgreSQL database in some particular service and you have a request to stream your changes. Without a doubt, you can easily do that with one of the tools, provided above We can also adapt our legacy data storages to stream their changes. This can eliminate your ETL monsters, which are transforming data from your legacy side to your modern microservices.\n\n![](/content/images/2017/06/Screen-Shot-2017-06-19-at-5-19-24-PM.png)\n\n### <a href=\"#kafkaconnectpattern\" name=\"kafkaconnectpattern\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Kafka Connect\n\nFinally, when you don't want to use event sourcing pattern and you don't have any CDC tools in your data storage, you can stick to Kafka Connect. \n\n![](/content/images/2017/06/Screen-Shot-2017-06-19-at-5-20-07-PM.png)\n\nThe idea is pretty much the same as in CDC, except that you will do a scheduled calls to the database to get a new data. This will, in fact, give additional load to the database. Furthermore, because you will do scheduled calls every period of time, your messages will appear in Kafka in batches, rather than in streams and if you choose a big interval of time to wait before each call, you will have your consumers waiting for the new data. For sure, Kafka Connect requires more configuration.\n\nI encourage you to read more about Kafka Connect, there's a very descriptive documentation, lot's of articles and youtube talks about it.\n\n### <a href=\"#conclusion\" name=\"conclusion\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Conclusion\n\nI mentioned three patterns, which you can use inside your microservice architecture. This article is an introduction to the Kafka and microservices, I didn't mention other patterns and if you know them - do a little comment and let's discuss.\n\n### <a href=\"#links\" name=\"links\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Links\n\n* [Demo](https://github.com/ivanursul/kafka-microservices)\n* [Slides](https://www.dropbox.com/s/qe8r04avc9sco4w/kafka%20microservices.zip?dl=0)","mobiledoc":null,"html":"<h3 id=\"ahrefintronameintroiclassfafalinkanchorariahiddentrueiaintro\"><a href=\"#intro\" name=\"intro\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Intro</h3>\n\n<p>Why do people do microservices architecture?  Let's start with a review of a classical, old-school, n-tier application, which is often called a monolith: <br />\n<img src=\"/content/images/2017/05/Screen-Shot-2017-05-25-at-12-55-37-PM.png?style=centerme\" alt=\"\" />\nEveryone worked with this types of applications, we were studying them in university, we did lot's of projects with this architecture and at first glance, it's a good architecture. We can immediately start doing systems on this architecture, they are perfectly simple and easy to implement. <br />\nBut as such system becomes bigger, it starts receiving lots of different issues. </p>\n\n<p>The first issue is about scalability, it doesn't scale well. If some part of such system will need to scale, then the overall system should be scaled. Since the system is divided into modules, it runs as one physical process and there's no option to scale one module. This can lead to expensive large instances, which you will have to maintain.</p>\n\n<p>The second issue is about engineers. Since it will become enormously big, you will have to hire a really big team for such system.</p>\n\n<h3 id=\"ahrefevolutionameevolutioniclassfafalinkanchorariahiddentrueiaevolution\"><a href=\"#evolutio\" name=\"evolution\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Evolution</h3>\n\n<p>How can you address this issues? One of the ways you can evolve is to make a transition to a microservices architecture</p>\n\n<p><img src=\"/content/images/2017/05/Screen-Shot-2017-05-25-at-1-50-35-PM.png?style=centerme\" alt=\"\" /></p>\n\n<p>The way it works is that you have a bunch of services with separate <strong>independent</strong> deployment pipelines, separate teams, and separate codebase. This services can be better written, supported and scaled. In theory.</p>\n\n<p>Here's how it looks in practice:</p>\n\n<p><img src=\"https://media.giphy.com/media/xUPGcoQgzgXZp263i8/giphy.gif?style=centerme\" alt=\"\" /></p>\n\n<p>You will get a lot of different services, running across the network, having different approaches, some of them will occasionally fail, some of them will have lots of bugs. <br />\nThis is the world, where you will live.</p>\n\n<h3 id=\"ahrefgoalsnamegoalsiclassfafalinkanchorariahiddentrueiagoals\"><a href=\"#goals\" name=\"goals\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Goals</h3>\n\n<p>What are the goals you want to achieve while migrating to microservices architecture? Of course, saying \"I want to migrate to microservices, because I would like to split the monolith application\" doesn't answer any goals, by implementing it you can get even more problems, than you had before, so you have to think carefully about your goals.</p>\n\n<h4 id=\"ahrefscalingintermsofpeoplenamescalingintermsofpeopleiclassfafalinkanchorariahiddentrueiascalingintermsofpeople\"><a href=\"#scalingintermsofpeople\" name=\"scalingintermsofpeople\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Scaling in terms of people</h4>\n\n<p>First of all, you would like to be able to scale in terms of people. <br />\n<img src=\"/content/images/2017/05/Screen-Shot-2017-05-25-at-4-19-36-PM.png?style=centerme\" alt=\"\" /></p>\n\n<p>By working in monolith application, you had a lot of engineers inefficiently working on one big application. Now, you can manage them more effectively, because you can assign more engineers on some big services, fewer engineers on smaller services. You can follow so-called <a href=\"http://www.westerndevs.com/microservices-sizing/\">pizza team</a> pattern or <a href=\"http://www.westerndevs.com/microservices-sizing/\">number of lines</a> pattern. Personally, I think that the size of the team should depend on a scope of the context this service is responsible.</p>\n\n<h4 id=\"ahrefscalingintermsofdatanamescalingintermsofdataiclassfafalinkanchorariahiddentrueiascalingintermsofdata\"><a href=\"#scalingintermsofdata\" name=\"scalingintermsofdata\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Scaling in terms of data</h4>\n\n<p><img src=\"/content/images/2017/05/Screen-Shot-2017-05-25-at-4-35-46-PM.png?style=centerme\" alt=\"\" /></p>\n\n<p>The second goal is about data. If you are migrating to microservices, then you should have enough data. This age is about different storages, so when migrating, you will get an option to choose the data storage you want. If you have some graph structures in your business domain, you can decide to use some graph storages like Neo4J. If you have search logic, then search indexes are for you. In case you have a non-stable schema, you can always decide to use NoSQL types of storages like MongoDB. </p>\n\n<h4 id=\"ahrefindependencenameindependenceiclassfafalinkanchorariahiddentrueiaserviceindependence\"><a href=\"#independence\" name=\"independence\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Service independence</h4>\n\n<p>And of course, you would like to be independent. Who doesn't like to be independent? <br />\n<img src=\"/content/images/2017/05/Screen-Shot-2017-05-25-at-4-45-13-PM.png?style=centerme\" alt=\"\" /></p>\n\n<p>The key idea here is that your service should be as much independent as he can. For sure, there will be lot's of other services, and it would be a bad idea to communicate with all of them. So, this service will have a separate codebase, separate deployment pipeline. You will have separate meetings, separate projects in your issue tracking system, separate Slack channel, whatever.</p>\n\n<h3 id=\"ahrefcommunicationnamecommunicationiclassfafalinkanchorariahiddentrueiaservicecommunication\"><a href=\"#communication\" name=\"communication\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Service communication</h3>\n\n<p>But companies are inevitably a collection of applications/modules/subsystems, which should work together to achieve one global result. In other words, your microservices should communicate. While working inside monolith, you couldn't take it seriously, because you had only one single process with a bunch of modules. Now, as your application evolves, you have to share information between services, since you will have them running across the network.</p>\n\n<p>So, what are the communication patterns for microservices? I will review three key patterns:</p>\n\n<ul>\n<li><strong>Shared database</strong></li>\n<li><strong>Request/Response aka Shared interfaces</strong></li>\n<li><strong>Asynchronous messaging through a message broker</strong></li>\n</ul>\n\n<p>Let's review all them, step by step.</p>\n\n<h4 id=\"ahrefsharedbnamesharedbiclassfafalinkanchorariahiddentrueiashareddatabase\"><a href=\"#sharedb\" name=\"sharedb\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Shared database</h4>\n\n<p>It's the first and the worst communication pattern, which I know. The idea is that you have many microservices with a single database for all of them.</p>\n\n<p><img src=\"/content/images/2017/05/Screen-Shot-2017-05-26-at-12-44-04-PM-1.png?style=centerme\" alt=\"\" /></p>\n\n<p>If you have many services, distributed across a network, you will probably want to have them independent. You can't have them independent if they all are writing and reading from a single database? Even if this database is NoSQL, supports AP from <a href=\"https://en.wikipedia.org/wiki/CAP_theorem\">CAP</a> theorem(Availability and Partition Tolerance), you still need to have a lot of coordination around your database.</p>\n\n<p>The true way to deal with databases in microservices infrastructure is to have them separate for every particular service. In other words, you have your database, you support it, you scale it according to your service needs.</p>\n\n<p><img src=\"/content/images/2017/05/Screen-Shot-2017-05-26-at-12-53-26-PM.png\" alt=\"\" /></p>\n\n<h4 id=\"ahrefserviceinterfacesnameserviceinterfacesiclassfafalinkanchorariahiddentrueiaserviceinterfaces\"><a href=\"#serviceinterfaces\" name=\"serviceinterfaces\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Service interfaces</h4>\n\n<p>We live in a world of http endpoints and we naively think that communication between microservices can be organized using this abstractions. This pattern deserves to be partially implemented in modern microservices architecture, but it should be carefully reviewed first. <br />\n<img src=\"/content/images/2017/05/Screen-Shot-2017-05-27-at-10-36-26-AM.png\" alt=\"\" /></p>\n\n<p>If you use service interfaces to query other services for some information, which you don't store in your storage, then it's perfectly okay. There are several reasons why you don't have this information in your storage, for example, you are a small service with a small storage and the information that you are requesting is far too big to store in your little storage. <br />\nHowever, if you want to use service interfaces for doing <strong>write</strong> operations, then you can get into lots of issues. I will explain them, step-by-step.</p>\n\n<h4 id=\"ahrefservicesizenameservicesizeiclassfafalinkanchorariahiddentrueiaservicesize\"><a href=\"#servicesize\" name=\"servicesize\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Service size</h4>\n\n<p><img src=\"/content/images/2017/06/Screen-Shot-2017-06-04-at-2-18-19-PM.png\" alt=\"\" />\nFirst of all, the size of different services is often different. So, if you decide to query some other service, think twice if you won't overload it. If you ever worked with DynamoDB, you know, that it has a notion of capacity provisioning - you chose an optimal number of read and writes. So, once I saw, how one service started to send requests to another service, which was using DynamoDB for storing its internal data. The number of requests per second was quite big and service started to fail.</p>\n\n<h4 id=\"ahrefcriticalpathnamecriticalpathiclassfafalinkanchorariahiddentrueiacriticalpath\"><a href=\"#criticalpath\" name=\"criticalpath\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Critical path</h4>\n\n<p><img src=\"/content/images/2017/06/Screen-Shot-2017-06-04-at-2-14-26-PM.png\" alt=\"\" /></p>\n\n<p>When we are talking about microservices, we can assume that not every service needs to be a part of this critical path. Some of the services can do the job later. For example, if you are deleting some user from the system, you don't need to update all the information in all services. You can do it later, without any problems.</p>\n\n<p>Additionally, it brings bigger latency, your users will be affected and for sure, they won't be happy with it.</p>\n\n<h4 id=\"ahrefconsistencynameconsistencyiclassfafalinkanchorariahiddentrueiaconsistency\"><a href=\"#consistency\" name=\"consistency\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Consistency</h4>\n\n<p>And even if you had to update all the services immediately, I bet, you could do that without bringing inconsistency to your system. </p>\n\n<p><img src=\"/content/images/2017/06/Screen-Shot-2017-06-04-at-2-19-32-PM.png\" alt=\"\" /></p>\n\n<p>Think about a case, when you are calling services one-by-one and one of the services fails. Some services are already executed, and in the middle of the process, one service fails. What would you do then? Well, you could introduce two-phase commit approach, but the problem is that not every database supports two-phase commits, and even if it does - revise CAP theorem, you can choose only two of three properties - Consistency, Availability, and Partition tolerance. Hopefully, you have Partition tolerance out of the box, and you can choose either Consistency or Availability. And it's better to have an available system, rather than consistent. Furthermore, most distributed system nowadays can guarantee eventual consistency.</p>\n\n<h3 id=\"ahrefasyncmessagingnameasyncmessagingiclassfafalinkanchorariahiddentrueiaasynchronousmessaging\"><a href=\"#asyncmessaging\" name=\"asyncmessaging\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Asynchronous messaging</h3>\n\n<p>Service interfaces are sometimes a good approach for getting some real-time data, but the bad thing about them is that they introduce real-time coupling. <br />\nWho worked some time in software engineering, knows, that the best option to avoid real-time coupling is to bring asynchronous approach. What does it mean in the scope of our microservices architecture? </p>\n\n<p><img src=\"/content/images/2017/06/Screen-Shot-2017-06-04-at-2-23-16-PM-1.png\" alt=\"\" /></p>\n\n<p>It means, that we will have some level of abstraction, some entity, which will do the operation of transporting messages between different services. This level of abstraction is often called a message broker or a message queue. An entity, which receives some messages from one service, stores it somewhere and sends this message to all consumers, that are subscribed to receive such type of messages. Interesting, don't you find? This way of transporting messages and doing synchronization brings more questions, than answers and needs to be carefully implemented. We will talk about this problems a bit later.</p>\n\n<p>When we do such architecture, we can then clearly see, that there are two types of data, in our infrastructure - so called, data-on-the-outside and data-on-the-inside. First one is used as a source of truth, as a storage, for giving information to the outside world, for outside requests. Second is used as a data, shared inside infrastructure for all interested services.</p>\n\n<p><img src=\"/content/images/2017/06/Screen-Shot-2017-06-04-at-2-37-05-PM.png\" alt=\"\" /></p>\n\n<p>This way of doing microservice communication is more natural, rather than doing communication each particular service. </p>\n\n<p><img src=\"/content/images/2017/06/Screen-Shot-2017-06-04-at-2-40-56-PM.png\" alt=\"\" /></p>\n\n<p>More formally, the architecture is more pluggable, it's a lot easier for the new service to start receiving existing data: you just start listening to specific information in message broker, and that's it. Existing producers of such messages can don't even know about this new service.</p>\n\n<h3 id=\"ahrefapachekafkanameapachekafkaiclassfafalinkanchorariahiddentrueiaapachekafka\"><a href=\"#apachekafka\" name=\"apachekafka\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Apache Kafka</h3>\n\n<p><img src=\"/content/images/2017/06/Screen-Shot-2017-06-04-at-2-44-24-PM.png\" alt=\"\" /></p>\n\n<p>Apache Kafka is an example of such message broker. Created inside LinkedIn, it later became one of the best solutions in the market. It's not a message queue, but rather a distributed, publish-subscribe messaging system. It is very fast, scalable and durable.</p>\n\n<p>Here, in this article, we'd like to know how Kafka can improve our microservices, right? Let's review a couple of patterns.</p>\n\n<h3 id=\"ahrefeventsourcingnameeventsourcingiclassfafalinkanchorariahiddentrueiaeventsourcing\"><a href=\"#eventsourcing\" name=\"eventsourcing\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Event Sourcing</h3>\n\n<p>It's the first pattern, which can be used together with Kafka. For those of you who don't know what is Event Sourcing pattern, I recommend reading some articles. The idea is pretty simple - you use Kafka as an event-sourcing storage, for storing all the events. Then, you use Kafka Consumer for reading those changes. In microservices, it means, that you will design your requests according to the fact, that you will store a message in Kafka and process it later. This means you won't be able to give an immediate answer and this forces you to change the way you process your data. </p>\n\n<p><img src=\"/content/images/2017/06/Screen-Shot-2017-06-19-at-5-15-06-PM.png\" alt=\"\" /></p>\n\n<p>However, this gives you more options to work with your data. First of all, as you know, Kafka has an option to rewind messages for specific consumers. You just give an instruction, that you need to rewind it 2 hours behind and that's it, your consumer will start processing older messages. That's a good compensation for delayed processing, don't you find? <br />\nI didn't mention debugging. As you store all the requests in Kafka, you will be able to analyze them in your non-production environment, find a bug in your code, fix it and rewind the messages. </p>\n\n<p>Another big advantage in having event sourced approach in your system is that you will have a better scaling opportunities. You will have three dedicated functions: writing to Kafka, reading from the Kafka(processing data to your separate storage) and reading from separate storage. Why don't you create separate instances for all those three functions? Here's how it should look:</p>\n\n<p><img src=\"/content/images/2017/06/Screen-Shot-2017-06-19-at-5-13-45-PM.png\" alt=\"\" /></p>\n\n<p>By having this separation your will have the ability to create 1 instance of write service, 2 instances of listener and 3 instances of Read service. If for some reasons, you will need to scale and add more instances to the write services -there won't be any problems.</p>\n\n<p>I wrote a lot of text about event sourcing pattern, but I haven't explained the relation with Kafka. Kafka fits well in event sourced systems, first of all, because of it's design. It has a retention period for messages, which is exactly <a href=\"\">checkpoint pattern</a> in event sourcing. It also has an option to save the latest version of your message, having the right to say that Kafka can be treated as a source of truth, in some situations. Kafka is also very fast, configurable and easy to use. That's a perfect candidate for event sourcing approach.</p>\n\n<h3 id=\"ahrefcdcnamecdciclassfafalinkanchorariahiddentrueiachangedatacapture\"><a href=\"#cdc\" name=\"cdc\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Change Data Capture</h3>\n\n<p>We've discussed event sourcing previously. While event sourcing intends to update it's database after inserting to event store, CDC(Change Data Capture) approach works completely different.</p>\n\n<p>Before we discuss this approach, let's imagine we have our microservice, it's data storage and some message broker like Kafka. We would like to update our storage during each request and we also need to update Kafka, so other microservices can reach information about our updates. Does anyone see a problem here? </p>\n\n<p><img src=\"/content/images/2017/06/Screen-Shot-2017-06-19-at-5-15-40-PM.png\" alt=\"\" /></p>\n\n<p>The problem is that we have two source, which needs to be updated. It's almost impossible to save information in data storage and insert a new message in Kafka and have a consistent system. This approach suffers from race conditions and it's very easy to insert a record in the database and have an exception in Kafka side. What should we do instead? </p>\n\n<p>The alternative way is to continue inserts into our data storage and stop inserting new messages into Kafka. How is Kafka going to be up-to-date? Let's think about what we're inserting in Kafka. The truth is that we are inserting the same message(or almost the same), so the other consumers(microservices) can react on this event. Change Data Capture approach is getting fresh changes from the database and inserts them to Kafka. </p>\n\n<p><img src=\"/content/images/2017/06/Screen-Shot-2017-06-19-at-5-18-25-PM.png\" alt=\"\" /></p>\n\n<p>For example, in PostgreSQL it uses a feature called <a href=\"\">logical decoding</a>, introduced in 9.4 version, to read <a href=\"\">Write-Ahead Log</a> of changes. You can find a lot of tools around, which provide CDC functionality. As I am using PostgreSQL, I know that there are such tools as <a href=\"\">Bottled Water</a>, <a href=\"\">Debezium</a> and a lot of smaller tools.</p>\n\n<p>The process consists of two parts:</p>\n\n<ul>\n<li>Collecting all information from the table and putting it to the Kafka side</li>\n<li>Reading the new data and continuously inserting it to the Kafka.</li>\n</ul>\n\n<p>The first part is quite big. It can take several hours before it finishes. The second step won't stop and will continue putting data to the Kafka, so you can read it and react.</p>\n\n<p>One of the biggest advantages, that I see is that we can adapt existing databases for streaming it's data changes. For example you have a PostgreSQL database in some particular service and you have a request to stream your changes. Without a doubt, you can easily do that with one of the tools, provided above We can also adapt our legacy data storages to stream their changes. This can eliminate your ETL monsters, which are transforming data from your legacy side to your modern microservices.</p>\n\n<p><img src=\"/content/images/2017/06/Screen-Shot-2017-06-19-at-5-19-24-PM.png\" alt=\"\" /></p>\n\n<h3 id=\"ahrefkafkaconnectpatternnamekafkaconnectpatterniclassfafalinkanchorariahiddentrueiakafkaconnect\"><a href=\"#kafkaconnectpattern\" name=\"kafkaconnectpattern\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Kafka Connect</h3>\n\n<p>Finally, when you don't want to use event sourcing pattern and you don't have any CDC tools in your data storage, you can stick to Kafka Connect. </p>\n\n<p><img src=\"/content/images/2017/06/Screen-Shot-2017-06-19-at-5-20-07-PM.png\" alt=\"\" /></p>\n\n<p>The idea is pretty much the same as in CDC, except that you will do a scheduled calls to the database to get a new data. This will, in fact, give additional load to the database. Furthermore, because you will do scheduled calls every period of time, your messages will appear in Kafka in batches, rather than in streams and if you choose a big interval of time to wait before each call, you will have your consumers waiting for the new data. For sure, Kafka Connect requires more configuration.</p>\n\n<p>I encourage you to read more about Kafka Connect, there's a very descriptive documentation, lot's of articles and youtube talks about it.</p>\n\n<h3 id=\"ahrefconclusionnameconclusioniclassfafalinkanchorariahiddentrueiaconclusion\"><a href=\"#conclusion\" name=\"conclusion\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Conclusion</h3>\n\n<p>I mentioned three patterns, which you can use inside your microservice architecture. This article is an introduction to the Kafka and microservices, I didn't mention other patterns and if you know them - do a little comment and let's discuss.</p>\n\n<h3 id=\"ahreflinksnamelinksiclassfafalinkanchorariahiddentrueialinks\"><a href=\"#links\" name=\"links\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Links</h3>\n\n<ul>\n<li><a href=\"https://github.com/ivanursul/kafka-microservices\">Demo</a></li>\n<li><a href=\"https://www.dropbox.com/s/qe8r04avc9sco4w/kafka%20microservices.zip?dl=0\">Slides</a></li>\n</ul>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2017-05-14T07:40:16.000Z","created_by":1,"updated_at":"2017-06-20T06:13:06.000Z","updated_by":1,"published_at":"2017-05-20T06:23:24.000Z","published_by":1},{"id":74,"uuid":"20524178-b3e6-458c-bb93-49cb9b39564f","title":"Apache Kafka in details","slug":"apache-kafka-in-details","markdown":"\n### <a href=\"#apachekafka\" name=\"apachekafka\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Apache Kafka\n\n![](/content/images/2017/06/Screen-Shot-2017-06-04-at-2-44-24-PM.png)\n\nApache Kafka is an example of such message broker. Created inside LinkedIn, it later became one of the best solutions in the market. It's not a message queue, but rather a distributed, publish-subscribe messaging system. It is very fast, scalable and durable.\n\n### <a href=\"#inanutshell\" name=\"inanutshell\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Kafka in a nutshell\n\n![](/content/images/2017/06/Screen-Shot-2017-06-04-at-2-45-58-PM.png)\n\nThe core idea about Apache Kafka is that it is a distributed, replicated message log. As simple as it can be, there's a ***.log** files, in which Kafka is sending messages. This type files are **append-only**, as per Kafka contract, which means Kafka will always push messages to the end of the file.\n\n### <a href=\"#kafkaarchitecture\" name=\"kafkaarchitecture\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Architecture\n\nThe architecture consist of several parts:\n\n* **Brokers**. Are used to receive messages, store and send them.\n* **Zookeper**. Apache Zookeper is mostly used for coordination and configuration.\n* **Producers**. Sends messages to Kafka\n* **Consumers**. Consumes messages from Kafka\n* **Connect**. Apache Connect is an API, a sort of adapter, which is used to read or write to third party sources.\n* **Kafka Streams**. Streams API is a new\n\n![](/content/images/2017/06/Screen-Shot-2017-06-04-at-3-04-15-PM.png)\n\nLet's try to understand Kafka in more details.\n\n### <a href=\"#broker\" name=\"broker\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Kafka Broker\n\n### <a href=\"#aot\" name=\"aot\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Anatomy of topic\n\n### <a href=\"#producers\" name=\"producers\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Producer\n\n\n### <a href=\"#consumer\" name=\"consumer\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Consumer\n\n\n### <a href=\"#consumergroups\" name=\"consumergroups\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Consumer groups\n\n### <a href=\"#retention\" name=\"retention\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Retention period\n\n### <a href=\"#logcompaction\" name=\"logcompaction\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Log compaction\n\n### <a href=\"#rewind\" name=\"rewind\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Historical Rewind\n\n### <a href=\"#Guarantees\" name=\"Guarantees\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Guarantees\n\n### <a href=\"#worstscenario\" name=\"worstscenario\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Worst scenario\n\n\nThe architecture looks very interesting and promising. There's an entity for creating messages, called producer and entity for storing this messages and an entity for consuming this messages, called consumer. Kafka Connect is either consumer or a producer, depending if it a sink or source connector. Kafka Streams is a completely new library, which is doing both consumer and producer work, but more efficiently.\n\nNice thing about Kafka is about it's flexibility and more natural model. Consumers are asking Brokers for messages. This is more natural, because Broker doesn't need acknowledge messages. Furthermore, if all the consumers are reading from the top of the log, then the performance can be even better, because Kafka is caching message from the tail of the log.\n\nAnother cool thing about Kafka is about it's partitioning model. The more partitions you have - the better throughput you will get. Kafka guarantees ordering in scope of a single partition. Of course, you can't achieve a total order within a whole topic, that's an unreachable goal, but you have it within a single partition and build your logic according to this assumption. This is a typical tradeoff, you will get a better performance, but your messages will be spread across multiple partitions.\n","mobiledoc":null,"html":"<h3 id=\"ahrefapachekafkanameapachekafkaiclassfafalinkanchorariahiddentrueiaapachekafka\"><a href=\"#apachekafka\" name=\"apachekafka\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Apache Kafka</h3>\n\n<p><img src=\"/content/images/2017/06/Screen-Shot-2017-06-04-at-2-44-24-PM.png\" alt=\"\" /></p>\n\n<p>Apache Kafka is an example of such message broker. Created inside LinkedIn, it later became one of the best solutions in the market. It's not a message queue, but rather a distributed, publish-subscribe messaging system. It is very fast, scalable and durable.</p>\n\n<h3 id=\"ahrefinanutshellnameinanutshelliclassfafalinkanchorariahiddentrueiakafkainanutshell\"><a href=\"#inanutshell\" name=\"inanutshell\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Kafka in a nutshell</h3>\n\n<p><img src=\"/content/images/2017/06/Screen-Shot-2017-06-04-at-2-45-58-PM.png\" alt=\"\" /></p>\n\n<p>The core idea about Apache Kafka is that it is a distributed, replicated message log. As simple as it can be, there's a <strong>*.log</strong> files, in which Kafka is sending messages. This type files are <strong>append-only</strong>, as per Kafka contract, which means Kafka will always push messages to the end of the file.</p>\n\n<h3 id=\"ahrefkafkaarchitecturenamekafkaarchitectureiclassfafalinkanchorariahiddentrueiaarchitecture\"><a href=\"#kafkaarchitecture\" name=\"kafkaarchitecture\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Architecture</h3>\n\n<p>The architecture consist of several parts:</p>\n\n<ul>\n<li><strong>Brokers</strong>. Are used to receive messages, store and send them.</li>\n<li><strong>Zookeper</strong>. Apache Zookeper is mostly used for coordination and configuration.</li>\n<li><strong>Producers</strong>. Sends messages to Kafka</li>\n<li><strong>Consumers</strong>. Consumes messages from Kafka</li>\n<li><strong>Connect</strong>. Apache Connect is an API, a sort of adapter, which is used to read or write to third party sources.</li>\n<li><strong>Kafka Streams</strong>. Streams API is a new</li>\n</ul>\n\n<p><img src=\"/content/images/2017/06/Screen-Shot-2017-06-04-at-3-04-15-PM.png\" alt=\"\" /></p>\n\n<p>Let's try to understand Kafka in more details.</p>\n\n<h3 id=\"ahrefbrokernamebrokericlassfafalinkanchorariahiddentrueiakafkabroker\"><a href=\"#broker\" name=\"broker\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Kafka Broker</h3>\n\n<h3 id=\"ahrefaotnameaoticlassfafalinkanchorariahiddentrueiaanatomyoftopic\"><a href=\"#aot\" name=\"aot\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Anatomy of topic</h3>\n\n<h3 id=\"ahrefproducersnameproducersiclassfafalinkanchorariahiddentrueiaproducer\"><a href=\"#producers\" name=\"producers\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Producer</h3>\n\n<h3 id=\"ahrefconsumernameconsumericlassfafalinkanchorariahiddentrueiaconsumer\"><a href=\"#consumer\" name=\"consumer\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Consumer</h3>\n\n<h3 id=\"ahrefconsumergroupsnameconsumergroupsiclassfafalinkanchorariahiddentrueiaconsumergroups\"><a href=\"#consumergroups\" name=\"consumergroups\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Consumer groups</h3>\n\n<h3 id=\"ahrefretentionnameretentioniclassfafalinkanchorariahiddentrueiaretentionperiod\"><a href=\"#retention\" name=\"retention\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Retention period</h3>\n\n<h3 id=\"ahreflogcompactionnamelogcompactioniclassfafalinkanchorariahiddentrueialogcompaction\"><a href=\"#logcompaction\" name=\"logcompaction\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Log compaction</h3>\n\n<h3 id=\"ahrefrewindnamerewindiclassfafalinkanchorariahiddentrueiahistoricalrewind\"><a href=\"#rewind\" name=\"rewind\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Historical Rewind</h3>\n\n<h3 id=\"ahrefguaranteesnameguaranteesiclassfafalinkanchorariahiddentrueiaguarantees\"><a href=\"#Guarantees\" name=\"Guarantees\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Guarantees</h3>\n\n<h3 id=\"ahrefworstscenarionameworstscenarioiclassfafalinkanchorariahiddentrueiaworstscenario\"><a href=\"#worstscenario\" name=\"worstscenario\"><i class=\"fa fa-link anchor\" aria-hidden=\"true\"></i></a> Worst scenario</h3>\n\n<p>The architecture looks very interesting and promising. There's an entity for creating messages, called producer and entity for storing this messages and an entity for consuming this messages, called consumer. Kafka Connect is either consumer or a producer, depending if it a sink or source connector. Kafka Streams is a completely new library, which is doing both consumer and producer work, but more efficiently.</p>\n\n<p>Nice thing about Kafka is about it's flexibility and more natural model. Consumers are asking Brokers for messages. This is more natural, because Broker doesn't need acknowledge messages. Furthermore, if all the consumers are reading from the top of the log, then the performance can be even better, because Kafka is caching message from the tail of the log.</p>\n\n<p>Another cool thing about Kafka is about it's partitioning model. The more partitions you have - the better throughput you will get. Kafka guarantees ordering in scope of a single partition. Of course, you can't achieve a total order within a whole topic, that's an unreachable goal, but you have it within a single partition and build your logic according to this assumption. This is a typical tradeoff, you will get a better performance, but your messages will be spread across multiple partitions.</p>","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2017-06-05T10:56:43.000Z","created_by":1,"updated_at":"2017-06-05T10:57:23.000Z","updated_by":1,"published_at":null,"published_by":null}],"posts_tags":[{"id":2,"post_id":3,"tag_id":2,"sort_order":0},{"id":3,"post_id":4,"tag_id":2,"sort_order":0},{"id":4,"post_id":6,"tag_id":2,"sort_order":0},{"id":5,"post_id":11,"tag_id":2,"sort_order":0},{"id":6,"post_id":12,"tag_id":2,"sort_order":0},{"id":7,"post_id":13,"tag_id":2,"sort_order":0},{"id":8,"post_id":14,"tag_id":22,"sort_order":0},{"id":9,"post_id":17,"tag_id":22,"sort_order":0},{"id":10,"post_id":18,"tag_id":2,"sort_order":0},{"id":11,"post_id":19,"tag_id":2,"sort_order":0},{"id":12,"post_id":20,"tag_id":2,"sort_order":0},{"id":13,"post_id":22,"tag_id":2,"sort_order":0},{"id":14,"post_id":23,"tag_id":2,"sort_order":0},{"id":15,"post_id":24,"tag_id":2,"sort_order":0},{"id":16,"post_id":25,"tag_id":2,"sort_order":0},{"id":17,"post_id":26,"tag_id":22,"sort_order":0},{"id":18,"post_id":28,"tag_id":2,"sort_order":0},{"id":19,"post_id":29,"tag_id":22,"sort_order":0},{"id":20,"post_id":31,"tag_id":2,"sort_order":0},{"id":21,"post_id":32,"tag_id":2,"sort_order":0},{"id":22,"post_id":33,"tag_id":4,"sort_order":0},{"id":23,"post_id":34,"tag_id":2,"sort_order":0},{"id":24,"post_id":35,"tag_id":24,"sort_order":0},{"id":25,"post_id":36,"tag_id":2,"sort_order":0},{"id":26,"post_id":37,"tag_id":6,"sort_order":0},{"id":27,"post_id":38,"tag_id":26,"sort_order":0},{"id":28,"post_id":40,"tag_id":3,"sort_order":0},{"id":29,"post_id":41,"tag_id":2,"sort_order":0},{"id":30,"post_id":42,"tag_id":2,"sort_order":0},{"id":31,"post_id":43,"tag_id":26,"sort_order":0},{"id":32,"post_id":44,"tag_id":2,"sort_order":0},{"id":33,"post_id":45,"tag_id":33,"sort_order":0},{"id":34,"post_id":46,"tag_id":33,"sort_order":0},{"id":35,"post_id":48,"tag_id":16,"sort_order":0},{"id":36,"post_id":49,"tag_id":3,"sort_order":0},{"id":37,"post_id":50,"tag_id":2,"sort_order":0},{"id":38,"post_id":51,"tag_id":32,"sort_order":0},{"id":39,"post_id":52,"tag_id":32,"sort_order":0},{"id":40,"post_id":53,"tag_id":2,"sort_order":0},{"id":41,"post_id":54,"tag_id":44,"sort_order":0},{"id":42,"post_id":6,"tag_id":22,"sort_order":1},{"id":43,"post_id":11,"tag_id":22,"sort_order":1},{"id":44,"post_id":12,"tag_id":22,"sort_order":1},{"id":45,"post_id":13,"tag_id":22,"sort_order":1},{"id":46,"post_id":18,"tag_id":22,"sort_order":1},{"id":47,"post_id":19,"tag_id":22,"sort_order":1},{"id":48,"post_id":20,"tag_id":22,"sort_order":1},{"id":49,"post_id":22,"tag_id":22,"sort_order":1},{"id":50,"post_id":23,"tag_id":22,"sort_order":1},{"id":51,"post_id":24,"tag_id":22,"sort_order":1},{"id":52,"post_id":25,"tag_id":22,"sort_order":1},{"id":53,"post_id":28,"tag_id":22,"sort_order":1},{"id":54,"post_id":31,"tag_id":22,"sort_order":1},{"id":55,"post_id":32,"tag_id":22,"sort_order":1},{"id":56,"post_id":34,"tag_id":11,"sort_order":1},{"id":57,"post_id":36,"tag_id":25,"sort_order":1},{"id":58,"post_id":37,"tag_id":36,"sort_order":1},{"id":59,"post_id":41,"tag_id":18,"sort_order":1},{"id":60,"post_id":42,"tag_id":16,"sort_order":1},{"id":61,"post_id":43,"tag_id":31,"sort_order":1},{"id":62,"post_id":44,"tag_id":40,"sort_order":1},{"id":63,"post_id":45,"tag_id":34,"sort_order":1},{"id":64,"post_id":46,"tag_id":34,"sort_order":1},{"id":65,"post_id":48,"tag_id":37,"sort_order":1},{"id":66,"post_id":49,"tag_id":18,"sort_order":1},{"id":67,"post_id":50,"tag_id":18,"sort_order":1},{"id":68,"post_id":51,"tag_id":43,"sort_order":1},{"id":69,"post_id":52,"tag_id":43,"sort_order":1},{"id":70,"post_id":53,"tag_id":18,"sort_order":1},{"id":71,"post_id":41,"tag_id":27,"sort_order":2},{"id":72,"post_id":42,"tag_id":3,"sort_order":2},{"id":73,"post_id":43,"tag_id":32,"sort_order":2},{"id":74,"post_id":45,"tag_id":35,"sort_order":2},{"id":75,"post_id":46,"tag_id":35,"sort_order":2},{"id":76,"post_id":49,"tag_id":38,"sort_order":2},{"id":77,"post_id":53,"tag_id":26,"sort_order":2},{"id":78,"post_id":42,"tag_id":29,"sort_order":3},{"id":79,"post_id":49,"tag_id":39,"sort_order":3},{"id":80,"post_id":53,"tag_id":42,"sort_order":3},{"id":81,"post_id":42,"tag_id":30,"sort_order":4},{"id":82,"post_id":53,"tag_id":43,"sort_order":4},{"id":83,"post_id":55,"tag_id":42,"sort_order":0},{"id":84,"post_id":55,"tag_id":43,"sort_order":1},{"id":85,"post_id":55,"tag_id":2,"sort_order":2},{"id":86,"post_id":55,"tag_id":18,"sort_order":3},{"id":87,"post_id":55,"tag_id":32,"sort_order":4},{"id":88,"post_id":55,"tag_id":41,"sort_order":5},{"id":89,"post_id":57,"tag_id":2,"sort_order":0},{"id":90,"post_id":57,"tag_id":18,"sort_order":1},{"id":91,"post_id":57,"tag_id":45,"sort_order":2},{"id":92,"post_id":58,"tag_id":46,"sort_order":0},{"id":93,"post_id":60,"tag_id":2,"sort_order":0},{"id":94,"post_id":60,"tag_id":47,"sort_order":1},{"id":95,"post_id":60,"tag_id":48,"sort_order":2},{"id":96,"post_id":60,"tag_id":49,"sort_order":3},{"id":97,"post_id":62,"tag_id":2,"sort_order":0},{"id":98,"post_id":62,"tag_id":50,"sort_order":1},{"id":99,"post_id":62,"tag_id":51,"sort_order":2},{"id":100,"post_id":64,"tag_id":18,"sort_order":0},{"id":101,"post_id":64,"tag_id":3,"sort_order":1},{"id":102,"post_id":64,"tag_id":2,"sort_order":2},{"id":103,"post_id":66,"tag_id":27,"sort_order":0},{"id":104,"post_id":66,"tag_id":52,"sort_order":2},{"id":105,"post_id":66,"tag_id":2,"sort_order":1},{"id":106,"post_id":70,"tag_id":2,"sort_order":0},{"id":107,"post_id":70,"tag_id":53,"sort_order":1}],"roles":[{"id":1,"uuid":"1cf50b97-5e4b-4222-82ab-aee5caae2685","name":"Administrator","description":"Administrators","created_at":"2016-12-03T07:35:07.000Z","created_by":1,"updated_at":"2016-12-03T07:35:07.000Z","updated_by":1},{"id":2,"uuid":"a614ed55-3bd0-43d4-8d0b-1f45dea4b950","name":"Editor","description":"Editors","created_at":"2016-12-03T07:35:07.000Z","created_by":1,"updated_at":"2016-12-03T07:35:07.000Z","updated_by":1},{"id":3,"uuid":"ba3dbd7a-e418-44ac-846e-567f5200d7b2","name":"Author","description":"Authors","created_at":"2016-12-03T07:35:07.000Z","created_by":1,"updated_at":"2016-12-03T07:35:07.000Z","updated_by":1},{"id":4,"uuid":"f173991a-c8d7-44da-a9aa-1c664b6dc616","name":"Owner","description":"Blog Owner","created_at":"2016-12-03T07:35:07.000Z","created_by":1,"updated_at":"2016-12-03T07:35:07.000Z","updated_by":1}],"roles_users":[{"id":1,"role_id":4,"user_id":1},{"id":2,"role_id":2,"user_id":2},{"id":3,"role_id":1,"user_id":3}],"settings":[{"id":1,"uuid":"965a439f-637e-4ad8-ba17-5c7c66a4d0ae","key":"databaseVersion","value":"005","type":"core","created_at":"2016-12-03T07:35:09.000Z","created_by":1,"updated_at":"2016-12-03T07:35:09.000Z","updated_by":1},{"id":2,"uuid":"e77783ab-b449-4419-bbd8-6e51e5bc47dd","key":"dbHash","value":"810532b6-62bf-49dc-9012-51244f97f388","type":"core","created_at":"2016-12-03T07:35:09.000Z","created_by":1,"updated_at":"2016-12-03T07:35:09.000Z","updated_by":1},{"id":3,"uuid":"70170f40-8908-420d-91c9-67baf7291fb4","key":"nextUpdateCheck","value":"1498047701","type":"core","created_at":"2016-12-03T07:35:09.000Z","created_by":1,"updated_at":"2017-06-20T12:21:38.000Z","updated_by":1},{"id":4,"uuid":"b9bcd793-4b43-46eb-b65d-e912661549cc","key":"displayUpdateNotification","value":"0.11.10","type":"core","created_at":"2016-12-03T07:35:09.000Z","created_by":1,"updated_at":"2017-06-20T12:21:38.000Z","updated_by":1},{"id":5,"uuid":"754751be-7d64-4a32-93ef-af2f057fa1f3","key":"title","value":"ivanursul","type":"blog","created_at":"2016-12-03T07:35:09.000Z","created_by":1,"updated_at":"2016-12-11T07:15:50.000Z","updated_by":1},{"id":6,"uuid":"63ec08c8-52e4-47b8-99d3-6389032894dc","key":"description","value":"Software engineer. A fan of CI/CD. Blogger.","type":"blog","created_at":"2016-12-03T07:35:09.000Z","created_by":1,"updated_at":"2016-12-11T07:15:50.000Z","updated_by":1},{"id":7,"uuid":"4608ac62-d867-4fce-82aa-b55d317728ba","key":"logo","value":"","type":"blog","created_at":"2016-12-03T07:35:09.000Z","created_by":1,"updated_at":"2016-12-11T07:15:50.000Z","updated_by":1},{"id":8,"uuid":"c82dbb82-cf38-440f-a931-bca3eff67e38","key":"cover","value":"","type":"blog","created_at":"2016-12-03T07:35:09.000Z","created_by":1,"updated_at":"2016-12-11T07:15:50.000Z","updated_by":1},{"id":9,"uuid":"7d313213-1ecd-4e7e-8adc-c7fce3a9ffe6","key":"defaultLang","value":"en_US","type":"blog","created_at":"2016-12-03T07:35:09.000Z","created_by":1,"updated_at":"2016-12-11T07:15:50.000Z","updated_by":1},{"id":10,"uuid":"fe601b74-3ed0-4fd7-bca2-68e92dad760b","key":"postsPerPage","value":"5","type":"blog","created_at":"2016-12-03T07:35:09.000Z","created_by":1,"updated_at":"2016-12-11T07:15:50.000Z","updated_by":1},{"id":11,"uuid":"7e7a3694-91b8-4943-855e-0de3b3e618b8","key":"forceI18n","value":"true","type":"blog","created_at":"2016-12-03T07:35:09.000Z","created_by":1,"updated_at":"2016-12-11T07:15:50.000Z","updated_by":1},{"id":12,"uuid":"29626868-10e7-4411-a1bb-3d9e486466e4","key":"permalinks","value":"/:slug/","type":"blog","created_at":"2016-12-03T07:35:09.000Z","created_by":1,"updated_at":"2016-12-11T07:15:50.000Z","updated_by":1},{"id":13,"uuid":"d4ea55ee-c9da-47b3-805c-c5f33590679b","key":"ghost_head","value":"<script>\n  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n\n  ga('create', 'UA-58194390-1', 'auto');\n  ga('send', 'pageview');\n\n</script>","type":"blog","created_at":"2016-12-03T07:35:09.000Z","created_by":1,"updated_at":"2016-12-11T07:15:50.000Z","updated_by":1},{"id":14,"uuid":"9a08befc-dd74-43bd-bf3b-0cad5c210918","key":"ghost_foot","value":"","type":"blog","created_at":"2016-12-03T07:35:09.000Z","created_by":1,"updated_at":"2016-12-11T07:15:50.000Z","updated_by":1},{"id":15,"uuid":"c7932cb0-4f35-4ef6-a716-1b49c63cd346","key":"facebook","value":"","type":"blog","created_at":"2016-12-03T07:35:09.000Z","created_by":1,"updated_at":"2016-12-11T07:15:50.000Z","updated_by":1},{"id":16,"uuid":"82143686-1adf-4e25-bf4a-83d2239acbd5","key":"twitter","value":"","type":"blog","created_at":"2016-12-03T07:35:09.000Z","created_by":1,"updated_at":"2016-12-11T07:15:50.000Z","updated_by":1},{"id":17,"uuid":"9ae9446d-3fc1-4c07-bbc4-b4b72038a674","key":"labs","value":"{\"subscribers\":true}","type":"blog","created_at":"2016-12-03T07:35:09.000Z","created_by":1,"updated_at":"2016-12-11T07:15:50.000Z","updated_by":1},{"id":18,"uuid":"9c056dfe-321d-467e-9895-2e4f22336d5b","key":"navigation","value":"[{\"label\":\"Home\",\"url\":\"/\"},{\"label\":\"Subscribe\",\"url\":\"/subscribe/\"}]","type":"blog","created_at":"2016-12-03T07:35:09.000Z","created_by":1,"updated_at":"2016-12-11T07:15:50.000Z","updated_by":1},{"id":19,"uuid":"e724e6d1-981b-4a3a-9df9-802fd7b02d7a","key":"slack","value":"[{\"url\":\"\"}]","type":"blog","created_at":"2016-12-03T07:35:09.000Z","created_by":1,"updated_at":"2016-12-11T07:15:50.000Z","updated_by":1},{"id":20,"uuid":"8dca1c6f-ecd0-4454-b62a-311f4da77939","key":"activeApps","value":"[]","type":"app","created_at":"2016-12-03T07:35:09.000Z","created_by":1,"updated_at":"2016-12-03T07:35:09.000Z","updated_by":1},{"id":21,"uuid":"ba34a9e3-ec7c-4b7e-b578-eedd79ce4218","key":"installedApps","value":"[]","type":"app","created_at":"2016-12-03T07:35:09.000Z","created_by":1,"updated_at":"2017-03-31T05:45:44.000Z","updated_by":1},{"id":22,"uuid":"bdf0879c-79ca-43f1-93b6-21bd11588568","key":"isPrivate","value":"false","type":"private","created_at":"2016-12-03T07:35:09.000Z","created_by":1,"updated_at":"2016-12-11T07:15:50.000Z","updated_by":1},{"id":23,"uuid":"afeffbf6-e5ed-46f5-b8f7-cbec339cced1","key":"password","value":"","type":"private","created_at":"2016-12-03T07:35:09.000Z","created_by":1,"updated_at":"2016-12-11T07:15:50.000Z","updated_by":1},{"id":24,"uuid":"18eb2237-d78f-498d-a9e4-707e20d65f9f","key":"activeTheme","value":"Upholsterygeist","type":"theme","created_at":"2016-12-03T07:35:09.000Z","created_by":1,"updated_at":"2016-12-11T07:15:50.000Z","updated_by":1}],"subscribers":[{"id":1,"uuid":"8ea1f73a-9726-4cd3-bae2-bf49332bff70","name":null,"email":"Ivanon2@gmail.com","status":"subscribed","post_id":null,"subscribed_url":null,"subscribed_referrer":null,"unsubscribed_url":null,"unsubscribed_at":null,"created_at":"2016-12-03T07:50:56.000Z","created_by":1,"updated_at":"2016-12-03T07:50:56.000Z","updated_by":1},{"id":2,"uuid":"5113bff5-3739-48d3-8fd4-225433f98465","name":null,"email":"mounir.ferdjallah@gmail.com","status":"subscribed","post_id":null,"subscribed_url":"http://ivanursul.com/subscribe/","subscribed_referrer":"https://ivanursul.com/","unsubscribed_url":null,"unsubscribed_at":null,"created_at":"2017-05-11T14:28:33.000Z","created_by":0,"updated_at":"2017-05-11T14:28:33.000Z","updated_by":0}],"tags":[{"id":1,"uuid":"f0030c4d-9ac6-4316-bf28-4222f789400b","name":"Getting Started","slug":"getting-started","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:35:07.000Z","created_by":1,"updated_at":"2016-12-03T07:35:07.000Z","updated_by":1},{"id":2,"uuid":"a34d1fcc-3580-46a9-8811-8cc534dd33c4","name":"java","slug":"java","description":"java language","image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":3,"uuid":"86301d2f-bec3-41d4-a828-9bddc4d4b716","name":"spring-framework","slug":"spring-framework","description":"web framework","image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":4,"uuid":"83e76038-1eb6-4e30-a614-e59b12d96b22","name":"maven","slug":"maven","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":5,"uuid":"859fc854-4d0c-4f85-9ded-541a2fd687ca","name":"jenkins","slug":"jenkins","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":6,"uuid":"9967a706-821d-4a6b-b7fc-d483298a2cf2","name":"gatling","slug":"gatling","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":7,"uuid":"5466efa6-a9b8-41da-afa2-f2d94c9058d6","name":"postman","slug":"postman","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":8,"uuid":"1b974e0a-7731-46ba-97c6-18224ae251cd","name":"liquibase","slug":"liquibase","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":9,"uuid":"4404fbb9-f5ef-4d2b-9986-d7839d17af21","name":"angular.js","slug":"angular-js","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":10,"uuid":"9d951498-935a-4a3b-9606-67b8b8ea2160","name":"digital ocean","slug":"digital-ocean","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":11,"uuid":"1b0185f9-3d81-442b-a452-134145d21b60","name":"guice","slug":"google-guice","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":12,"uuid":"00eabf4a-dec7-4ca1-9a95-b3aeda9a1072","name":"postgresql","slug":"postgresql","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":13,"uuid":"d266c34d-b2ca-446f-a898-dbab4ae1a7c7","name":"database","slug":"database","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":14,"uuid":"9dfc9deb-21fa-492d-ae91-b204b747c5ea","name":"sql","slug":"sql","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":15,"uuid":"e352c08f-fb76-4a39-b42b-1ffc267833be","name":"encoding","slug":"encoding","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":16,"uuid":"aa6572fd-539e-4b90-bf7d-800632fafc24","name":"junit","slug":"junit","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":17,"uuid":"a99e5b7b-81a7-44c7-a95d-dcd1745a72f4","name":"jhipster","slug":"jhipster","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":18,"uuid":"454ef328-6d41-45d4-905a-7d0aad5d6874","name":"spring boot","slug":"spring-boot","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":19,"uuid":"11e12083-59b2-4a34-826d-fb49ad94ad9d","name":"java8","slug":"java-8","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":20,"uuid":"ef8228a0-87ed-4061-a1c2-80b937c928c4","name":"rest","slug":"rest","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":21,"uuid":"aa25c1d2-1501-4cb1-901f-cdf57bea8e63","name":"gradle","slug":"gradle","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":22,"uuid":"b25940cb-f423-4255-a1b9-7dbae0cdeb27","name":"programming","slug":"programming","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":23,"uuid":"5cd0c188-48c1-40f6-b7d7-d7d70845d794","name":"guice","slug":"guice","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":24,"uuid":"d90ff056-14dc-4c50-9915-bbf958a160eb","name":"javacc","slug":"javacc","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":25,"uuid":"4162c7bf-be63-4629-80dc-b57e9ef2e0a7","name":"lazybones","slug":"lazybones","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":26,"uuid":"0e4f0b8b-3dee-4086-838f-cbdad0665340","name":"docker","slug":"docker","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":27,"uuid":"997bb321-2186-4f1d-95dd-ccd61f786f28","name":"mongodb","slug":"mongodb","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":28,"uuid":"c71624b6-999c-4503-8fae-dd0b8a4023c1","name":"spring-framework","slug":"spring-framework-2","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":29,"uuid":"51f253c6-c6c0-40a8-b319-a2dc33a19d56","name":"integration testing","slug":"integration-testing","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":30,"uuid":"d9e27f9f-49c7-4b01-98b9-15734a680928","name":"mockito","slug":"mockito","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":31,"uuid":"e471130d-c3c2-4a51-8c72-e320649a9067","name":"docker-machine","slug":"docker-machine","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":32,"uuid":"6a7fbeae-7b25-4bd5-897a-1d2204831bc4","name":"digitalocean","slug":"digitalocean","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":33,"uuid":"19e805fc-f0f2-4fe4-869b-8d15bbb8a30e","name":"apache kafka","slug":"apache-kafka","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":34,"uuid":"fb196e77-045a-4dc4-b3c6-ee664d8f4456","name":"event-processing","slug":"event-processing","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":35,"uuid":"be4b7e90-7463-47c3-8183-b8134aa93cdd","name":"streaming","slug":"streaming","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":36,"uuid":"a9358c9f-546e-4c5a-8cd8-bc9b27d60d20","name":"performance testing","slug":"performance-testing","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":37,"uuid":"db900496-6d4d-4679-911c-d9b47e8fb60d","name":"unit testing","slug":"unit-testing","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:27.000Z","created_by":1,"updated_at":"2016-12-03T07:36:27.000Z","updated_by":1},{"id":38,"uuid":"aa4cf80b-aae3-424b-b121-9dfc3fb02f84","name":"dropwizard","slug":"dropwizard","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:27.000Z","created_by":1,"updated_at":"2016-12-03T07:36:27.000Z","updated_by":1},{"id":39,"uuid":"42b449fe-992f-48f1-aa0b-282669fbf05f","name":"metrics","slug":"metrics","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:27.000Z","created_by":1,"updated_at":"2016-12-03T07:36:27.000Z","updated_by":1},{"id":40,"uuid":"2e2ff3b2-2cd7-43a1-9c04-8bea8760e1d9","name":"tee","slug":"tee","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:27.000Z","created_by":1,"updated_at":"2016-12-03T07:36:27.000Z","updated_by":1},{"id":41,"uuid":"62bfd0b1-c85e-4da5-8fe7-8913e770bbad","name":"devops","slug":"devops","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:27.000Z","created_by":1,"updated_at":"2016-12-03T07:36:27.000Z","updated_by":1},{"id":42,"uuid":"5095eaf7-5ed4-471b-98f3-38e24bce8abc","name":"ansible","slug":"ansible","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:27.000Z","created_by":1,"updated_at":"2016-12-03T07:36:27.000Z","updated_by":1},{"id":43,"uuid":"592e2d28-0fd1-4762-94b1-c524beb4c4cf","name":"terraform","slug":"terraform","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:27.000Z","created_by":1,"updated_at":"2016-12-03T07:36:27.000Z","updated_by":1},{"id":44,"uuid":"78cd45ad-d720-433a-952d-2d2870d5e7cf","name":"linux","slug":"linux","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-03T07:36:27.000Z","created_by":1,"updated_at":"2016-12-03T07:36:27.000Z","updated_by":1},{"id":45,"uuid":"2ba91c9a-506f-49d7-b9ae-d04a6f3cdb86","name":"guava","slug":"guava","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-06T18:37:40.000Z","created_by":1,"updated_at":"2016-12-06T18:37:40.000Z","updated_by":1},{"id":46,"uuid":"eeaf8794-8791-4754-ad0e-28ad6b0b8ad4","name":"pull requests","slug":"pull-requests","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-07T15:13:39.000Z","created_by":1,"updated_at":"2016-12-07T15:13:39.000Z","updated_by":1},{"id":47,"uuid":"7feed2b6-2ce4-406e-8163-f1b97d36de30","name":"amazon lambda","slug":"amazon-lambda","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-11T09:43:03.000Z","created_by":1,"updated_at":"2016-12-11T09:43:03.000Z","updated_by":1},{"id":48,"uuid":"a63af829-c5fc-4bfa-9c2c-1bf3b5bc4057","name":"amazon api gateway","slug":"amazon-api-gateway","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-11T09:43:03.000Z","created_by":1,"updated_at":"2016-12-11T09:43:03.000Z","updated_by":1},{"id":49,"uuid":"27e34852-a88f-4cc5-b037-4bc891302c3d","name":"amazon dynamodb","slug":"amazon-dynamodb","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-11T09:43:03.000Z","created_by":1,"updated_at":"2016-12-11T09:43:03.000Z","updated_by":1},{"id":50,"uuid":"ce417edf-19ef-46b4-b543-81738c327d9f","name":"slf4j","slug":"slf4j","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-17T08:37:41.000Z","created_by":1,"updated_at":"2016-12-17T08:37:41.000Z","updated_by":1},{"id":51,"uuid":"1810d719-be36-48e2-a145-ea279802928a","name":"logback","slug":"logback","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-12-17T08:37:41.000Z","created_by":1,"updated_at":"2016-12-17T08:37:41.000Z","updated_by":1},{"id":52,"uuid":"e6b08b19-0e9c-407c-893a-52d5865ae815","name":"migrations","slug":"migrations","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2017-02-06T12:47:57.000Z","created_by":1,"updated_at":"2017-02-06T12:47:57.000Z","updated_by":1},{"id":53,"uuid":"2b839ad4-7efa-48d4-af94-38449b4694db","name":"concurrency","slug":"concurrency","description":null,"image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2017-02-25T07:19:34.000Z","created_by":1,"updated_at":"2017-02-25T07:19:34.000Z","updated_by":1}],"users":[{"id":1,"uuid":"109b7e37-26d6-4f5f-8fae-52f0ef316bac","name":"Ivan Ursul","slug":"ivan","password":"$2a$10$iq0a6jitAPmPoZBw7zd5/.nikzMzV.T/x/qsaHlLE0VXWzKuRLKo6","email":"Ivanon2@gmail.com","image":"//www.gravatar.com/avatar/85cbeff8ef4927acc72a28f9ee51ced8?s=250&d=mm&r=x","cover":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"accessibility":null,"status":"active","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"tour":null,"last_login":"2017-06-19T08:22:56.000Z","created_at":"2016-12-03T07:35:09.000Z","created_by":1,"updated_at":"2017-06-19T08:22:56.000Z","updated_by":1},{"id":2,"uuid":"d753c7c2-7f9a-43af-a6ba-2ac83debd06b","name":"Maryana Koman","slug":"maryana-koman","password":"$2a$10$7tZlv6jOejE176dW8v9hWeyxnTXgcBHhz.BTnkZ7gCU5vipO65Hi.","email":"maryana.koman@gmail.com","image":null,"cover":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"accessibility":null,"status":"locked","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"tour":null,"last_login":"2016-11-28T16:29:03.000Z","created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1},{"id":3,"uuid":"4ee6e034-dd85-4f24-8fb8-99eff9efb412","name":"Ivan Ursul - Official","slug":"ivan-ursul-official","password":"$2a$10$SXPvM.VP955Gzl2iTYpjfu3/pgmGNwTJLv455nTBgaO5RDpeXMWNG","email":"me@ivanursul.com","image":null,"cover":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"accessibility":null,"status":"locked","language":"en_US","visibility":"public","meta_title":null,"meta_description":null,"tour":null,"last_login":"2016-05-31T17:50:03.000Z","created_at":"2016-12-03T07:36:26.000Z","created_by":1,"updated_at":"2016-12-03T07:36:26.000Z","updated_by":1}]}}]}